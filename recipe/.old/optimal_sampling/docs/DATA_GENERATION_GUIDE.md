# æ•°æ®ç”Ÿæˆç®¡çº¿ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ•°æ®ç”Ÿæˆç®¡çº¿å®ç°äº†åŸºäºæœ€ä¼˜é‡‡æ ·åˆ†å¸ƒ q* çš„æ•°æ®ç”ŸæˆåŠŸèƒ½ï¼Œç”¨äº RLHF è®­ç»ƒæ•°æ®çš„å‡†å¤‡ã€‚

## ğŸ—ï¸ æ¶æ„

```
optimal_sampling_model.py    # æ ¸å¿ƒæ¨¡å‹ç±»
    â”œâ”€â”€ AlphaComputer         # Alphaå‚æ•°è®¡ç®—å™¨
    â”œâ”€â”€ DiagnosticComputer    # è¯Šæ–­ä¿¡æ¯è®¡ç®—å™¨
    â””â”€â”€ OptimalSamplingModel  # ä¸»æ¨¡å‹ç±»

generate_data.py              # æ•°æ®ç”Ÿæˆè„šæœ¬
    â”œâ”€â”€ DatasetAdapter        # æ•°æ®é›†é€‚é…å™¨åŸºç±»
    â”œâ”€â”€ DeepScaleRAdapter     # DeepScaleRæ•°æ®é›†é€‚é…å™¨
    â””â”€â”€ DataGenerator         # æ•°æ®ç”Ÿæˆå™¨
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch transformers datasets tqdm
# å¯é€‰: VLLMæ”¯æŒ
pip install vllm
```

### 2. åŸºç¡€ä½¿ç”¨

```bash
# ä½¿ç”¨ç›¸åŒæ¨¡å‹ (Ï€_Î¸ = Ï€_t), å›ºå®šalpha
python generate_data.py \
    --model_theta meta-llama/Llama-2-7b-hf \
    --dataset agentica-org/DeepScaleR-Preview-Dataset \
    --output data/generated_fixed_alpha.jsonl \
    --alpha_method fixed \
    --fixed_alpha 0.5 \
    --num_samples 1000 \
    --batch_size 8 \
    --save_diagnostics

# ä½¿ç”¨ä¸åŒæ¨¡å‹, KLå¯¹ç§°æ–¹æ³•
python generate_data.py \
    --model_theta meta-llama/Llama-2-7b-hf \
    --model_t meta-llama/Llama-2-7b-chat-hf \
    --dataset agentica-org/DeepScaleR-Preview-Dataset \
    --output data/generated_kl_symmetry.jsonl \
    --alpha_method kl_symmetry \
    --num_samples 1000 \
    --batch_size 4 \
    --save_diagnostics

# ä½¿ç”¨ç†µå…¬å¼ (å¿«é€Ÿè¿‘ä¼¼)
python generate_data.py \
    --model_theta meta-llama/Llama-2-7b-hf \
    --model_t meta-llama/Llama-2-7b-chat-hf \
    --dataset agentica-org/DeepScaleR-Preview-Dataset \
    --output data/generated_entropy.jsonl \
    --alpha_method entropy \
    --num_samples 1000 \
    --batch_size 8 \
    --save_diagnostics
```

### 3. è¾“å‡ºæ ¼å¼

ç”Ÿæˆçš„æ•°æ®ä¸º JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œç¬¦åˆ OpenAI API çš„ messages æ ¼å¼ï¼š

```json
{
  "messages": [
    {
      "role": "user",
      "content": "What is the capital of France?"
    },
    {
      "role": "assistant",
      "content": "The capital of France is Paris..."
    }
  ],
  "sample_idx": 0,
  "original_field1": "...",
  "original_field2": "..."
}
```

è¯Šæ–­ä¿¡æ¯æ–‡ä»¶ (`.diagnostics.jsonl`):

```json
{
  "sample_idx": 0,
  "alpha_mean": 0.523,
  "alpha_std": 0.045,
  "ess_ratio_mean": 0.987,
  "ess_ratio_std": 0.112,
  "kl_theta_mean": 0.234,
  "kl_t_mean": 0.231
}
```

## ğŸ“š è¯¦ç»†æ–‡æ¡£

### OptimalSamplingModel ç±»

#### åˆå§‹åŒ–å‚æ•°

```python
model = OptimalSamplingModel(
    model_theta_path="meta-llama/Llama-2-7b-hf",  # Ï€_Î¸ æ¨¡å‹è·¯å¾„
    model_t_path="meta-llama/Llama-2-7b-chat-hf", # Ï€_t æ¨¡å‹è·¯å¾„ (å¯é€‰)
    backend="transformers",                        # "transformers" æˆ– "vllm"
    alpha_method="kl_symmetry",                   # "fixed", "kl_symmetry", "entropy"
    fixed_alpha=0.5,                              # å›ºå®šalphaå€¼
    alpha_tol=1e-6,                               # KLå¯¹ç§°æ±‚è§£å®¹å·®
    device="cuda",                                 # è®¾å¤‡
    dtype=torch.float16                           # æ•°æ®ç±»å‹
)
```

#### Alpha è®¡ç®—æ–¹æ³•

1. **fixed**: å›ºå®šalphaå€¼
   - æœ€å¿«ï¼Œä¸éœ€è¦è®¡ç®—
   - é€‚åˆå¿«é€Ÿæµ‹è¯•
   - å‚æ•°: `fixed_alpha`

2. **kl_symmetry**: KLå¯¹ç§°æ¡ä»¶ (ç†è®ºæœ€ä¼˜)
   - æ±‚è§£ D_KL(q*||Ï€_Î¸) = D_KL(q*||Ï€_t)
   - äºŒåˆ†æ³•ï¼Œ20æ¬¡è¿­ä»£
   - æ¯ä¸ªtokençº¦ 2-3ms
   - **æ¨èç”¨äºæœ€ç»ˆè®­ç»ƒæ•°æ®**

3. **entropy**: ç†µå…¬å¼å¿«é€Ÿè¿‘ä¼¼
   - Î± â‰ˆ H(Ï€_Î¸) / [H(Ï€_Î¸) + H(Ï€_t)]
   - æœ€å¿«ï¼Œæ— è¿­ä»£
   - è¿‘ä¼¼ç²¾åº¦è¾ƒé«˜
   - é€‚åˆéœ€è¦é€Ÿåº¦çš„åœºæ™¯

#### ç”Ÿæˆæ–¹æ³•

```python
outputs = model.generate(
    prompts=["Hello, how are you?"],  # è¾“å…¥prompts
    max_new_tokens=100,                # æœ€å¤§ç”Ÿæˆtokenæ•°
    temperature=1.0,                   # é‡‡æ ·æ¸©åº¦
    top_p=1.0,                         # nucleus sampling
    top_k=-1,                          # top-k sampling
    return_diagnostics=True            # è¿”å›è¯Šæ–­ä¿¡æ¯
)

# è¾“å‡º
print(outputs.generated_texts)       # ç”Ÿæˆçš„æ–‡æœ¬
print(outputs.alpha_values)          # [batch, seq_len]
print(outputs.ess_ratios)            # [batch, seq_len]
print(outputs.diagnostics)           # Dict[str, Tensor]
```

### æ•°æ®é›†é€‚é…å™¨

#### DeepScaleRAdapter (è‡ªåŠ¨æ£€æµ‹æ ¼å¼)

```python
adapter = DeepScaleRAdapter(
    dataset_name="agentica-org/DeepScaleR-Preview-Dataset",
    split="train"
)

# è‡ªåŠ¨æ£€æµ‹ä»¥ä¸‹å­—æ®µ:
# - prompt: prompt, question, instruction, input, text
# - response: response, answer, output, completion
# - messages: messages (OpenAIæ ¼å¼)
```

#### GenericAdapter (é€šç”¨)

```python
adapter = GenericAdapter(
    dataset_name="your/dataset",
    split="train",
    prompt_field="question",      # æŒ‡å®špromptå­—æ®µ
    response_field="answer"       # æŒ‡å®šresponseå­—æ®µ
)
```

### å‘½ä»¤è¡Œå‚æ•°å®Œæ•´åˆ—è¡¨

```bash
# æ¨¡å‹å‚æ•°
--model_theta PATH              # Ï€_Î¸ æ¨¡å‹è·¯å¾„ (å¿…éœ€)
--model_t PATH                  # Ï€_t æ¨¡å‹è·¯å¾„ (å¯é€‰)
--backend {transformers,vllm}   # Backendé€‰æ‹©

# Alphaæ–¹æ³•
--alpha_method {fixed,kl_symmetry,entropy}  # Alphaè®¡ç®—æ–¹æ³•
--fixed_alpha FLOAT             # å›ºå®šalphaå€¼ (é»˜è®¤0.5)

# æ•°æ®é›†
--dataset NAME                  # HuggingFaceæ•°æ®é›†åç§° (å¿…éœ€)
--dataset_split SPLIT           # æ•°æ®é›†split (é»˜è®¤train)
--dataset_adapter {auto,deepscaler,generic}
--prompt_field FIELD            # Promptå­—æ®µå
--response_field FIELD          # Responseå­—æ®µå

# ç”Ÿæˆå‚æ•°
--num_samples INT               # ç”Ÿæˆæ ·æœ¬æ•° (é»˜è®¤å…¨éƒ¨)
--start_idx INT                 # èµ·å§‹ç´¢å¼• (ç”¨äºæ–­ç‚¹ç»­ä¼ )
--batch_size INT                # Batchå¤§å° (é»˜è®¤8)
--max_new_tokens INT            # æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤512)
--temperature FLOAT             # é‡‡æ ·æ¸©åº¦ (é»˜è®¤1.0)

# è¾“å‡º
--output PATH                   # è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¿…éœ€)
--save_diagnostics              # ä¿å­˜è¯Šæ–­ä¿¡æ¯

# è®¾å¤‡
--device DEVICE                 # è®¾å¤‡ (é»˜è®¤cuda)
--dtype {float16,bfloat16,float32}  # æ•°æ®ç±»å‹ (é»˜è®¤float16)
```

## ğŸ”¬ å®éªŒå»ºè®®

### 1. å¯¹æ¯”ä¸åŒAlphaæ–¹æ³•

```bash
# ç”Ÿæˆ3ç§æ–¹æ³•çš„æ•°æ®è¿›è¡Œå¯¹æ¯”
for method in fixed kl_symmetry entropy; do
    python generate_data.py \
        --model_theta meta-llama/Llama-2-7b-hf \
        --model_t meta-llama/Llama-2-7b-chat-hf \
        --dataset agentica-org/DeepScaleR-Preview-Dataset \
        --output data/generated_${method}.jsonl \
        --alpha_method $method \
        --num_samples 1000 \
        --save_diagnostics
done

# åˆ†æè¯Šæ–­ä¿¡æ¯
python analyze_diagnostics.py data/generated_*.diagnostics.jsonl
```

### 2. ä¸åŒAlphaå›ºå®šå€¼

```bash
# æµ‹è¯•ä¸åŒçš„å›ºå®šalphaå€¼
for alpha in 0.3 0.5 0.7; do
    python generate_data.py \
        --model_theta meta-llama/Llama-2-7b-hf \
        --model_t meta-llama/Llama-2-7b-chat-hf \
        --dataset agentica-org/DeepScaleR-Preview-Dataset \
        --output data/generated_alpha_${alpha}.jsonl \
        --alpha_method fixed \
        --fixed_alpha $alpha \
        --num_samples 1000 \
        --save_diagnostics
done
```

### 3. æ–­ç‚¹ç»­ä¼ 

```bash
# å¦‚æœç”Ÿæˆä¸­æ–­, å¯ä»¥ä»æŒ‡å®šä½ç½®ç»§ç»­
python generate_data.py \
    --model_theta meta-llama/Llama-2-7b-hf \
    --model_t meta-llama/Llama-2-7b-chat-hf \
    --dataset agentica-org/DeepScaleR-Preview-Dataset \
    --output data/generated.jsonl \
    --alpha_method kl_symmetry \
    --start_idx 500 \
    --num_samples 1000 \
    --save_diagnostics
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. Batch Size è°ƒæ•´

```bash
# å°æ¨¡å‹ (GPT-2, Llama-7B) - ä½¿ç”¨è¾ƒå¤§batch
--batch_size 16

# å¤§æ¨¡å‹ (Llama-13B, 30B) - ä½¿ç”¨è¾ƒå°batch
--batch_size 4

# æ ¹æ®GPUå†…å­˜è°ƒæ•´
```

### 2. æ•°æ®ç±»å‹é€‰æ‹©

```bash
# A100/H100 - ä½¿ç”¨ bfloat16 (æ›´ç¨³å®š)
--dtype bfloat16

# V100/å…¶ä»– - ä½¿ç”¨ float16 (æ›´å¿«)
--dtype float16

# è°ƒè¯•/å°æ¨¡å‹ - ä½¿ç”¨ float32
--dtype float32
```

### 3. Alphaæ–¹æ³•é€‰æ‹©

| æ–¹æ³• | é€Ÿåº¦ | ç²¾åº¦ | æ¨èåœºæ™¯ |
|------|------|------|----------|
| fixed | â­â­â­ | â­ | å¿«é€Ÿæµ‹è¯• |
| entropy | â­â­ | â­â­ | å¿«é€Ÿç”Ÿæˆ |
| kl_symmetry | â­ | â­â­â­ | æœ€ç»ˆè®­ç»ƒæ•°æ® |

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°batch size
--batch_size 2

# å‡å°max_new_tokens
--max_new_tokens 256

# ä½¿ç”¨float16
--dtype float16
```

### é—®é¢˜2: æ•°æ®é›†æ ¼å¼ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨generic adapterå¹¶æ‰‹åŠ¨æŒ‡å®šå­—æ®µ
--dataset_adapter generic \
--prompt_field your_prompt_field \
--response_field your_response_field
```

### é—®é¢˜3: ç”Ÿæˆé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨entropyæ–¹æ³• (æ›´å¿«)
--alpha_method entropy

# å¢å¤§batch size
--batch_size 16

# è€ƒè™‘ä½¿ç”¨VLLM (æ³¨æ„: å½“å‰VLLMä¸æ”¯æŒå®Œæ•´q*é‡‡æ ·)
--backend vllm  # éœ€è¦è‡ªè¡Œå®ç°è¿‘ä¼¼æ–¹æ³•
```

## ğŸ“ ä»£ç ç¤ºä¾‹

### Python API ä½¿ç”¨

```python
from optimal_sampling_model import create_optimal_sampling_model

# åˆ›å»ºæ¨¡å‹
model = create_optimal_sampling_model(
    model_theta="meta-llama/Llama-2-7b-hf",
    model_t="meta-llama/Llama-2-7b-chat-hf",
    alpha_method="kl_symmetry"
)

# ç”Ÿæˆ
outputs = model.generate(
    prompts=["What is AI?", "Explain quantum computing"],
    max_new_tokens=100,
    temperature=1.0
)

# æŸ¥çœ‹ç»“æœ
for i, (text, alpha, ess) in enumerate(zip(
    outputs.generated_texts,
    outputs.alpha_values,
    outputs.ess_ratios
)):
    print(f"\n=== Sample {i+1} ===")
    print(f"Text: {text}")
    print(f"Alpha (mean): {alpha.mean():.3f}")
    print(f"ESS ratio (mean): {ess.mean():.3f}")
```

### è‡ªå®šä¹‰æ•°æ®é›†é€‚é…å™¨

```python
from generate_data import DatasetAdapter

class MyCustomAdapter(DatasetAdapter):
    def get_prompt(self, idx: int) -> str:
        sample = self.dataset[idx]
        # è‡ªå®šä¹‰promptæå–é€»è¾‘
        return sample["my_custom_field"]

    def get_metadata(self, idx: int) -> dict:
        sample = self.dataset[idx]
        return {
            "sample_idx": idx,
            "custom_meta": sample.get("meta", {})
        }

# ä½¿ç”¨
from generate_data import DataGenerator
from optimal_sampling_model import create_optimal_sampling_model

model = create_optimal_sampling_model(...)
adapter = MyCustomAdapter("my/dataset", "train")
generator = DataGenerator(model, adapter, "output.jsonl")
generator.generate(num_samples=1000)
```

## ğŸ¯ æœ€ä½³å®è·µ

1. **å…ˆç”¨å°æ•°æ®é›†æµ‹è¯•**: ç”¨ `--num_samples 10` éªŒè¯ç®¡çº¿æ­£å¸¸å·¥ä½œ
2. **ä½¿ç”¨è¯Šæ–­ä¿¡æ¯**: å¯ç”¨ `--save_diagnostics` ç›‘æ§ESS ratio
3. **æ£€æŸ¥alphaåˆ†å¸ƒ**: alphaåº”è¯¥åœ¨ [0.2, 0.8] èŒƒå›´å†…ï¼Œè¿‡äºæç«¯å¯èƒ½æœ‰é—®é¢˜
4. **æ‰¹é‡å®éªŒ**: ä½¿ç”¨è„šæœ¬æ‰¹é‡æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ
5. **ä¿å­˜checkpoint**: å¯¹äºå¤§è§„æ¨¡ç”Ÿæˆï¼Œå®šæœŸæ£€æŸ¥è¾“å‡ºæ–‡ä»¶

## ğŸ“š ç›¸å…³æ–‡ä»¶

- `optimal_sampling_model.py` - æ ¸å¿ƒæ¨¡å‹å®ç°
- `generate_data.py` - æ•°æ®ç”Ÿæˆè„šæœ¬
- `experiment_design.md` - å®éªŒè®¾è®¡æ–¹æ¡ˆ
- `proof_final.md` - ç†è®ºè¯æ˜

## ğŸ¤ è´¡çŒ®

å¦‚æœå‘ç°bugæˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æissueæˆ–PRã€‚

---

**å‡†å¤‡å¥½å¼€å§‹ç”Ÿæˆæ•°æ®äº†å—ï¼Ÿ** ğŸš€
