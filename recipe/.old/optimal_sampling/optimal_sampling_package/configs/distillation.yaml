# Configuration for Semi On-Policy Distillation

# Model settings
teacher_model: "Qwen/Qwen2.5-72B-Instruct"  # Large model (oracle)
theta_model: "Qwen/Qwen2.5-7B-Instruct"     # Small model (student)

# Alpha computation - use KL symmetry for balanced mixing
alpha_method: "kl_symmetry"
alpha_min: 0.6  # Higher weight on teacher for quality
alpha_max: 1.0

# System prompts for semi on-policy distillation
teacher_system_prompt: |
  You are a math reasoning expert. Given a problem and the correct answer,
  generate detailed step-by-step reasoning that leads to the answer.
  Focus on clear logical steps and mathematical principles.

theta_system_prompt: |
  You are a helpful math problem solver. Solve the problem step by step,
  showing your reasoning clearly.

# Enable chat template for proper formatting
enable_chat_template: true

# Performance settings
gpu_memory_utilization: 0.45  # Leave room for student model
enable_prefix_caching: true

# Generation settings for reasoning
max_tokens: 1024  # Longer for detailed reasoning
temperature: 0.8  # Balance creativity and correctness
top_p: 0.95
top_k: -1

# Track alpha statistics for analysis
track_alpha_stats: true

# Distillation-specific settings
distillation:
  # Sample multiple outputs per problem
  num_samples: 4

  # Diverse prefix strategies
  prefix_templates:
    - "Problem: {problem}\nAnswer: {answer}\nDetailed reasoning:"
    - "Question: {problem}\nCorrect solution: {answer}\nStep-by-step explanation:"
    - "Given: {problem}\nResult: {answer}\nProof:"

  # Quality filtering
  min_reasoning_length: 100  # Minimum tokens in reasoning
  require_answer_match: true  # Verify answer appears in reasoning

  # On-policy coverage
  sample_from_prefixes: ["", "Let's think step by step.", "First, "]
  coverage_threshold: 0.85  # Target on-policy coverage
