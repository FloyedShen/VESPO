# vLLM Batch Processing ä¸åŒ vLLM Core å¯¹é½æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ Optimal Sampling V1 ä¸­æ‰¹å¤„ç†ï¼ˆbatch processingï¼‰çš„å…³é”® bugã€vLLM å†…éƒ¨è¡Œä¸ºä»¥åŠä¸¤ä¸ª vLLM å®ä¾‹çš„å¯¹é½ç­–ç•¥ã€‚

---

## ğŸ“‹ ç›®å½•

1. [Bug æ€»ç»“](#bug-æ€»ç»“)
2. [vLLM å†…éƒ¨è¡Œä¸ºåˆ†æ](#vllm-å†…éƒ¨è¡Œä¸ºåˆ†æ)
3. [ä¸¤ä¸ª vLLM Core çš„å¯¹é½ç­–ç•¥](#ä¸¤ä¸ª-vllm-core-çš„å¯¹é½ç­–ç•¥)
4. [å…³é”®è¦ç‚¹æ€»ç»“](#å…³é”®è¦ç‚¹æ€»ç»“)

---

## ğŸ› Bug æ€»ç»“

åœ¨æ‰¹å¤„ç†å®ç°ä¸­å‘ç°äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š

### Bug 1: Logits ç´¢å¼•ç†è§£é”™è¯¯ï¼ˆå·²ä¿®å¤ï¼‰

#### é”™è¯¯ç†è§£

æœ€åˆè®¤ä¸º vLLM çš„ logits tensor å¤§å°æ˜¯ `[current_batch_size, vocab_size]`ï¼Œå¹¶å°è¯•å»ºç«‹ Request ID â†’ Batch Index çš„æ˜ å°„ã€‚

```python
# âŒ é”™è¯¯å®ç°
request_id_to_batch_idx = {}
batch_idx = 0
for idx in self.enabled_requests:
    request_id_to_batch_idx[idx] = batch_idx
    batch_idx += 1

# ç„¶åç”¨ batch_idx è®¿é—® logits
batch_idx = request_id_to_batch_idx[request_id]
logits[batch_idx] = mixed_logits
```

#### æ­£ç¡®ç†è§£

vLLM ä½¿ç”¨**å›ºå®šå¤§å°**çš„ logits tensor: `[max_num_reqs, vocab_size]`ï¼Œå¯ä»¥**ç›´æ¥ç”¨ request index** è®¿é—® logitsã€‚

```python
# âœ… æ­£ç¡®å®ç°ï¼ˆä¸ vLLM å†…ç½® LogitsProcessors ä¸€è‡´ï¼‰
for request_idx, theta_logits in theta_logits_dict.items():
    logits[request_idx] = mixed_logits  # ç›´æ¥ä½¿ç”¨ request_idx
```

#### è¯æ®ï¼ˆæ¥è‡ª vLLM æºç ï¼‰

vLLM å†…ç½®çš„ LogitsProcessors éƒ½æ˜¯ç›´æ¥ç”¨ request index è®¿é—®ï¼š

```python
# vllm/v1/sample/logits_processor/builtin.py

# LogitBiasLogitsProcessor
def apply(self, logits: torch.Tensor) -> torch.Tensor:
    if self.biases:
        logits[self.logits_slice] += self.bias_tensor
        # logits_slice = (req_indices, tok_indices)
    return logits

# MinTokensLogitsProcessor
def apply(self, logits: torch.Tensor) -> torch.Tensor:
    if self.min_toks:
        logits[self.logits_slice] = -float("inf")
        # logits_slice åŒ…å« request indices
    return logits
```

### Bug 2: Original Prompt ç´¢å¼•é”™è¯¯ï¼ˆçœŸæ­£çš„ Bugï¼‰âš ï¸

è¿™æ˜¯å¯¼è‡´æ‰¹å¤„ç†ä¸­æ‰€æœ‰è¯·æ±‚éƒ½è·å–ç¬¬ä¸€ä¸ª prompt çš„**æ ¹æœ¬åŸå› **ã€‚

#### Bug ä»£ç 

```python
# âŒ é”™è¯¯ï¼šæ‰€æœ‰è¯·æ±‚éƒ½å– original_prompts[0]
for index, params, prompt_tok_ids, output_tok_ids in batch_update.added:
    original_prompt = params.extra_args.get("original_prompts", [""])[0]
    #                                                              ^^^
    #                                                              æ€»æ˜¯ [0]ï¼
```

#### é—®é¢˜è¡¨ç°

```python
# æ‰¹å¤„ç†è¯·æ±‚
prompts = ["Problem 1", "Problem 2", "Problem 3"]
theta_prompts = ["Theta 1", "Theta 2", "Theta 3"]

# Bug å¯¼è‡´ï¼š
# Request 0 â†’ original_prompts[0] = "Theta 1" âœ… æ­£ç¡®
# Request 1 â†’ original_prompts[0] = "Theta 1" âŒ é”™è¯¯ï¼åº”è¯¥æ˜¯ "Theta 2"
# Request 2 â†’ original_prompts[0] = "Theta 1" âŒ é”™è¯¯ï¼åº”è¯¥æ˜¯ "Theta 3"

# ç»“æœï¼šæ‰€æœ‰è¯·æ±‚çš„ theta model éƒ½çœ‹åˆ°äº†ç¬¬ä¸€ä¸ªé—®é¢˜çš„ prompt
```

#### ä¿®å¤ä»£ç 

```python
# âœ… æ­£ç¡®ï¼šæ ¹æ® request index è·å–å¯¹åº” prompt
for index, params, prompt_tok_ids, output_tok_ids in batch_update.added:
    original_prompts_list = params.extra_args.get("original_prompts", [])
    original_prompt = original_prompts_list[index] if index < len(original_prompts_list) else ""
    #                                      ^^^^^
    #                                      ä½¿ç”¨ request index
```

#### å®é™…æ¡ˆä¾‹

**é—®é¢˜æè¿°**ï¼šæ‰¹å¤„ç†ä¸¤ä¸ªæ•°å­¦é—®é¢˜æ—¶ï¼Œç¬¬äºŒä¸ªé—®é¢˜çš„è¾“å‡ºæ··å…¥äº†ç¬¬ä¸€ä¸ªé—®é¢˜çš„å†…å®¹ã€‚

```python
# è¾“å…¥
problem1 = "The operation âŠ— is defined as aâŠ—b = 3a+4b..."
problem2 = "Doug constructs a square window..."

# Bug å¯¼è‡´çš„è¾“å‡ºï¼ˆé”™è¯¯ï¼‰
outputs.generated_texts[1]:
"<think>
Okay, let's try to figure out the problem step by step.
So, Doug has a square thing called aâŠ— window..."
# â†‘ æ··å…¥äº† problem1 çš„ âŠ— æ“ä½œç¬¦

# ä¿®å¤åçš„è¾“å‡ºï¼ˆæ­£ç¡®ï¼‰
outputs.generated_texts[1]:
"<think>
Doug constructs a square window using the following steps..."
# âœ… åªè®¨è®º window æ„é€ é—®é¢˜
```

---

## ğŸ” vLLM å†…éƒ¨è¡Œä¸ºåˆ†æ

### 1. Batch å’Œ Request ç´¢å¼•æœºåˆ¶

vLLM ä½¿ç”¨å›ºå®šå®¹é‡çš„ç¨€ç–æ•°ç»„æ¥ç®¡ç†è¯·æ±‚ï¼š

```
InputBatch ç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ max_num_reqs = 64 (å›ºå®šå®¹é‡)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ _req_ids: [req0, req1, None, None,...] â”‚ â† ç¨€ç–æ•°ç»„
â”‚                                        â”‚
â”‚ req_id_to_index: {                     â”‚
â”‚   "req0": 0,  â† request index          â”‚
â”‚   "req1": 1                            â”‚
â”‚ }                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Logits Tensor:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [0]: logits_0   â”‚ â† Request 0 çš„ logits
â”‚ [1]: logits_1   â”‚ â† Request 1 çš„ logits
â”‚ [2]: unused     â”‚
â”‚ ...             â”‚
â”‚ [63]: unused    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Shape: [max_num_reqs, vocab_size]
```

**å…³é”®å‘ç°**ï¼š
- Logits tensor å¤§å°å›ºå®šä¸º `[max_num_reqs, vocab_size]`
- Request index æ˜¯åœ¨ batch ä¸­çš„ä½ç½®ï¼ˆ0-basedï¼‰
- å¯ä»¥ç›´æ¥ç”¨ `logits[request_idx]` è®¿é—®å¯¹åº”è¯·æ±‚çš„ logits

### 2. Index å¤ç”¨æœºåˆ¶ï¼ˆé‡è¦ï¼ï¼‰âš ï¸

å½“å¹¶å‘è¯·æ±‚æ•°è¶…è¿‡ `max_num_reqs` æ—¶ï¼ŒvLLM ä¼šå¤ç”¨å·²å®Œæˆè¯·æ±‚çš„ indexï¼š

```python
# åœºæ™¯ï¼šmax_num_reqs = 64
# æ—¶åˆ» T1: 64 ä¸ªè¯·æ±‚å…¨æ»¡
requests = {0, 1, 2, ..., 63}

# æ—¶åˆ» T2: Request 5 å®Œæˆ
# BatchUpdate: removed=[5]
# â†’ Index 5 è¢«é‡Šæ”¾

# æ—¶åˆ» T3: æ–°è¯·æ±‚ Request_new åŠ å…¥
# BatchUpdate: added=[(5, params, prompt_toks, output_toks)]
#                     ^
#                     å¤ç”¨ index 5
```

#### Index å¤ç”¨çš„å®‰å…¨å¤„ç†

**å…³é”®**ï¼šå¿…é¡»æŒ‰ç…§ vLLM è§„å®šçš„é¡ºåºå¤„ç† BatchUpdateï¼š

```python
# âœ… æ­£ç¡®é¡ºåºï¼ˆæˆ‘ä»¬çš„å®ç°ï¼‰
def update_state(self, batch_update: BatchUpdate):
    # Step 1: å…ˆå¤„ç† removed - æ¸…ç†æ—§è¯·æ±‚ï¼Œé‡Šæ”¾ index
    for index in batch_update.removed:
        self.request_states.pop(index, None)
        self.enabled_requests.discard(index)
        self.alpha_history.pop(index, None)

    # Step 2: å†å¤„ç† added - æ·»åŠ æ–°è¯·æ±‚ï¼Œå¯èƒ½å¤ç”¨åˆšé‡Šæ”¾çš„ index
    for index, params, prompt_toks, output_toks in batch_update.added:
        self.request_states[index] = (...)  # å®‰å…¨ï¼šæ—§æ•°æ®å·²æ¸…ç†

    # Step 3: æœ€åå¤„ç† moved - ç§»åŠ¨/äº¤æ¢è¯·æ±‚
    for adx, bdx, direct in batch_update.moved:
        # ...
```

```python
# âŒ é”™è¯¯é¡ºåºï¼ˆä¼šå¯¼è‡´æ•°æ®æ··æ·†ï¼‰
def update_state(self, batch_update: BatchUpdate):
    # å…ˆå¤„ç† added
    for index, params, ... in batch_update.added:
        self.request_states[index] = (...)  # å±é™©ï¼šå¯èƒ½è¦†ç›–è¿˜æœªæ¸…ç†çš„æ—§æ•°æ®

    # åå¤„ç† removed
    for index in batch_update.removed:
        self.request_states.pop(index, None)  # å¯èƒ½åˆ é™¤åˆšæ·»åŠ çš„æ–°æ•°æ®ï¼
```

#### Index å¤ç”¨ç¤ºä¾‹

```python
# Batch çŠ¶æ€æ¼”åŒ–
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ T0: åˆå§‹çŠ¶æ€ (3 ä¸ªè¯·æ±‚)                           â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Index 0: Request_A (prompt="What is AI?")        â”‚
# â”‚ Index 1: Request_B (prompt="What is ML?")        â”‚
# â”‚ Index 2: Request_C (prompt="What is DL?")        â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ T1: Request_A å®Œæˆ                                â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ BatchUpdate.removed = [0]                        â”‚
# â”‚ â†’ æ¸…ç† Index 0 çš„æ‰€æœ‰æ•°æ®                         â”‚
# â”‚   - request_states.pop(0)                        â”‚
# â”‚   - alpha_history.pop(0)                         â”‚
# â”‚   - ä¿å­˜ alpha history åˆ°æ–‡ä»¶                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ T2: æ–°è¯·æ±‚ Request_D åŠ å…¥ï¼Œå¤ç”¨ Index 0          â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ BatchUpdate.added = [                            â”‚
# â”‚   (0, params_D, prompt_toks_D, output_toks_D)    â”‚
# â”‚ ]                                                â”‚
# â”‚ â†’ å®‰å…¨ï¼šIndex 0 å·²è¢«æ¸…ç†ï¼Œå¯ä»¥å®‰å…¨å¤ç”¨             â”‚
# â”‚   request_states[0] = (prompt_D, output_D, ...)  â”‚
# â”‚   alpha_history[0] = []  # æ–°çš„ alpha å†å²        â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# å½“å‰çŠ¶æ€:
# Index 0: Request_D (prompt="What is RL?")  â† å¤ç”¨
# Index 1: Request_B (prompt="What is ML?")
# Index 2: Request_C (prompt="What is DL?")
```

#### æ½œåœ¨é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

**é—®é¢˜ 1**ï¼šå¦‚æœå¤„ç†é¡ºåºé”™è¯¯ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®æ··æ·†

```python
# âŒ é”™è¯¯åœºæ™¯
# T1: Request_A (index=0) è¿˜åœ¨è¿è¡Œ
# T2: é”™è¯¯åœ°å…ˆå¤„ç† addedï¼Œæ·»åŠ  Request_D åˆ° index 0
#     â†’ è¦†ç›–äº† Request_A çš„æ•°æ®ï¼
# T3: å†å¤„ç† removedï¼Œåˆ é™¤ index 0
#     â†’ Request_D çš„æ•°æ®ä¹Ÿè¢«åˆ é™¤äº†ï¼
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- âœ… ä¸¥æ ¼æŒ‰ç…§ `removed â†’ added â†’ moved` çš„é¡ºåºå¤„ç†
- âœ… åœ¨ `removed` ä¸­å½»åº•æ¸…ç†æ‰€æœ‰ç›¸å…³æ•°æ®
- âœ… åœ¨ `added` ä¸­é‡æ–°åˆå§‹åŒ–æ‰€æœ‰æ•°æ®

**é—®é¢˜ 2**ï¼šAlpha history æ–‡ä»¶åå†²çª

å½“ index è¢«å¤ç”¨æ—¶ï¼Œä¸åŒè¯·æ±‚å¯èƒ½ä½¿ç”¨ç›¸åŒçš„ indexï¼Œå¯¼è‡´æ–‡ä»¶åå†²çªï¼š

```python
# Request_A (index=0) å®Œæˆï¼Œä¿å­˜ alpha_history_0.json
# Request_D (index=0) å®Œæˆï¼Œä¿å­˜ alpha_history_0.json  â† è¦†ç›–ï¼
```

**å½“å‰è§£å†³æ–¹æ¡ˆ**ï¼š
- æ¯æ¬¡ `removed` æ—¶ç«‹å³ä¿å­˜ alpha history
- åœ¨ä¸»è¿›ç¨‹çš„ `generate()` æ–¹æ³•ä¸­ç«‹å³è¯»å–å¹¶æ¸…ç†æ–‡ä»¶
- ä½¿ç”¨ `time.sleep(0.01)` ç¡®ä¿æ–‡ä»¶ I/O å®Œæˆ

**æ›´å¥½çš„æ–¹æ¡ˆ**ï¼ˆæœªæ¥ä¼˜åŒ–ï¼‰ï¼š
- ä½¿ç”¨ UUID æˆ– timestamp ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
- åœ¨ `extra_args` ä¸­ä¼ é€’å”¯ä¸€çš„ request ID

```python
# æ”¹è¿›çš„æ–‡ä»¶å‘½å
alpha_file = f"alpha_history_{request_uuid}_{index}.json"
# æˆ–
alpha_file = f"alpha_history_{timestamp}_{index}.json"
```

### 3. BatchUpdate å’Œ LogitsProcessor ç”Ÿå‘½å‘¨æœŸ

vLLM åœ¨æ¯ä¸ªç”Ÿæˆ step ä¸­è°ƒç”¨ LogitsProcessorï¼š

```python
# æ¯ä¸ª step:
1. update_state(batch_update)  # æ›´æ–°è¯·æ±‚çŠ¶æ€
   â”œâ”€ batch_update.added: [(index, params, prompt_toks, output_toks), ...]
   â”œâ”€ batch_update.removed: [index1, index2, ...]
   â””â”€ batch_update.moved: [(from_idx, to_idx, directionality), ...]

2. apply(logits)  # å¤„ç† logits
   â””â”€ logits shape: [max_num_reqs, vocab_size]

3. Sample token from logits

4. Append token to output_tok_ids (è‡ªåŠ¨)
```

#### BatchUpdate æ•°æ®ç»“æ„

```python
@dataclass(frozen=True)
class BatchUpdate:
    batch_size: int  # å½“å‰ batch ä¸­çš„è¯·æ±‚æ•°

    # æ·»åŠ çš„è¯·æ±‚ï¼š(index, params, prompt_tok_ids, output_tok_ids)
    added: Sequence[AddedRequest]

    # ç§»é™¤çš„è¯·æ±‚ç´¢å¼•
    removed: Sequence[RemovedRequest]

    # ç§»åŠ¨/äº¤æ¢çš„è¯·æ±‚ï¼š(from_idx, to_idx, directionality)
    moved: Sequence[MovedRequest]
```

**é‡è¦**ï¼š`output_tok_ids` æ˜¯ä¸€ä¸ª**å¼•ç”¨**ï¼ˆlist referenceï¼‰ï¼ŒvLLM ä¼šè‡ªåŠ¨åœ¨æ¯ä¸ª step å append æ–° token åˆ°è¿™ä¸ª listã€‚

### 3. SamplingParams.extra_args åœ¨æ‰¹å¤„ç†ä¸­çš„è¡Œä¸º

**å…³é”®å‘ç°**ï¼šåœ¨æ‰¹å¤„ç†ä¸­ï¼Œæ‰€æœ‰è¯·æ±‚**å…±äº«åŒä¸€ä¸ª SamplingParams å¯¹è±¡**ï¼

```python
# optimal_sampling_v1.py çš„ generate() æ–¹æ³•
sampling_params = SamplingParams(
    max_tokens=100,
    temperature=0.8,
    extra_args={
        "theta_model_path": "Qwen/Qwen2.5-1.5B",
        "original_prompts": ["prompt0", "prompt1", "prompt2"],  # â† List
        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        # æ‰€æœ‰è¯·æ±‚å…±äº«è¿™ä¸ª list
    }
)

# vLLM å†…éƒ¨ï¼šæ‰€æœ‰è¯·æ±‚éƒ½å¼•ç”¨åŒä¸€ä¸ª sampling_params
for i, prompt in enumerate(prompts):
    request = Request(
        prompt=prompt,
        sampling_params=sampling_params,  # â† å…±äº«åŒä¸€ä¸ªå¯¹è±¡
        ...
    )
```

**æ¨è®º**ï¼š`extra_args` ä¸­çš„ list å¿…é¡»æŒ‰ **request index** ç´¢å¼•ï¼Œè€Œä¸æ˜¯ç”¨å›ºå®šçš„ `[0]`ã€‚

---

## ğŸ”— ä¸¤ä¸ª vLLM Core çš„å¯¹é½ç­–ç•¥

Optimal Sampling V1 ä½¿ç”¨åµŒå¥—çš„ vLLM æ¶æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Outer vLLM (Teacher Model Ï€_t, å¤§æ¨¡å‹)               â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ EngineCore (subprocess)                â”‚         â”‚
â”‚  â”‚                                        â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚
â”‚  â”‚  â”‚ OptimalSamplingLogitsProcessor  â”‚  â”‚         â”‚
â”‚  â”‚  â”‚                                  â”‚  â”‚         â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚         â”‚
â”‚  â”‚  â”‚  â”‚ Inner vLLM (Theta Ï€_Î¸)     â”‚  â”‚  â”‚         â”‚
â”‚  â”‚  â”‚  â”‚ å°æ¨¡å‹ï¼Œè·å– theta logits   â”‚  â”‚  â”‚         â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¯¹é½ç‚¹ 1: Prompt å¯¹é½ï¼ˆSemi On-Policy Distillationï¼‰

**ç›®æ ‡**ï¼šTeacher å’Œ Theta çœ‹åˆ°**ä¸åŒçš„åˆå§‹ prompt**ï¼Œä½†**ç›¸åŒçš„ç”Ÿæˆåºåˆ—**ã€‚

#### åœºæ™¯ï¼šéšå¼çŸ¥è¯†è’¸é¦

```python
# ç¤ºä¾‹ï¼šéšå¼çŸ¥è¯†è’¸é¦
teacher_prompt = "Problem: 2x+3=7\nAnswer: x=2\nReasoning:"
theta_prompt   = "Problem: 2x+3=7\nReasoning:"
                 # â†‘ Theta çœ‹ä¸åˆ°ç­”æ¡ˆ

# Teacher tokenization:
teacher_tokens = tokenizer.encode(teacher_prompt)
# â†’ [1, 2, 3, 4, 5, 6, 7, 8, 9]  (å‡è®¾)

# Theta tokenization (å•ç‹¬è¿›è¡Œ):
theta_tokens = tokenizer.encode(theta_prompt)
# â†’ [1, 2, 3, 4, 10]  (ä¸åŒï¼å› ä¸ºæ²¡æœ‰ "Answer: x=2" éƒ¨åˆ†)

# ç¬¬ä¸€ä¸ªç”Ÿæˆ token: 11
# Teacher çœ‹åˆ°: [1,2,3,4,5,6,7,8,9, 11]
#               ^^^^^^^^^^^^^^^^^ ^
#               teacher prompt    ç”Ÿæˆ
#
# Theta çœ‹åˆ°:   [1,2,3,4,10, 11]
#               ^^^^^^^^^ ^
#               theta prompt ç”Ÿæˆ
#               â†‘ ä¸åŒå‰ç¼€ï¼Œç›¸åŒåç¼€
```

#### å®ç°ï¼ˆåœ¨ `guide_model_v1.py`ï¼‰

```python
def get_logits_for_requests(self, request_data: Dict[int, Dict]) -> Dict[int, torch.Tensor]:
    prompts = []
    for idx in indices:
        if request_data[idx].get("original_prompt"):
            # 1. æå– teacher å·²ç”Ÿæˆçš„ output tokens
            teacher_prompt_len = request_data[idx]["teacher_prompt_len"]
            full_sequence = request_data[idx]["token_ids"]
            output_tokens = full_sequence[teacher_prompt_len:]

            # 2. å•ç‹¬ tokenize theta çš„ prompt
            original_prompt = request_data[idx]["original_prompt"]

            if self.system_prompt:
                theta_prompt_text = f"{self.system_prompt}\n\n{original_prompt}"
            else:
                theta_prompt_text = original_prompt

            if self.enable_chat_template:
                messages = []
                if self.system_prompt:
                    messages.append({"role": "system", "content": self.system_prompt})
                if original_prompt:
                    messages.append({"role": "user", "content": original_prompt})
                theta_prompt_text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )

            # 3. Tokenize theta's prompt
            theta_prompt_tokens = self.tokenizer.encode(theta_prompt_text, add_special_tokens=False)

            # 4. ç»„åˆï¼štheta çš„ prompt + å…±äº«çš„ output tokens
            theta_full_sequence = theta_prompt_tokens + output_tokens
            #                     ^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^
            #                     theta è‡ªå·±çš„ prompt    teacher ç”Ÿæˆçš„

            prompts.append({"prompt_token_ids": theta_full_sequence})
        else:
            # Fallback: ä½¿ç”¨ teacher çš„ tokensï¼ˆå‘åå…¼å®¹ï¼‰
            prompts.append({"prompt_token_ids": request_data[idx]["token_ids"]})

    # è·å– theta logits
    outputs = self.llm.generate(prompts=prompts, sampling_params=...)
    return result
```

### å¯¹é½ç‚¹ 2: Request Index å¯¹é½

**å…³é”®**ï¼šTeacher çš„ request index å¿…é¡»ä¸ Theta çš„ request index ä¸€è‡´ã€‚

```python
# logits_processor_v1.py

# update_state() ä¸­ï¼š
for index, params, prompt_tok_ids, output_tok_ids in batch_update.added:
    # âœ… ç”¨ request index è·å–å¯¹åº”çš„ prompt
    original_prompts_list = params.extra_args.get("original_prompts", [])
    original_prompt = original_prompts_list[index]  # â† å…³é”®ï¼
    #                                      ^^^^^

    self.request_states[index] = (prompt_tok_ids, output_tok_ids, params, original_prompt)

# apply() ä¸­ï¼š
request_data = {}
for idx in self.enabled_requests:
    if idx in self.request_states:
        prompt_tok_ids, output_tok_ids, params, original_prompt = self.request_states[idx]
        request_data[idx] = {
            "token_ids": full_sequence,
            "original_prompt": original_prompt,  # â† æ­£ç¡®çš„ prompt
            "teacher_prompt_len": len(prompt_tok_ids)
        }

# è·å– theta logits
theta_logits_dict = self.theta_model.get_logits_for_requests(request_data)
# â†’ è¿”å› {idx: logits} æ˜ å°„

# æ··åˆ logits
for request_idx, theta_logits in theta_logits_dict.items():
    # âœ… ç”¨ç›¸åŒçš„ request_idx è®¿é—® teacher logits å’Œæ›´æ–° logits
    logits_t = logits[request_idx]
    # ... compute mixed_logits ...
    logits[request_idx] = mixed_logits
```

### å¯¹é½ç‚¹ 3: Output Tokens å¯¹é½ï¼ˆè‡ªåŠ¨å¯¹é½ï¼‰

vLLM é€šè¿‡**å¼•ç”¨ä¼ é€’** `output_tok_ids` å®ç°è‡ªåŠ¨åŒæ­¥ï¼š

```python
# BatchUpdate.added ä¸­:
for index, params, prompt_tok_ids, output_tok_ids in batch_update.added:
    #                                ^^^^^^^^^^^^^^
    #                                è¿™æ˜¯ä¸€ä¸ª list referenceï¼

    self.request_states[index] = (prompt_tok_ids, output_tok_ids, params, original_prompt)
    #                                              ^^^^^^^^^^^^^^
    #                                              ä¿å­˜å¼•ç”¨

# æ¯æ¬¡ apply() æ—¶ï¼š
prompt_tok_ids, output_tok_ids, params, original_prompt = self.request_states[idx]
full_sequence = prompt_tok_ids + output_tok_ids.copy()
#                                ^^^^^^^^^^^^^^
#                                è¿™ä¸ª list å·²ç»è¢« vLLM è‡ªåŠ¨æ›´æ–°äº†ï¼

# vLLM åœ¨æ¯ä¸ª step åè‡ªåŠ¨ append æ–° tokenï¼š
# Step 1: output_tok_ids = []
# Step 2: output_tok_ids = [token_1]  â† vLLM è‡ªåŠ¨ append
# Step 3: output_tok_ids = [token_1, token_2]  â† vLLM è‡ªåŠ¨ append
# ...
```

---

## âœ… å®Œæ•´çš„å¯¹é½æµç¨‹ç¤ºä¾‹

```python
# Step 1: åˆå§‹åŒ–
sampler = OptimalSamplingV1(
    model_teacher="Qwen/Qwen2.5-3B",  # Outer vLLM
    model_theta="Qwen/Qwen2.5-1.5B",  # Inner vLLM
    alpha_method="kl_symmetry"
)

# Step 2: æ‰¹é‡ç”Ÿæˆï¼ˆä¸¤ä¸ªä¸åŒçš„é—®é¢˜ï¼‰
outputs = sampler.generate(
    prompts=[
        "Problem: X\nAnswer: A\nReason:",  # Teacher prompt (index=0)
        "Problem: Y\nAnswer: B\nReason:"   # Teacher prompt (index=1)
    ],
    theta_prompts=[
        "Problem: X\nReason:",  # Theta prompt (index=0) - æ²¡æœ‰ç­”æ¡ˆ
        "Problem: Y\nReason:"   # Theta prompt (index=1) - æ²¡æœ‰ç­”æ¡ˆ
    ],
    max_tokens=100,
    temperature=0.8
)

# Step 3: vLLM å†…éƒ¨æµç¨‹ï¼ˆæ¯ä¸ªç”Ÿæˆ stepï¼‰
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Generation Loop (é‡å¤ max_tokens æ¬¡)                 â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 1. update_state(batch_update) - åªåœ¨ step 0 è°ƒç”¨     â”‚
# â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
# â”‚    â”‚ Request 0:                                 â”‚    â”‚
# â”‚    â”‚  - teacher_prompt: "Problem: X\nAnswer:..."â”‚    â”‚
# â”‚    â”‚  - theta_prompt: "Problem: X\nReason:"     â”‚    â”‚
# â”‚    â”‚  - output_toks: [] (å¼•ç”¨)                  â”‚    â”‚
# â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
# â”‚    â”‚ Request 1:                                 â”‚    â”‚
# â”‚    â”‚  - teacher_prompt: "Problem: Y\nAnswer:..."â”‚    â”‚
# â”‚    â”‚  - theta_prompt: "Problem: Y\nReason:"     â”‚    â”‚
# â”‚    â”‚  - output_toks: [] (å¼•ç”¨)                  â”‚    â”‚
# â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 2. apply(logits) - æ¯ä¸ª step éƒ½è°ƒç”¨                   â”‚
# â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
# â”‚    â”‚ A. æ„å»º request_data                       â”‚    â”‚
# â”‚    â”‚    Request 0: {                            â”‚    â”‚
# â”‚    â”‚      "token_ids": teacher_prompt_toks +    â”‚    â”‚
# â”‚    â”‚                   output_toks,             â”‚    â”‚
# â”‚    â”‚      "original_prompt": "Problem: X...",   â”‚    â”‚
# â”‚    â”‚      "teacher_prompt_len": len(...)        â”‚    â”‚
# â”‚    â”‚    }                                       â”‚    â”‚
# â”‚    â”‚    Request 1: { ... }                      â”‚    â”‚
# â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
# â”‚    â”‚ B. è·å– theta logits                       â”‚    â”‚
# â”‚    â”‚    theta_model.get_logits_for_requests()   â”‚    â”‚
# â”‚    â”‚    â†’ å•ç‹¬ tokenize theta prompt            â”‚    â”‚
# â”‚    â”‚    â†’ ç»„åˆ theta_prompt_toks + output_toks  â”‚    â”‚
# â”‚    â”‚    â†’ è·å– theta logits                     â”‚    â”‚
# â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
# â”‚    â”‚ C. æ··åˆ logits                             â”‚    â”‚
# â”‚    â”‚    for req_idx in [0, 1]:                  â”‚    â”‚
# â”‚    â”‚      logits_t = logits[req_idx]            â”‚    â”‚
# â”‚    â”‚      logits_theta = theta_logits[req_idx]  â”‚    â”‚
# â”‚    â”‚      alpha = compute_alpha(...)            â”‚    â”‚
# â”‚    â”‚      q_star = mix(logits_t, logits_theta)  â”‚    â”‚
# â”‚    â”‚      logits[req_idx] = q_star              â”‚    â”‚
# â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 3. Sample token from mixed logits                    â”‚
# â”‚    - Request 0: sample token_0                       â”‚
# â”‚    - Request 1: sample token_1                       â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 4. Append token to output_tok_ids (vLLM è‡ªåŠ¨)        â”‚
# â”‚    - Request 0: output_toks[0].append(token_0)       â”‚
# â”‚    - Request 1: output_toks[1].append(token_1)       â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# Repeat steps 2-4 until max_tokens or EOS
```

### å…³é”®æ—¶åˆ»çš„æ•°æ®çŠ¶æ€

å‡è®¾ä¸¤ä¸ªé—®é¢˜ï¼Œç”Ÿæˆ 3 ä¸ª tokenï¼š

```python
# Initial state (Step 0)
Request 0:
  teacher_prompt_toks: [1, 2, 3, 4, 5]  # "Problem: X\nAnswer: A\nReason:"
  theta_prompt_toks:   [1, 2, 3, 6]     # "Problem: X\nReason:"
  output_toks: []

Request 1:
  teacher_prompt_toks: [10, 11, 12, 13, 14]  # "Problem: Y\nAnswer: B\nReason:"
  theta_prompt_toks:   [10, 11, 12, 15]      # "Problem: Y\nReason:"
  output_toks: []

# Step 1: Generate first token
apply() sees:
  Request 0:
    teacher: [1,2,3,4,5] + [] = [1,2,3,4,5]
    theta:   [1,2,3,6] + []   = [1,2,3,6]
  Request 1:
    teacher: [10,11,12,13,14] + [] = [10,11,12,13,14]
    theta:   [10,11,12,15] + []    = [10,11,12,15]

Sampled: token_0=20, token_1=30
vLLM appends: output_toks[0] = [20], output_toks[1] = [30]

# Step 2: Generate second token
apply() sees:
  Request 0:
    teacher: [1,2,3,4,5] + [20] = [1,2,3,4,5,20]
    theta:   [1,2,3,6] + [20]   = [1,2,3,6,20]
    #                     ^^^^
    #                     å…±äº«çš„ç”Ÿæˆåºåˆ—
  Request 1:
    teacher: [10,11,12,13,14] + [30] = [10,11,12,13,14,30]
    theta:   [10,11,12,15] + [30]    = [10,11,12,15,30]
    #                         ^^^^
    #                         å…±äº«çš„ç”Ÿæˆåºåˆ—

Sampled: token_0=21, token_1=31
vLLM appends: output_toks[0] = [20,21], output_toks[1] = [30,31]

# Step 3: Generate third token
apply() sees:
  Request 0:
    teacher: [1,2,3,4,5] + [20,21] = [1,2,3,4,5,20,21]
    theta:   [1,2,3,6] + [20,21]   = [1,2,3,6,20,21]
  Request 1:
    teacher: [10,11,12,13,14] + [30,31] = [10,11,12,13,14,30,31]
    theta:   [10,11,12,15] + [30,31]    = [10,11,12,15,30,31]

# ...ç»§ç»­ç›´åˆ° max_tokens æˆ– EOS
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- Teacher å’Œ Theta çš„**åˆå§‹ prompt ä¸åŒ**ï¼ˆå®ç°éšå¼çŸ¥è¯†è’¸é¦ï¼‰
- ä½†ç”Ÿæˆçš„ **output tokens å®Œå…¨ç›¸åŒ**ï¼ˆæ¥è‡ªæ··åˆåˆ†å¸ƒ q*ï¼‰
- Request 0 å’Œ Request 1 çš„æ•°æ®**å®Œå…¨ç‹¬ç«‹**ï¼ˆé€šè¿‡ request index åŒºåˆ†ï¼‰

---

## ğŸ“ å…³é”®è¦ç‚¹æ€»ç»“

### 1. vLLM Logits Tensor æ˜¯å›ºå®šå¤§å°çš„

```python
# âœ… æ­£ç¡®
logits.shape = [max_num_reqs, vocab_size]  # å›ºå®šå¤§å°
logits[request_idx] = mixed_logits          # ç›´æ¥ç”¨ request index

# âŒ é”™è¯¯
logits.shape = [current_batch_size, vocab_size]  # åŠ¨æ€å¤§å°
batch_idx = map_request_to_batch[request_idx]   # ä¸éœ€è¦æ˜ å°„
```

### 2. Index å¤ç”¨æœºåˆ¶å¿…é¡»æ­£ç¡®å¤„ç†âš ï¸

**å…³é”®è§„åˆ™**ï¼šä¸¥æ ¼æŒ‰ç…§ `removed â†’ added â†’ moved` çš„é¡ºåºå¤„ç† BatchUpdate

```python
# âœ… æ­£ç¡®
def update_state(self, batch_update):
    # 1. å…ˆæ¸…ç†å·²å®Œæˆçš„è¯·æ±‚ï¼ˆé‡Šæ”¾ indexï¼‰
    for index in batch_update.removed:
        self.request_states.pop(index, None)
        self.alpha_history.pop(index, None)

    # 2. å†æ·»åŠ æ–°è¯·æ±‚ï¼ˆå¯èƒ½å¤ç”¨åˆšé‡Šæ”¾çš„ indexï¼‰
    for index, params, ... in batch_update.added:
        self.request_states[index] = (...)
        self.alpha_history[index] = []

    # 3. æœ€åå¤„ç†ç§»åŠ¨/äº¤æ¢
    for adx, bdx, direct in batch_update.moved:
        # ...

# âŒ é”™è¯¯ï¼šé¡ºåºé”™è¯¯ä¼šå¯¼è‡´æ•°æ®æ··æ·†
```

**ä¸ºä»€ä¹ˆé‡è¦**ï¼š
- å½“å¹¶å‘è¯·æ±‚æ•° > `max_num_reqs` æ—¶ï¼ŒvLLM ä¼šå¤ç”¨ index
- é”™è¯¯çš„é¡ºåºå¯èƒ½å¯¼è‡´æ–°æ—§è¯·æ±‚æ•°æ®æ··æ·†
- Alpha history æ–‡ä»¶å¯èƒ½è¢«è¦†ç›–

### 3. æ‰¹å¤„ç†ä¸­æ‰€æœ‰è¯·æ±‚å…±äº« SamplingParams

```python
# SamplingParams æ˜¯å…±äº«çš„
sampling_params = SamplingParams(
    extra_args={
        "original_prompts": ["p0", "p1", "p2"]  # List for all requests
    }
)

# åœ¨ LogitsProcessor ä¸­ï¼Œå¿…é¡»æ ¹æ® request index ç´¢å¼•
original_prompt = original_prompts_list[index]  # âœ… æ­£ç¡®
original_prompt = original_prompts_list[0]      # âŒ é”™è¯¯
```

### 3. Request Index æ˜¯å…³é”®

Request index ç”¨äºï¼š
- ç´¢å¼• logits tensor: `logits[request_idx]`
- ç´¢å¼• original_prompts: `original_prompts[request_idx]`
- ç´¢å¼• alpha_history: `alpha_history[request_idx]`
- ç´¢å¼• request_states: `request_states[request_idx]`

### 4. Output Tokens è‡ªåŠ¨åŒæ­¥

vLLM é€šè¿‡å¼•ç”¨ä¼ é€’ `output_tok_ids`ï¼Œæ— éœ€æ‰‹åŠ¨åŒæ­¥ï¼š

```python
# BatchUpdate.added:
for index, params, prompt_tok_ids, output_tok_ids in batch_update.added:
    self.request_states[index] = (..., output_tok_ids, ...)
    #                                  ^^^^^^^^^^^^^^
    #                                  è¿™æ˜¯ä¸€ä¸ª list reference

# apply() ä¸­è‡ªåŠ¨è·å–æœ€æ–°çš„ output_tok_ids
_, output_tok_ids, _, _ = self.request_states[idx]
# output_tok_ids å·²ç»åŒ…å«æ‰€æœ‰å·²ç”Ÿæˆçš„ tokensï¼ˆvLLM è‡ªåŠ¨ appendï¼‰
```

### 5. ä¸¤ä¸ª vLLM Core çš„å¯¹é½

| ç»´åº¦ | Teacher (Outer) | Theta (Inner) | å¯¹é½æ–¹å¼ |
|------|----------------|---------------|---------|
| **Prompt** | å®Œæ•´ promptï¼ˆå«ç­”æ¡ˆï¼‰ | éƒ¨åˆ† promptï¼ˆæ— ç­”æ¡ˆï¼‰ | åˆ†åˆ« tokenize |
| **Output Tokens** | ç›¸åŒ | ç›¸åŒ | å¼•ç”¨ä¼ é€’è‡ªåŠ¨åŒæ­¥ |
| **Request Index** | ç›¸åŒ | ç›¸åŒ | ç›´æ¥ä¼ é€’ |
| **Logits** | Teacher logits | Theta logits | æŒ‰ request_idx æ··åˆ |

### 6. è°ƒè¯•æŠ€å·§

```python
# åœ¨ LogitsProcessor ä¸­æ‰“å°åˆ° stderrï¼ˆsubprocess å¯è§ï¼‰
import sys

# 1. æ£€æŸ¥ request index å’Œ original_prompt çš„å¯¹åº”å…³ç³»
def update_state(self, batch_update):
    if batch_update:
        print(f"BatchUpdate: added={len(batch_update.added)}, "
              f"removed={len(batch_update.removed)}, "
              f"moved={len(batch_update.moved)}",
              file=sys.stderr, flush=True)

        for index in batch_update.removed:
            print(f"  Removing request {index}", file=sys.stderr, flush=True)

        for index, params, _, _ in batch_update.added:
            original_prompts = params.extra_args.get("original_prompts", [])
            prompt = original_prompts[index] if index < len(original_prompts) else "N/A"
            print(f"  Adding request {index}: prompt='{prompt[:30]}...'",
                  file=sys.stderr, flush=True)

# 2. æ£€æŸ¥ index å¤ç”¨
def apply(self, logits):
    print(f"apply() called: enabled_requests={sorted(self.enabled_requests)}",
          file=sys.stderr, flush=True)

    for idx in self.enabled_requests:
        if idx in self.request_states:
            _, output_toks, _, original_prompt = self.request_states[idx]
            print(f"  Request {idx}: {len(output_toks)} tokens, "
                  f"prompt='{original_prompt[:20]}...'",
                  file=sys.stderr, flush=True)

# 3. æ£€æŸ¥ logits tensor å¤§å°
def apply(self, logits):
    print(f"logits.shape = {logits.shape}, "
          f"max_num_reqs = {logits.shape[0]}, "
          f"enabled = {len(self.enabled_requests)}",
          file=sys.stderr, flush=True)

# 4. æ£€æµ‹ index å¤ç”¨å†²çª
def update_state(self, batch_update):
    if batch_update:
        # æ£€æµ‹æ˜¯å¦æœ‰ index åŒæ—¶å‡ºç°åœ¨ removed å’Œ added ä¸­ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
        removed_set = set(batch_update.removed)
        added_indices = {idx for idx, _, _, _ in batch_update.added}
        reused = removed_set & added_indices
        if reused:
            print(f"  Index reuse detected: {sorted(reused)}",
                  file=sys.stderr, flush=True)

# 5. Alpha history æ–‡ä»¶å†²çªæ£€æµ‹
def update_state(self, batch_update):
    if batch_update and self.alpha_storage_dir:
        for index in batch_update.removed:
            alpha_file = Path(self.alpha_storage_dir) / f"alpha_history_{index}.json"
            if alpha_file.exists():
                print(f"  Warning: alpha_history_{index}.json already exists, "
                      f"will be overwritten on next reuse",
                      file=sys.stderr, flush=True)
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [SUMMARY.md](SUMMARY.md) - Optimal Sampling V1 æ€»ä½“æ¶æ„
- [distillation_guide.md](distillation_guide.md) - çŸ¥è¯†è’¸é¦ä½¿ç”¨æŒ‡å—
- [BENCHMARK_REPORT.md](BENCHMARK_REPORT.md) - æ€§èƒ½åŸºå‡†æµ‹è¯•

---

## ğŸ“ é—®é¢˜åé¦ˆ

å¦‚æœåœ¨æ‰¹å¤„ç†ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. âœ… æ˜¯å¦ä½¿ç”¨ `original_prompts[index]` è€Œä¸æ˜¯ `original_prompts[0]`
2. âœ… æ˜¯å¦ç›´æ¥ç”¨ `logits[request_idx]` è®¿é—® logits
3. âœ… æ˜¯å¦åœ¨ `guide_model_v1.py` ä¸­å•ç‹¬ tokenize theta prompt
4. âœ… æ˜¯å¦æ­£ç¡®å¤„ç† `output_tok_ids` çš„å¼•ç”¨ä¼ é€’

å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æºä»£ç æ³¨é‡Šæˆ–æäº¤ issueã€‚
