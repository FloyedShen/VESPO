#!/usr/bin/env python3
"""
æ­£ç¡®æ€§æµ‹è¯•ï¼šéªŒè¯ Optimal Sampling ç”Ÿæˆçš„è´¨é‡

æµ‹è¯•å†…å®¹:
1. åŸºç¡€ç”Ÿæˆæµ‹è¯• - ç¡®ä¿èƒ½ç”Ÿæˆåˆç†æ–‡æœ¬
2. æ•°å­¦æ¨ç†æµ‹è¯• - éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§
3. æ¸©åº¦ä¸€è‡´æ€§æµ‹è¯• - éªŒè¯ teacher å’Œ theta ä½¿ç”¨ç›¸åŒæ¸©åº¦
4. Alpha åˆç†æ€§æµ‹è¯• - éªŒè¯ alpha åœ¨åˆç†èŒƒå›´å†…
5. ä¸åŒå‰ç¼€æµ‹è¯• - éªŒè¯ teacher å’Œ student æ¥æ”¶ä¸åŒè¾“å…¥
"""

import sys
import re
from optimal_sampling import OptimalSamplingV1


def test_basic_generation():
    """æµ‹è¯• 1: åŸºç¡€ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 1: åŸºç¡€ç”ŸæˆåŠŸèƒ½")
    print("=" * 80)

    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=False,
    )

    prompts = ["Say hello in 5 words:"]

    outputs = sampler.generate(
        prompts=prompts,
        max_tokens=20,
        temperature=0.8,
        use_optimal_sampling=True
    )

    text = outputs.generated_texts[0]
    print(f"\nç”Ÿæˆæ–‡æœ¬: '{text}'")

    # éªŒè¯
    assert len(text) > 0, "ç”Ÿæˆæ–‡æœ¬ä¸ºç©º"
    assert len(text.split()) <= 25, "ç”Ÿæˆæ–‡æœ¬è¿‡é•¿"

    print("âœ… åŸºç¡€ç”Ÿæˆæµ‹è¯•é€šè¿‡")
    return True


def test_math_reasoning():
    """æµ‹è¯• 2: æ•°å­¦æ¨ç†æ­£ç¡®æ€§"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: æ•°å­¦æ¨ç†æ­£ç¡®æ€§")
    print("=" * 80)

    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=False,
    )

    # ç®€å•æ•°å­¦é—®é¢˜
    problems = [
        {
            "question": "What is 2 + 2?",
            "answer": "4",
            "teacher_prompt": "Problem: What is 2 + 2?\nAnswer: 4\nExplain:",
            "student_prompt": "Problem: What is 2 + 2?\nSolve:",
        },
        {
            "question": "What is 3 Ã— 5?",
            "answer": "15",
            "teacher_prompt": "Problem: What is 3 Ã— 5?\nAnswer: 15\nExplain:",
            "student_prompt": "Problem: What is 3 Ã— 5?\nSolve:",
        },
    ]

    for i, prob in enumerate(problems):
        print(f"\né—®é¢˜ {i+1}: {prob['question']}")

        outputs = sampler.generate(
            prompts=[prob["teacher_prompt"]],
            theta_prompts=[prob["student_prompt"]],
            max_tokens=100,
            temperature=0.7,
            use_optimal_sampling=True
        )

        text = outputs.generated_texts[0]
        print(f"ç”Ÿæˆ: {text[:200]}...")

        # éªŒè¯ç­”æ¡ˆå‡ºç°åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­
        answer_found = prob["answer"] in text
        print(f"ç­”æ¡ˆ '{prob['answer']}' æ˜¯å¦å‡ºç°: {answer_found}")

        if not answer_found:
            print(f"âš ï¸ è­¦å‘Š: ç­”æ¡ˆæœªå‡ºç°åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­")
        else:
            print(f"âœ… ç­”æ¡ˆæ­£ç¡®å‡ºç°")

    print("\nâœ… æ•°å­¦æ¨ç†æµ‹è¯•å®Œæˆ")
    return True


def test_temperature_consistency():
    """æµ‹è¯• 3: æ¸©åº¦å‚æ•°ä¸€è‡´æ€§"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: æ¸©åº¦å‚æ•°ä¸€è‡´æ€§")
    print("=" * 80)

    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=True,  # å¼€å¯è¿½è¸ªä»¥éªŒè¯
    )

    prompts = ["Count from 1 to 10:"]

    # æµ‹è¯•ä¸åŒæ¸©åº¦
    for temp in [0.5, 0.8, 1.0]:
        print(f"\næµ‹è¯•æ¸©åº¦: {temp}")

        outputs = sampler.generate(
            prompts=prompts,
            max_tokens=30,
            temperature=temp,
            use_optimal_sampling=True
        )

        text = outputs.generated_texts[0]
        print(f"ç”Ÿæˆ (temp={temp}): {text[:100]}...")

        # ä½æ¸©åº¦åº”è¯¥æ›´ç¡®å®šæ€§
        if temp == 0.5:
            low_temp_text = text
        elif temp == 1.0:
            high_temp_text = text
            # é«˜æ¸©åº¦åº”è¯¥ä¸ä½æ¸©åº¦æœ‰å·®å¼‚ï¼ˆé€šå¸¸ï¼‰
            if low_temp_text == high_temp_text:
                print("âš ï¸ æ³¨æ„: ä¸åŒæ¸©åº¦ç”Ÿæˆç›¸åŒæ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯ç¡®å®šæ€§é—®é¢˜ï¼‰")

    print("\nâœ… æ¸©åº¦ä¸€è‡´æ€§æµ‹è¯•å®Œæˆ")
    return True


def test_different_prompts():
    """æµ‹è¯• 4: Teacher å’Œ Student ä¸åŒå‰ç¼€"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 4: Teacher å’Œ Student æ¥æ”¶ä¸åŒè¾“å…¥")
    print("=" * 80)

    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=False,
    )

    # Teacher çœ‹åˆ°ç­”æ¡ˆï¼ŒStudent ä¸çœ‹
    teacher_prompt = "Problem: Solve 2x + 3 = 7\nAnswer: x = 2\nReasoning:"
    student_prompt = "Problem: Solve 2x + 3 = 7\nReasoning:"

    print(f"\nTeacher prompt: {teacher_prompt}")
    print(f"Student prompt: {student_prompt}")

    outputs = sampler.generate(
        prompts=[teacher_prompt],
        theta_prompts=[student_prompt],
        max_tokens=150,
        temperature=0.8,
        use_optimal_sampling=True
    )

    text = outputs.generated_texts[0]
    print(f"\nç”Ÿæˆæ¨ç†: {text[:300]}...")

    # éªŒè¯ç”ŸæˆåŒ…å«æ¨ç†æ­¥éª¤
    has_reasoning = any(keyword in text.lower() for keyword in
                       ["step", "first", "then", "solve", "subtract", "divide"])

    print(f"\nåŒ…å«æ¨ç†å…³é”®è¯: {has_reasoning}")

    if has_reasoning:
        print("âœ… æˆåŠŸç”Ÿæˆæ¨ç†è¿‡ç¨‹")
    else:
        print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ¨ç†æ­¥éª¤")

    print("\nâœ… ä¸åŒå‰ç¼€æµ‹è¯•å®Œæˆ")
    return True


def test_alpha_values():
    """æµ‹è¯• 5: Alpha å€¼åˆç†æ€§"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 5: Alpha å€¼åˆç†æ€§")
    print("=" * 80)

    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=True,
    )

    prompts = ["Write a short poem about AI:"]

    outputs = sampler.generate(
        prompts=prompts,
        max_tokens=50,
        temperature=0.8,
        use_optimal_sampling=True
    )

    # è·å– alpha ç»Ÿè®¡
    alpha_stats = outputs.alpha_stats[0] if outputs.alpha_stats else None

    if alpha_stats:
        print(f"\nAlpha ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {alpha_stats['mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {alpha_stats['std']:.4f}")
        print(f"  æœ€å°å€¼: {alpha_stats['min']:.4f}")
        print(f"  æœ€å¤§å€¼: {alpha_stats['max']:.4f}")
        print(f"  æ ·æœ¬æ•°: {alpha_stats['count']}")

        # éªŒè¯ alpha åœ¨åˆç†èŒƒå›´
        assert 0 <= alpha_stats['mean'] <= 1, "Alpha å¹³å‡å€¼è¶…å‡º [0,1] èŒƒå›´"
        assert 0 <= alpha_stats['min'] <= 1, "Alpha æœ€å°å€¼è¶…å‡º [0,1] èŒƒå›´"
        assert 0 <= alpha_stats['max'] <= 1, "Alpha æœ€å¤§å€¼è¶…å‡º [0,1] èŒƒå›´"

        # KL å¯¹ç§°æ€§åº”è¯¥è®© alpha åœ¨ 0.5 é™„è¿‘ï¼ˆå¯¹äºç›¸ä¼¼æ¨¡å‹ï¼‰
        if 0.3 < alpha_stats['mean'] < 0.7:
            print("âœ… Alpha åœ¨åˆç†èŒƒå›´å†… (0.3-0.7)")
        else:
            print(f"âš ï¸ Alpha åç¦»ä¸­å¿ƒå€¼: {alpha_stats['mean']:.4f}")
    else:
        print("âš ï¸ æœªè·å–åˆ° Alpha ç»Ÿè®¡")

    print("\nâœ… Alpha å€¼æµ‹è¯•å®Œæˆ")
    return True


def test_batch_processing():
    """æµ‹è¯• 6: æ‰¹é‡å¤„ç†"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 6: æ‰¹é‡å¤„ç†")
    print("=" * 80)

    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=False,
    )

    # æ‰¹é‡å¤„ç†
    batch_size = 4
    prompts = [f"Count from 1 to {i+3}:" for i in range(batch_size)]

    print(f"\næ‰¹é‡å¤§å°: {batch_size}")

    outputs = sampler.generate(
        prompts=prompts,
        max_tokens=30,
        temperature=0.8,
        use_optimal_sampling=True
    )

    print(f"\nç”Ÿæˆç»“æœæ•°é‡: {len(outputs.generated_texts)}")
    assert len(outputs.generated_texts) == batch_size, "ç”Ÿæˆæ•°é‡ä¸åŒ¹é…"

    for i, text in enumerate(outputs.generated_texts):
        print(f"\nè¯·æ±‚ {i+1}: {text[:80]}...")

    print("\nâœ… æ‰¹é‡å¤„ç†æµ‹è¯•é€šè¿‡")
    return True


def test_baseline_comparison():
    """æµ‹è¯• 7: Optimal vs Teacher-only å¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 7: Optimal Sampling vs Teacher-only å¯¹æ¯”")
    print("=" * 80)

    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=False,
    )

    prompt = "Explain machine learning in simple terms:"

    # Optimal Sampling
    print("\n[Optimal Sampling]")
    outputs_optimal = sampler.generate(
        prompts=[prompt],
        max_tokens=80,
        temperature=0.8,
        use_optimal_sampling=True
    )
    optimal_text = outputs_optimal.generated_texts[0]
    print(f"ç”Ÿæˆ: {optimal_text[:200]}...")

    # Teacher-only
    print("\n[Teacher-only Baseline]")
    outputs_baseline = sampler.generate(
        prompts=[prompt],
        max_tokens=80,
        temperature=0.8,
        use_optimal_sampling=False
    )
    baseline_text = outputs_baseline.generated_texts[0]
    print(f"ç”Ÿæˆ: {baseline_text[:200]}...")

    # æ¯”è¾ƒ
    print(f"\næ–‡æœ¬é•¿åº¦å¯¹æ¯”:")
    print(f"  Optimal: {len(optimal_text)} å­—ç¬¦")
    print(f"  Baseline: {len(baseline_text)} å­—ç¬¦")

    # ä¸¤è€…åº”è¯¥éƒ½èƒ½ç”Ÿæˆåˆç†æ–‡æœ¬
    assert len(optimal_text) > 10, "Optimal ç”Ÿæˆè¿‡çŸ­"
    assert len(baseline_text) > 10, "Baseline ç”Ÿæˆè¿‡çŸ­"

    print("\nâœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ")
    return True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸ§ª Optimal Sampling æ­£ç¡®æ€§æµ‹è¯•å¥—ä»¶")
    print("=" * 80)

    tests = [
        ("åŸºç¡€ç”Ÿæˆ", test_basic_generation),
        ("æ•°å­¦æ¨ç†", test_math_reasoning),
        ("æ¸©åº¦ä¸€è‡´æ€§", test_temperature_consistency),
        ("ä¸åŒå‰ç¼€", test_different_prompts),
        ("Alpha å€¼", test_alpha_values),
        ("æ‰¹é‡å¤„ç†", test_batch_processing),
        ("å¯¹æ¯”æµ‹è¯•", test_baseline_comparison),
    ]

    passed = 0
    failed = 0

    for name, test_func in tests:
        try:
            test_func()
            passed += 1
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {name}")
            print(f"é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            failed += 1

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print(f"é€šè¿‡: {passed}/{len(tests)}")
    print(f"å¤±è´¥: {failed}/{len(tests)}")

    if failed == 0:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print(f"\nâŒ {failed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == '__main__':
    exit(main())
