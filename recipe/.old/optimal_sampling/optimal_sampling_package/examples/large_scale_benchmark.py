#!/usr/bin/env python3
"""
å¤§è§„æ¨¡æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼šåºåˆ—é•¿åº¦ vs æ‰¹é‡å¤§å°

æµ‹è¯•çŸ©é˜µ:
- åºåˆ—é•¿åº¦ (max_tokens): [1k, 2k, 4k, 8k]
- æ‰¹é‡å¤§å° (batch_size): [1, 4, 8, 16, 32, 64]
- æ€»é…ç½®: 4 Ã— 6 = 24 ä¸ª

ç›®æ ‡:
1. æ‰¾å‡ºæœ€ä½³ååé‡é…ç½®
2. åˆ†æåºåˆ—é•¿åº¦å¯¹æ€§èƒ½çš„å½±å“
3. åˆ†ææ‰¹é‡å¤§å°å¯¹æ€§èƒ½çš„å½±å“
4. ç”Ÿæˆæ€§èƒ½çƒ­åŠ›å›¾æ•°æ®
"""

import time
import torch
import json
import numpy as np
from typing import List, Dict
from optimal_sampling import OptimalSamplingV1


def generate_prompts(num_prompts: int, complexity: str = "medium") -> List[str]:
    """ç”Ÿæˆæµ‹è¯•prompts"""
    if complexity == "short":
        templates = [
            "What is {a} + {b}?",
            "Calculate: {a} Ã— {b}",
            "Solve for x: {a}x = {b}",
        ]
    elif complexity == "medium":
        templates = [
            "A train travels at {a} km/h for {b} hours. What distance?",
            "In a class of {a} students, {b}% are boys. How many boys?",
            "A store sells apples at ${a} each. Cost of {b} apples?",
        ]
    else:  # long
        templates = [
            "A company has {a} employees working {b} hours per week at ${c}/hour. "
            "Calculate monthly salary cost for 4 weeks.",
            "A rectangular garden is {a}m Ã— {b}m. A path of {c}m width surrounds it. "
            "Calculate the path area.",
            "In a school with {a} classes of {b} students each, {c}% attend a trip. "
            "Each bus holds {d} students. How many buses needed?",
        ]

    prompts = []
    for i in range(num_prompts):
        template = templates[i % len(templates)]
        params = {chr(97+j): np.random.randint(2, 20) for j in range(4)}
        prompts.append(template.format(**params))

    return prompts


def run_benchmark_config(
    sampler: OptimalSamplingV1,
    batch_size: int,
    max_tokens: int,
    use_optimal_sampling: bool = True
) -> Dict:
    """è¿è¡Œå•ä¸ªé…ç½®çš„ benchmark"""

    mode = "optimal" if use_optimal_sampling else "baseline"

    # ç”Ÿæˆ prompts
    complexity = "short" if max_tokens <= 1024 else "medium" if max_tokens <= 2048 else "long"
    prompts = generate_prompts(batch_size, complexity)

    # é¢„çƒ­ (å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œ)
    if not hasattr(run_benchmark_config, '_warmed_up'):
        print("  [é¢„çƒ­] Running warmup...")
        sampler.generate(
            prompts=prompts[:min(2, batch_size)],
            max_tokens=min(100, max_tokens),
            temperature=0.8,
            use_optimal_sampling=use_optimal_sampling
        )
        run_benchmark_config._warmed_up = True

    # æµ‹é‡GPUå†…å­˜
    torch.cuda.reset_peak_memory_stats()
    mem_before = torch.cuda.memory_allocated() / 1024**3

    # å¼€å§‹è®¡æ—¶
    start_time = time.time()

    try:
        # ç”Ÿæˆ
        outputs = sampler.generate(
            prompts=prompts,
            max_tokens=max_tokens,
            temperature=0.8,
            use_optimal_sampling=use_optimal_sampling
        )

        end_time = time.time()
        elapsed = end_time - start_time

        # è®¡ç®—æŒ‡æ ‡
        mem_after = torch.cuda.memory_allocated() / 1024**3
        mem_peak = torch.cuda.max_memory_allocated() / 1024**3

        total_tokens = sum(outputs.num_tokens)
        throughput = total_tokens / elapsed
        latency_per_request = elapsed / batch_size
        avg_tokens_per_request = total_tokens / batch_size

        result = {
            "batch_size": batch_size,
            "max_tokens": max_tokens,
            "mode": mode,
            "elapsed_time": elapsed,
            "total_tokens": total_tokens,
            "throughput": throughput,
            "latency_per_request": latency_per_request,
            "avg_tokens_per_request": avg_tokens_per_request,
            "memory_before_gb": mem_before,
            "memory_after_gb": mem_after,
            "memory_peak_gb": mem_peak,
            "success": True,
            "error": None
        }

    except Exception as e:
        result = {
            "batch_size": batch_size,
            "max_tokens": max_tokens,
            "mode": mode,
            "success": False,
            "error": str(e)
        }
        print(f"  âŒ Error: {e}")

    return result


def main():
    print("=" * 80)
    print("å¤§è§„æ¨¡æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼šåºåˆ—é•¿åº¦ vs æ‰¹é‡å¤§å°")
    print("=" * 80)

    # æµ‹è¯•çŸ©é˜µ
    max_tokens_list = [1024, 2048, 4096, 8192]  # 1k, 2k, 4k, 8k
    batch_sizes = [1, 4, 8, 16, 32, 64]

    total_configs = len(max_tokens_list) * len(batch_sizes)

    print(f"\næµ‹è¯•çŸ©é˜µ:")
    print(f"  åºåˆ—é•¿åº¦: {max_tokens_list}")
    print(f"  æ‰¹é‡å¤§å°: {batch_sizes}")
    print(f"  æ€»é…ç½®æ•°: {total_configs}")
    print(f"  æ¯ä¸ªé…ç½®æµ‹è¯• 2 ç§æ¨¡å¼ (Optimal + Baseline)")
    print(f"  æ€»æµ‹è¯•æ•°: {total_configs * 2}")

    # åˆå§‹åŒ– sampler (å…¨å±€å¤ç”¨)
    print(f"\n[åˆå§‹åŒ–] Loading models...")
    sampler = OptimalSamplingV1(
        model_teacher="Qwen/Qwen2.5-3B-Instruct",
        model_theta="Qwen/Qwen2.5-1.5B-Instruct",
        alpha_method="kl_symmetry",
        gpu_memory_utilization=0.45,
        track_alpha_stats=False,  # æ€§èƒ½æ¨¡å¼
    )
    print("âœ… Models loaded!")

    # è¿è¡Œæ‰€æœ‰é…ç½®
    all_results = []
    config_idx = 0

    for max_tokens in max_tokens_list:
        for batch_size in batch_sizes:
            config_idx += 1

            print(f"\n{'=' * 80}")
            print(f"é…ç½® {config_idx}/{total_configs}: "
                  f"BS={batch_size}, MT={max_tokens}")
            print(f"{'=' * 80}")

            # Optimal Sampling
            print(f"\n[{config_idx}.1] Optimal Sampling")
            optimal_result = run_benchmark_config(
                sampler,
                batch_size=batch_size,
                max_tokens=max_tokens,
                use_optimal_sampling=True
            )

            if optimal_result["success"]:
                print(f"  âœ… å®Œæˆ")
                print(f"     æ—¶é—´: {optimal_result['elapsed_time']:.2f}s")
                print(f"     ååé‡: {optimal_result['throughput']:.2f} tok/s")
                print(f"     å»¶è¿Ÿ: {optimal_result['latency_per_request']:.3f}s/req")
                print(f"     Tokens/req: {optimal_result['avg_tokens_per_request']:.1f}")

            all_results.append(optimal_result)

            # Teacher-only Baseline
            print(f"\n[{config_idx}.2] Teacher-only Baseline")
            baseline_result = run_benchmark_config(
                sampler,
                batch_size=batch_size,
                max_tokens=max_tokens,
                use_optimal_sampling=False
            )

            if baseline_result["success"]:
                print(f"  âœ… å®Œæˆ")
                print(f"     æ—¶é—´: {baseline_result['elapsed_time']:.2f}s")
                print(f"     ååé‡: {baseline_result['throughput']:.2f} tok/s")
                print(f"     å»¶è¿Ÿ: {baseline_result['latency_per_request']:.3f}s/req")
                print(f"     Tokens/req: {baseline_result['avg_tokens_per_request']:.1f}")

                # è®¡ç®—åŠ é€Ÿæ¯”
                if optimal_result["success"]:
                    speedup = baseline_result['throughput'] / optimal_result['throughput']
                    print(f"\n     âš–ï¸  Baseline vs Optimal: {speedup:.2f}x faster")

            all_results.append(baseline_result)

            # ä¿å­˜ä¸­é—´ç»“æœ
            with open("large_scale_benchmark_results.json", 'w') as f:
                json.dump(all_results, f, indent=2)

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n\n" + "=" * 80)
    print("æ€»ç»“æŠ¥å‘Š")
    print("=" * 80)

    # åˆ†ææœ€ä½³é…ç½®
    optimal_results = [r for r in all_results if r["success"] and r["mode"] == "optimal"]

    if optimal_results:
        # æŒ‰ååé‡æ’åº
        sorted_by_throughput = sorted(optimal_results, key=lambda x: x["throughput"], reverse=True)
        best = sorted_by_throughput[0]

        print(f"\nğŸ† æœ€ä½³ååé‡é…ç½® (Optimal Sampling):")
        print(f"   æ‰¹é‡å¤§å°: {best['batch_size']}")
        print(f"   åºåˆ—é•¿åº¦: {best['max_tokens']}")
        print(f"   ååé‡: {best['throughput']:.2f} tok/s")
        print(f"   å»¶è¿Ÿ: {best['latency_per_request']:.3f}s/req")

        # æŒ‰å»¶è¿Ÿæ’åº
        sorted_by_latency = sorted(optimal_results, key=lambda x: x["latency_per_request"])
        lowest_latency = sorted_by_latency[0]

        print(f"\nâš¡ æœ€ä½å»¶è¿Ÿé…ç½® (Optimal Sampling):")
        print(f"   æ‰¹é‡å¤§å°: {lowest_latency['batch_size']}")
        print(f"   åºåˆ—é•¿åº¦: {lowest_latency['max_tokens']}")
        print(f"   å»¶è¿Ÿ: {lowest_latency['latency_per_request']:.3f}s/req")
        print(f"   ååé‡: {lowest_latency['throughput']:.2f} tok/s")

    # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
    print(f"\n\nğŸ“Š ååé‡çƒ­åŠ›å›¾ (Optimal Sampling, tok/s):")
    print(f"{'BS \\ MT':<10}", end="")
    for mt in max_tokens_list:
        print(f"{mt:>10}", end="")
    print()
    print("-" * (10 + 10 * len(max_tokens_list)))

    for bs in batch_sizes:
        print(f"{bs:<10}", end="")
        for mt in max_tokens_list:
            # æŸ¥æ‰¾å¯¹åº”ç»“æœ
            result = next(
                (r for r in optimal_results
                 if r["batch_size"] == bs and r["max_tokens"] == mt),
                None
            )
            if result:
                print(f"{result['throughput']:>10.1f}", end="")
            else:
                print(f"{'N/A':>10}", end="")
        print()

    print(f"\n\nğŸ“Š å»¶è¿Ÿçƒ­åŠ›å›¾ (Optimal Sampling, s/req):")
    print(f"{'BS \\ MT':<10}", end="")
    for mt in max_tokens_list:
        print(f"{mt:>10}", end="")
    print()
    print("-" * (10 + 10 * len(max_tokens_list)))

    for bs in batch_sizes:
        print(f"{bs:<10}", end="")
        for mt in max_tokens_list:
            result = next(
                (r for r in optimal_results
                 if r["batch_size"] == bs and r["max_tokens"] == mt),
                None
            )
            if result:
                print(f"{result['latency_per_request']:>10.3f}", end="")
            else:
                print(f"{'N/A':>10}", end="")
        print()

    # å¯¹æ¯” Optimal vs Baseline
    print(f"\n\nâš–ï¸  Speedup çƒ­åŠ›å›¾ (Baseline / Optimal, å€æ•°):")
    print(f"{'BS \\ MT':<10}", end="")
    for mt in max_tokens_list:
        print(f"{mt:>10}", end="")
    print()
    print("-" * (10 + 10 * len(max_tokens_list)))

    for bs in batch_sizes:
        print(f"{bs:<10}", end="")
        for mt in max_tokens_list:
            optimal = next(
                (r for r in all_results
                 if r["success"] and r["mode"] == "optimal"
                 and r["batch_size"] == bs and r["max_tokens"] == mt),
                None
            )
            baseline = next(
                (r for r in all_results
                 if r["success"] and r["mode"] == "baseline"
                 and r["batch_size"] == bs and r["max_tokens"] == mt),
                None
            )
            if optimal and baseline:
                speedup = baseline["throughput"] / optimal["throughput"]
                print(f"{speedup:>10.2f}", end="")
            else:
                print(f"{'N/A':>10}", end="")
        print()

    # æ€§èƒ½è¶‹åŠ¿åˆ†æ
    print(f"\n\nğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ:")

    # æŒ‰åºåˆ—é•¿åº¦åˆ†æ
    print(f"\n1. åºåˆ—é•¿åº¦çš„å½±å“ (å›ºå®š BS=16):")
    for mt in max_tokens_list:
        result = next(
            (r for r in optimal_results
             if r["batch_size"] == 16 and r["max_tokens"] == mt),
            None
        )
        if result:
            print(f"   MT={mt:>4}: {result['throughput']:>7.2f} tok/s, "
                  f"{result['latency_per_request']:>6.3f}s/req")

    # æŒ‰æ‰¹é‡å¤§å°åˆ†æ
    print(f"\n2. æ‰¹é‡å¤§å°çš„å½±å“ (å›ºå®š MT=2048):")
    for bs in batch_sizes:
        result = next(
            (r for r in optimal_results
             if r["batch_size"] == bs and r["max_tokens"] == 2048),
            None
        )
        if result:
            print(f"   BS={bs:>2}: {result['throughput']:>7.2f} tok/s, "
                  f"{result['latency_per_request']:>6.3f}s/req")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    output_file = "large_scale_benchmark_results.json"
    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)

    print(f"\n\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    print("\n" + "=" * 80)
    print("ğŸ‰ å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    main()
