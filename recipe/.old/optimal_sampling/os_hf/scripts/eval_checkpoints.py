#!/usr/bin/env python3
"""
Checkpoint è¯„æµ‹è„šæœ¬

åŠŸèƒ½ï¼š
1. éå†ç›®å½•ä¸­çš„æ‰€æœ‰ HF model checkpoint
2. å¯¹æ¯ä¸ªæ¨¡å‹ä½¿ç”¨ vLLM è¿›è¡Œéƒ¨ç½²
3. åœ¨å¤šä¸ªæ•°å­¦æ•°æ®é›†ä¸Šè¯„ä¼°æ€§èƒ½
4. ä½¿ç”¨ math_verify éªŒè¯ç­”æ¡ˆ
5. ä¿å­˜è¯¦ç»†ç»“æœåˆ° jsonl å’Œæ±‡æ€»è¡¨æ ¼

æ”¯æŒçš„æ•°æ®é›†ï¼š
- math-ai/amc23 (question, answer)
- HuggingFaceH4/aime_2024 (problem, answer)
- math-ai/aime25 (problem, answer)
- HuggingFaceH4/MATH-500 (problem, answer)
"""

import argparse
import json
import os
import sys
import time
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

import pandas as pd
from datasets import load_dataset
from tqdm import tqdm

try:
    from math_verify import parse, verify
    USE_MATH_VERIFY = True
except ImportError:
    print("Warning: math_verify not installed. Install with: pip install math-verify")
    USE_MATH_VERIFY = False

@dataclass
class EvalResult:
    """å•æ¡è¯„æµ‹ç»“æœ"""
    dataset: str
    problem: str
    ground_truth: str
    model_response: str
    model_answer: str  # ä» response ä¸­æå–çš„ç­”æ¡ˆ
    is_correct: bool
    model_name: str
    checkpoint_step: Optional[int] = None


@dataclass
class DatasetConfig:
    """æ•°æ®é›†é…ç½®"""
    name: str
    hf_name: str
    question_col: str
    answer_col: str
    split: str = "test"


# æ•°æ®é›†é…ç½®
DATASETS = [
    DatasetConfig(
        name="amc23",
        hf_name="math-ai/amc23",
        question_col="question",
        answer_col="answer",
        split="test"
    ),
    DatasetConfig(
        name="aime_2024",
        hf_name="HuggingFaceH4/aime_2024",
        question_col="problem",
        answer_col="answer",
        split="train"  # è¿™ä¸ªæ•°æ®é›†å¯èƒ½åªæœ‰ train split
    ),
    DatasetConfig(
        name="aime25",
        hf_name="math-ai/aime25",
        question_col="problem",
        answer_col="answer",
        split="test"
    ),
    DatasetConfig(
        name="MATH-500",
        hf_name="HuggingFaceH4/MATH-500",
        question_col="problem",
        answer_col="answer",
        split="test"
    ),
]


class VLLMServer:
    """vLLM ç®¡ç†å™¨ - ä½¿ç”¨ Python API"""

    def __init__(self, model_path: str, port: int = 8000, tensor_parallel_size: Optional[int] = None, verbose: bool = False, chat_template: Optional[str] = None):
        self.model_path = model_path
        self.port = port
        self.tensor_parallel_size = tensor_parallel_size or self._get_gpu_count()
        self.verbose = verbose
        self.chat_template = chat_template
        self.llm = None
        self.tokenizer = None  # ç”¨äºåº”ç”¨chat template

    def _get_gpu_count(self) -> int:
        """è·å–å¯ç”¨çš„ GPU æ•°é‡"""
        try:
            result = subprocess.run(
                ["nvidia-smi", "--list-gpus"],
                capture_output=True,
                text=True,
                check=True
            )
            return len(result.stdout.strip().split('\n'))
        except Exception:
            return 1

    def start(self) -> bool:
        """åˆå§‹åŒ– vLLM æ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"Loading vLLM model: {self.model_path}")
        print(f"Tensor Parallel Size: {self.tensor_parallel_size}")
        if self.chat_template:
            print(f"Chat Template: {self.chat_template}")
        print(f"{'='*60}\n")

        try:
            from vllm import LLM, SamplingParams
            from transformers import AutoTokenizer

            # åŠ è½½tokenizerç”¨äºåº”ç”¨chat template
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )

            # åˆå§‹åŒ– LLM
            self.llm = LLM(
                model=self.model_path,
                tensor_parallel_size=self.tensor_parallel_size,
                trust_remote_code=True,
                max_model_len=16384,
                gpu_memory_utilization=0.95,
                enforce_eager=True,
            )

            print("âœ… Model loaded successfully!")
            return True

        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            import traceback
            traceback.print_exc()
            return False

    def generate(
        self,
        prompt: str,
        max_tokens: int = 40960,
        temperature: float = 0.0,
        top_p: float = 1.0,
    ) -> Optional[str]:
        """ç”Ÿæˆå•ä¸ªå›å¤"""
        results = self.generate_batch(
            [prompt],
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p
        )
        return results[0] if results else None

    def generate_batch(
        self,
        prompts: List,  # Can be List[str] or List[List[Dict]]
        max_tokens: int = 40960,
        temperature: float = 0.0,
        top_p: float = 1.0,
    ) -> List[Optional[str]]:
        """æ‰¹é‡ç”Ÿæˆå›å¤

        Args:
            prompts: å¯ä»¥æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–chat messagesåˆ—è¡¨
                     - List[str]: ç›´æ¥çš„promptæ–‡æœ¬
                     - List[List[Dict]]: chat messagesæ ¼å¼ï¼Œä¼šè‡ªåŠ¨åº”ç”¨chat template
        """
        if self.llm is None:
            print("âŒ Model not loaded")
            return [None] * len(prompts)

        try:
            from vllm import SamplingParams

            # å¦‚æœpromptsæ˜¯chat messagesæ ¼å¼ï¼Œä½¿ç”¨tokenizeråº”ç”¨chat template
            processed_prompts = []
            for prompt in prompts:
                if isinstance(prompt, list) and len(prompt) > 0 and isinstance(prompt[0], dict):
                    # Chat messagesæ ¼å¼ï¼Œä½¿ç”¨tokenizerçš„chat template
                    if self.tokenizer and hasattr(self.tokenizer, 'apply_chat_template'):
                        text = self.tokenizer.apply_chat_template(
                            prompt,
                            tokenize=False,
                            add_generation_prompt=True
                        )
                        processed_prompts.append(text)
                    else:
                        # å¦‚æœtokenizerä¸æ”¯æŒchat templateï¼Œé€€å›åˆ°ç®€å•æ‹¼æ¥
                        text = prompt[0]["content"] if prompt else ""
                        processed_prompts.append(text)
                else:
                    # å­—ç¬¦ä¸²æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                    processed_prompts.append(prompt)

            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
            )

            outputs = self.llm.generate(processed_prompts, sampling_params)

            results = []
            for output in outputs:
                if output and output.outputs:
                    results.append(output.outputs[0].text.strip())
                else:
                    results.append(None)

            return results

        except Exception as e:
            print(f"âŒ Generation error: {e}")
            import traceback
            traceback.print_exc()
            return [None] * len(prompts)

    def stop(self):
        """æ¸…ç†èµ„æº"""
        print(f"\nğŸ›‘ Cleaning up vLLM resources...")
        if self.llm is not None:
            # vLLM ä¼šåœ¨ Python é€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ˜¾å¼åˆ é™¤
            del self.llm
            self.llm = None

            # æ¸…ç† CUDA ç¼“å­˜
            import torch
            torch.cuda.empty_cache()

            print("âœ… Resources cleaned")


def find_checkpoints(root_dir: str, filter_checkpoint_pattern: bool = True) -> List[Path]:
    """
    æŸ¥æ‰¾ç›®å½•ä¸­çš„æ‰€æœ‰ HF model checkpointï¼ˆé€’å½’æŸ¥æ‰¾ï¼‰

    Args:
        root_dir: æ ¹ç›®å½•
        filter_checkpoint_pattern: æ˜¯å¦åªä¿ç•™è·¯å¾„ä¸­åŒ…å« 'checkpoint-' çš„ç›®å½•ï¼ˆé¿å…è¯„æµ‹è®­ç»ƒæ ¹è·¯å¾„çš„åˆå§‹æ¨¡å‹ï¼‰

    Returns:
        List[Path]: checkpoint è·¯å¾„åˆ—è¡¨
    """
    root_path = Path(root_dir)
    checkpoints = []

    print(f"ğŸ” Recursively searching for checkpoints in: {root_dir}")
    if filter_checkpoint_pattern:
        print("   Filtering: only directories containing 'checkpoint-' in path")

    # é€’å½’æŸ¥æ‰¾åŒ…å« config.json çš„ç›®å½•
    for path in root_path.rglob("config.json"):
        model_dir = path.parent

        # è¿‡æ»¤ï¼šåªä¿ç•™è·¯å¾„ä¸­åŒ…å« 'checkpoint-' çš„ç›®å½•
        if filter_checkpoint_pattern:
            # æ£€æŸ¥è·¯å¾„çš„ä»»ä½•éƒ¨åˆ†æ˜¯å¦åŒ…å« 'checkpoint-' æ¨¡å¼
            path_parts = model_dir.parts
            has_checkpoint_pattern = any('checkpoint-' in part for part in path_parts)
            if not has_checkpoint_pattern:
                continue

        # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ HF modelï¼ˆåŒ…å« config.json å’Œæ¨¡å‹æƒé‡æ–‡ä»¶ï¼‰
        has_weights = (
            (model_dir / "pytorch_model.bin").exists() or
            (model_dir / "model.safetensors").exists() or
            list(model_dir.glob("pytorch_model-*.bin")) or
            list(model_dir.glob("model-*.safetensors"))
        )

        if has_weights:
            checkpoints.append(model_dir)

    # æŒ‰è·¯å¾„æ’åºï¼ˆé€šå¸¸ checkpoint-100, checkpoint-200 ç­‰ä¼šæŒ‰æ•°å­—é¡ºåºï¼‰
    checkpoints.sort()

    return checkpoints


def extract_checkpoint_info(checkpoint_path) -> Tuple[str, Optional[int]]:
    """
    ä» checkpoint è·¯å¾„æå–æ¨¡å‹åç§°å’Œæ­¥æ•°

    Args:
        checkpoint_path: checkpoint è·¯å¾„ï¼ˆPath æˆ– strï¼‰

    Returns:
        Tuple[str, Optional[int]]: (model_name, step)
    """
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ˆHF æ¨¡å‹åï¼‰ï¼Œç›´æ¥è¿”å›
    if isinstance(checkpoint_path, str):
        # HuggingFace æ¨¡å‹åç§°ï¼Œä½¿ç”¨æ–œæ æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ä½œä¸ºç›®å½•å
        model_name = checkpoint_path.replace('/', '_')
        return model_name, None

    # å¦‚æœæ˜¯ Path å¯¹è±¡ï¼ŒæŒ‰åŸé€»è¾‘å¤„ç†
    path_str = str(checkpoint_path)

    # å°è¯•ä»è·¯å¾„ä¸­æå– step ä¿¡æ¯
    step = None
    if "checkpoint-" in path_str:
        try:
            step_str = path_str.split("checkpoint-")[-1].split("/")[0]
            step = int(step_str)
        except ValueError:
            pass

    # æå–æ¨¡å‹åç§°ï¼šçˆ¶ç›®å½•åç§° + checkpoint åç§°
    # ä¾‹å¦‚ï¼šqwen3-4b-base-DeepScaleR-pure-q14b-sft/checkpoint-10
    parent_name = checkpoint_path.parent.name
    checkpoint_name = checkpoint_path.name

    # å¦‚æœçˆ¶ç›®å½•ä¸æ˜¯ checkpoints è¿™æ ·çš„æ ¹ç›®å½•ï¼Œåˆ™åŒ…å«çˆ¶ç›®å½•åç§°
    if parent_name and parent_name not in ['checkpoints', 'models', 'outputs']:
        model_name = f"{parent_name}/{checkpoint_name}"
    else:
        model_name = checkpoint_name

    return model_name, step


def is_checkpoint_evaluated(checkpoint_path, output_root: Path) -> bool:
    """
    æ£€æŸ¥ checkpoint æ˜¯å¦å·²ç»å®Œæˆè¯„æµ‹

    Args:
        checkpoint_path: checkpoint è·¯å¾„ï¼ˆPath æˆ– strï¼‰
        output_root: è¾“å‡ºæ ¹ç›®å½•

    Returns:
        bool: æ˜¯å¦å·²å®Œæˆè¯„æµ‹
    """
    model_name, _ = extract_checkpoint_info(checkpoint_path)
    output_dir = output_root / model_name

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ summary.json æ–‡ä»¶
    summary_file = output_dir / "summary.json"
    if not summary_file.exists():
        return False

    # æ£€æŸ¥ summary.json æ˜¯å¦åŒ…å«æ‰€æœ‰æ•°æ®é›†çš„ç»“æœ
    try:
        with open(summary_file, 'r', encoding='utf-8') as f:
            summary = json.load(f)

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ•°æ®é›†éƒ½æœ‰ç»“æœ
        expected_datasets = {ds.name for ds in DATASETS}
        evaluated_datasets = set(summary.keys())

        if expected_datasets.issubset(evaluated_datasets):
            return True
        else:
            missing = expected_datasets - evaluated_datasets
            print(f"   âš ï¸  Incomplete evaluation: missing {missing}")
            return False

    except Exception as e:
        print(f"   âš ï¸  Error reading summary.json: {e}")
        return False


def extract_answer_from_response(response: str) -> str:
    """
    ä»æ¨¡å‹å›å¤ä¸­æå–ç­”æ¡ˆ

    å¸¸è§æ ¼å¼ï¼š
    - \\boxed{ç­”æ¡ˆ}
    - Answer: ç­”æ¡ˆ
    - æœ€åä¸€è¡Œ

    Args:
        response: æ¨¡å‹å›å¤

    Returns:
        str: æå–çš„ç­”æ¡ˆ
    """
    # å°è¯•æå– \boxed{} ä¸­çš„å†…å®¹
    if "\\boxed{" in response:
        start = response.rfind("\\boxed{")
        if start != -1:
            # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·
            count = 1
            i = start + 7  # len("\\boxed{")
            while i < len(response) and count > 0:
                if response[i] == '{':
                    count += 1
                elif response[i] == '}':
                    count -= 1
                i += 1

            if count == 0:
                return response[start + 7:i - 1].strip()

    # å°è¯•æå– "Answer:" åé¢çš„å†…å®¹
    if "Answer:" in response:
        answer = response.split("Answer:")[-1].strip()
        # å–ç¬¬ä¸€è¡Œ
        answer = answer.split('\n')[0].strip()
        return answer

    # å°è¯•æå– "ç­”æ¡ˆæ˜¯" åé¢çš„å†…å®¹
    if "ç­”æ¡ˆæ˜¯" in response:
        answer = response.split("ç­”æ¡ˆæ˜¯")[-1].strip()
        answer = answer.split('\n')[0].strip()
        return answer

    # è¿”å›æœ€åä¸€ä¸ªéç©ºè¡Œ
    lines = [line.strip() for line in response.split('\n') if line.strip()]
    if lines:
        return lines[-1]

    return response.strip()


def verify_answer(model_answer: str, ground_truth: str) -> bool:
    """
    éªŒè¯ç­”æ¡ˆæ˜¯å¦æ­£ç¡®

    Args:
        model_answer: æ¨¡å‹ç»™å‡ºçš„ç­”æ¡ˆ
        ground_truth: æ­£ç¡®ç­”æ¡ˆ

    Returns:
        bool: æ˜¯å¦æ­£ç¡®
    """
    if not USE_MATH_VERIFY:
        # å¦‚æœæ²¡æœ‰ math_verifyï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²æ¯”è¾ƒ
        return model_answer.strip() == ground_truth.strip()

    try:
        gold = parse(ground_truth)
        answer = parse(model_answer)
        return verify(gold, answer)
    except Exception:
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒ
        return model_answer.strip() == ground_truth.strip()


def create_prompt(question: str, dataset_name: str):
    """
    åˆ›å»ºæ¨ç† prompt

    Args:
        question: é—®é¢˜
        dataset_name: æ•°æ®é›†åç§°

    Returns:
        List[Dict]: Chat messagesæ ¼å¼ï¼Œä¼šè‡ªåŠ¨åº”ç”¨æ¨¡å‹çš„chat template
                    [{"role": "user", "content": "..."}]
    """
    # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
    content = f"{question}\n\nPlease provide your answer in the format \\boxed{{answer}}."

    # è¿”å› chat messages æ ¼å¼ï¼ŒvLLMä¼šè‡ªåŠ¨åº”ç”¨chat template
    return [{"role": "user", "content": content}]


def evaluate_on_dataset(
    server: VLLMServer,
    dataset_config: DatasetConfig,
    model_name: str,
    checkpoint_step: Optional[int],
    max_samples: Optional[int] = None,
    batch_size: int = 32,
) -> List[EvalResult]:
    """
    åœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°

    Args:
        server: vLLM server
        dataset_config: æ•°æ®é›†é…ç½®
        model_name: æ¨¡å‹åç§°
        checkpoint_step: checkpoint æ­¥æ•°
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        batch_size: æ‰¹é‡æ¨ç†çš„æ‰¹æ¬¡å¤§å°

    Returns:
        List[EvalResult]: è¯„æµ‹ç»“æœåˆ—è¡¨
    """
    print(f"\n{'='*60}")
    print(f"Evaluating on: {dataset_config.name}")
    print(f"{'='*60}\n")

    # åŠ è½½æ•°æ®é›†
    try:
        dataset = load_dataset(dataset_config.hf_name, split=dataset_config.split)
    except Exception as e:
        print(f"âŒ Failed to load dataset {dataset_config.hf_name}: {e}")
        # å°è¯•ä½¿ç”¨å…¶ä»– split
        try:
            dataset = load_dataset(dataset_config.hf_name, split="train")
            print(f"âœ… Loaded 'train' split instead")
        except Exception as e2:
            print(f"âŒ Failed to load dataset with 'train' split: {e2}")
            return []

    # é™åˆ¶æ ·æœ¬æ•°
    if max_samples and len(dataset) > max_samples:
        dataset = dataset.select(range(max_samples))

    print(f"Dataset size: {len(dataset)}")
    print(f"Batch size: {batch_size}")

    results = []

    # æ‰¹é‡å¤„ç†æ•°æ®é›†
    dataset_list = list(dataset)
    num_batches = (len(dataset_list) + batch_size - 1) // batch_size

    for batch_idx in tqdm(range(num_batches), desc=f"Evaluating {dataset_config.name}"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(dataset_list))
        batch_examples = dataset_list[start_idx:end_idx]

        # å‡†å¤‡æ‰¹é‡æ•°æ®
        questions = [ex[dataset_config.question_col] for ex in batch_examples]
        ground_truths = [str(ex[dataset_config.answer_col]) for ex in batch_examples]
        prompts = [create_prompt(q, dataset_config.name) for q in questions]

        # æ‰¹é‡ç”Ÿæˆå›å¤
        responses = server.generate_batch(prompts)

        # å¤„ç†æ‰¹é‡ç»“æœ
        for idx, (question, ground_truth, response) in enumerate(zip(questions, ground_truths, responses)):
            if response is None:
                print(f"âš ï¸  Generation failed for sample {start_idx + idx}")
                response = ""

            # æå–ç­”æ¡ˆ
            model_answer = extract_answer_from_response(response)

            # éªŒè¯ç­”æ¡ˆ
            is_correct = verify_answer(model_answer, ground_truth)

            print(
                f"[PROMPT] {question}\n"
                f"[RESPONSE] {response}\n"
                f"[MODEL_ANSWER] {model_answer}\n"
                f"[GROUND TRUTH] {ground_truth}\n"
                f"[IS_CORRECT] {is_correct}"
            )

            # ä¿å­˜ç»“æœ
            result = EvalResult(
                dataset=dataset_config.name,
                problem=question,
                ground_truth=ground_truth,
                model_response=response,
                model_answer=model_answer,
                is_correct=is_correct,
                model_name=model_name,
                checkpoint_step=checkpoint_step,
            )
            results.append(result)

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = sum(r.is_correct for r in results) / len(results) if results else 0.0
    print(f"\nâœ… {dataset_config.name} Accuracy: {accuracy:.2%} ({sum(r.is_correct for r in results)}/{len(results)})")

    return results


def save_results(
    results: List[EvalResult],
    output_dir: Path,
    dataset_name: str,
):
    """
    ä¿å­˜è¯„æµ‹ç»“æœåˆ° jsonl

    Args:
        results: è¯„æµ‹ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        dataset_name: æ•°æ®é›†åç§°
    """
    output_file = output_dir / f"{dataset_name}.jsonl"

    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(asdict(result), ensure_ascii=False) + '\n')

    print(f"ğŸ“ Results saved to: {output_file}")


def create_summary(
    all_results: Dict[str, List[EvalResult]],
    output_dir: Path,
):
    """
    åˆ›å»ºæ±‡æ€»ç»Ÿè®¡

    Args:
        all_results: æ‰€æœ‰æ•°æ®é›†çš„ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    summary = {}

    for dataset_name, results in all_results.items():
        if results:
            accuracy = sum(r.is_correct for r in results) / len(results)
            correct = sum(r.is_correct for r in results)
            total = len(results)

            summary[dataset_name] = {
                "accuracy": accuracy,
                "correct": correct,
                "total": total,
            }

    # ä¿å­˜ä¸º JSON
    summary_file = output_dir / "summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“Š Summary saved to: {summary_file}")

    # æ‰“å°æ±‡æ€»
    print(f"\n{'='*60}")
    print("EVALUATION SUMMARY")
    print(f"{'='*60}")
    for dataset_name, stats in summary.items():
        print(f"{dataset_name:20s}: {stats['accuracy']:.2%} ({stats['correct']}/{stats['total']})")
    print(f"{'='*60}\n")

    return summary


def create_summary_table(
    results_dir: Path,
    output_file: Path,
):
    """
    åˆ›å»ºæ‰€æœ‰ checkpoint çš„æ±‡æ€»è¡¨æ ¼

    Args:
        results_dir: ç»“æœç›®å½•
        output_file: è¾“å‡ºæ–‡ä»¶
    """
    rows = []

    # éå†æ‰€æœ‰æ¨¡å‹çš„ç»“æœç›®å½•
    for model_dir in sorted(results_dir.iterdir()):
        if not model_dir.is_dir():
            continue

        summary_file = model_dir / "summary.json"
        if not summary_file.exists():
            continue

        # è¯»å– summary
        with open(summary_file, 'r', encoding='utf-8') as f:
            summary = json.load(f)

        # æå–æ¨¡å‹ä¿¡æ¯
        model_name = model_dir.name

        # åˆ›å»ºè¡Œ
        row = {"model": model_name}
        for dataset_name, stats in summary.items():
            row[f"{dataset_name}_accuracy"] = stats["accuracy"]
            row[f"{dataset_name}_correct"] = stats["correct"]
            row[f"{dataset_name}_total"] = stats["total"]

        rows.append(row)

    # åˆ›å»º DataFrame
    if rows:
        df = pd.DataFrame(rows)

        # æŒ‰æ¨¡å‹åç§°æ’åº
        df = df.sort_values("model")

        # ä¿å­˜ä¸º CSV
        df.to_csv(output_file, index=False)
        print(f"\nğŸ“Š Summary table saved to: {output_file}")

        # æ‰“å°è¡¨æ ¼
        print(f"\n{df.to_string(index=False)}\n")
    else:
        print("âš ï¸  No results found to create summary table")


def evaluate_checkpoint(
    checkpoint_path,
    output_root: Path,
    port: int = 8000,
    max_samples: Optional[int] = None,
    tensor_parallel_size: Optional[int] = None,
    verbose: bool = False,
    chat_template: Optional[str] = None,
    batch_size: int = 32,
):
    """
    è¯„ä¼°å•ä¸ª checkpoint

    Args:
        checkpoint_path: checkpoint è·¯å¾„ï¼ˆPath å¯¹è±¡æˆ– HF æ¨¡å‹åå­—ç¬¦ä¸²ï¼‰
        output_root: è¾“å‡ºæ ¹ç›®å½•
        port: vLLM server ç«¯å£
        max_samples: æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°
        tensor_parallel_size: Tensor parallel size
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„ vLLM å¯åŠ¨æ—¥å¿—
        chat_template: è‡ªå®šä¹‰ chat template æ–‡ä»¶è·¯å¾„ (.j2 Jinja2 æ¨¡æ¿)
        batch_size: æ‰¹é‡æ¨ç†çš„æ‰¹æ¬¡å¤§å°
    """
    # æå–æ¨¡å‹ä¿¡æ¯
    model_name, checkpoint_step = extract_checkpoint_info(checkpoint_path)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = output_root / model_name
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'#'*60}")
    print(f"# Evaluating: {model_name}")
    if checkpoint_step:
        print(f"# Checkpoint Step: {checkpoint_step}")
    print(f"# Model Path: {checkpoint_path}")
    print(f"# Output: {output_dir}")
    print(f"{'#'*60}\n")

    # å¯åŠ¨ vLLM server
    server = VLLMServer(
        model_path=str(checkpoint_path),
        port=port,
        tensor_parallel_size=tensor_parallel_size,
        verbose=verbose,
        chat_template=chat_template,
    )

    if not server.start():
        print(f"âŒ Failed to start server for {checkpoint_path}")
        return

    try:
        # åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¯„ä¼°
        all_results = {}

        for dataset_config in DATASETS:
            results = evaluate_on_dataset(
                server=server,
                dataset_config=dataset_config,
                model_name=model_name,
                checkpoint_step=checkpoint_step,
                max_samples=max_samples,
                batch_size=batch_size,
            )

            if results:
                # ä¿å­˜ç»“æœ
                save_results(results, output_dir, dataset_config.name)
                all_results[dataset_config.name] = results

        # åˆ›å»ºæ±‡æ€»
        if all_results:
            create_summary(all_results, output_dir)

    finally:
        # åœæ­¢ server
        server.stop()


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate all checkpoints in a directory",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # è¯„ä¼°ç›®å½•ä¸­çš„æ‰€æœ‰ checkpoint
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results

  # ä½¿ç”¨æŒ‡å®šçš„ GPU æ•°é‡
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results \\
      --tensor-parallel-size 4

  # ä½¿ç”¨è‡ªå®šä¹‰ chat template
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results \\
      --chat-template ./templates/qwen.j2

  # æµ‹è¯•æ¨¡å¼ï¼ˆæ¯ä¸ªæ•°æ®é›†åªè¯„ä¼° 10 ä¸ªæ ·æœ¬ï¼‰
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results \\
      --max-samples 10

  # è¯¦ç»†æ—¥å¿—æ¨¡å¼ï¼ˆè°ƒè¯•ç”¨ï¼‰
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results \\
      --verbose

  # æ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²å®Œæˆçš„ checkpointï¼‰
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results \\
      --resume

  # åŒ…å«æ‰€æœ‰æ¨¡å‹ï¼ˆä¸è¿‡æ»¤ checkpoint- è·¯å¾„ï¼‰
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results \\
      --no-filter

  # æ·»åŠ  HuggingFace ä¸Šçš„å…¶ä»–æ¨¡å‹è¿›è¡Œå¯¹æ¯”è¯„æµ‹
  python eval_checkpoints.py \\
      --checkpoint-dir /path/to/checkpoints \\
      --output-dir ./eval_results \\
      --extra-models Qwen/Qwen2.5-Math-7B-Instruct deepseek-ai/DeepSeek-Math-7B-Instruct

  # åªè¯„æµ‹ HuggingFace æ¨¡å‹ï¼ˆä¸æœç´¢æœ¬åœ° checkpointï¼‰
  python eval_checkpoints.py \\
      --checkpoint-dir /nonexistent \\
      --output-dir ./eval_results \\
      --extra-models Qwen/Qwen2.5-Math-7B-Instruct meta-llama/Llama-3.1-8B-Instruct
        """
    )

    parser.add_argument(
        '--checkpoint-dir',
        type=str,
        required=True,
        help='åŒ…å« checkpoint çš„æ ¹ç›®å½•'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='./eval_results',
        help='è¯„æµ‹ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./eval_results)'
    )

    parser.add_argument(
        '--port',
        type=int,
        default=8000,
        help='vLLM server ç«¯å£ (é»˜è®¤: 8000)'
    )

    parser.add_argument(
        '--tensor-parallel-size',
        type=int,
        default=None,
        help='Tensor parallel size (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹ GPU æ•°é‡)'
    )

    parser.add_argument(
        '--max-samples',
        type=int,
        default=None,
        help='æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†çš„ vLLM å¯åŠ¨æ—¥å¿—'
    )

    parser.add_argument(
        '--chat-template',
        type=str,
        default=None,
        help='è‡ªå®šä¹‰ chat template æ–‡ä»¶è·¯å¾„ (.j2 Jinja2 æ¨¡æ¿æ–‡ä»¶)'
    )

    parser.add_argument(
        '--resume',
        action='store_true',
        help='æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²å®Œæˆè¯„æµ‹çš„ checkpointï¼ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨å®Œæ•´çš„ summary.jsonï¼‰'
    )

    parser.add_argument(
        '--no-filter',
        action='store_true',
        help='ä¸è¿‡æ»¤ checkpoint è·¯å¾„ï¼šè¯„æµ‹æ‰€æœ‰æ‰¾åˆ°çš„æ¨¡å‹ï¼ˆé»˜è®¤åªè¯„æµ‹è·¯å¾„ä¸­åŒ…å« "checkpoint-" çš„æ¨¡å‹ï¼‰'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=32,
        help='æ‰¹é‡æ¨ç†çš„æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32)'
    )

    parser.add_argument(
        '--extra-models',
        type=str,
        nargs='*',
        default=[],
        help='é¢å¤–çš„ HuggingFace æ¨¡å‹åˆ—è¡¨ï¼Œç”¨äºå¯¹æ¯”è¯„æµ‹ï¼ˆä¾‹å¦‚: Qwen/Qwen2.5-Math-7B-Instruct deepseek-ai/DeepSeek-Math-7B-Instructï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥ chat template æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if args.chat_template and not Path(args.chat_template).exists():
        print(f"âŒ Chat template file not found: {args.chat_template}")
        sys.exit(1)

    # æ£€æŸ¥ math_verify
    if not USE_MATH_VERIFY:
        print("âš ï¸  math_verify not available, will use string comparison")
        print("   Install with: pip install math-verify")

    # æŸ¥æ‰¾æ‰€æœ‰ checkpointï¼ˆé€’å½’æœç´¢ï¼‰
    print(f"\n{'='*60}")
    print("Checkpoint Discovery")
    print(f"{'='*60}")
    checkpoints = find_checkpoints(
        args.checkpoint_dir,
        filter_checkpoint_pattern=not args.no_filter
    )

    if not checkpoints:
        print(f"âŒ No checkpoints found in {args.checkpoint_dir}")
        if not args.no_filter:
            print("   ğŸ’¡ Tip: Use --no-filter to search for all models (not just checkpoint-* directories)")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° checkpoint ä½†æœ‰ extra_modelsï¼Œç»§ç»­æ‰§è¡Œ
        if not args.extra_models:
            sys.exit(1)
        else:
            print("   â„¹ï¸  But extra models are provided, continuing...")

    print(f"\nâœ… Found {len(checkpoints)} checkpoint(s):")
    for ckpt in checkpoints:
        print(f"   - {ckpt}")

    # æ·»åŠ  extra models
    if args.extra_models:
        print(f"\n{'='*60}")
        print("Extra HuggingFace Models")
        print(f"{'='*60}")
        print(f"Adding {len(args.extra_models)} extra model(s) for evaluation:")
        for model in args.extra_models:
            print(f"   - {model}")
            checkpoints.append(model)  # ç›´æ¥æ·»åŠ å­—ç¬¦ä¸²
        print()
    else:
        print()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_root = Path(args.output_dir)
    output_root.mkdir(parents=True, exist_ok=True)

    # å¦‚æœå¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œè¿‡æ»¤å·²å®Œæˆçš„ checkpoint
    if args.resume:
        print(f"\n{'='*60}")
        print("Resume Mode: Checking for completed checkpoints")
        print(f"{'='*60}")

        checkpoints_to_eval = []
        skipped_count = 0

        for checkpoint_path in checkpoints:
            model_name, step = extract_checkpoint_info(checkpoint_path)
            if is_checkpoint_evaluated(checkpoint_path, output_root):
                print(f"â­ï¸  Skipping (already evaluated): {model_name}")
                skipped_count += 1
            else:
                checkpoints_to_eval.append(checkpoint_path)

        print(f"\nğŸ“Š Resume Summary:")
        print(f"   - Total checkpoints: {len(checkpoints)}")
        print(f"   - Already evaluated: {skipped_count}")
        print(f"   - To evaluate: {len(checkpoints_to_eval)}")
        print()

        if not checkpoints_to_eval:
            print("âœ… All checkpoints already evaluated!")
            print(f"   Results directory: {output_root}")

            # ç›´æ¥è·³åˆ°åˆ›å»ºæ±‡æ€»è¡¨æ ¼
            print(f"\n{'#'*60}")
            print("# Creating summary table for all checkpoints")
            print(f"{'#'*60}\n")

            summary_table_file = output_root / "summary_table.csv"
            create_summary_table(output_root, summary_table_file)

            print(f"\n{'#'*60}")
            print("# Evaluation complete!")
            print(f"# Results saved to: {output_root}")
            print(f"{'#'*60}\n")

            return

        checkpoints = checkpoints_to_eval
    else:
        print(f"ğŸ’¡ Tip: Use --resume to skip already evaluated checkpoints\n")

    # è¯„ä¼°æ¯ä¸ª checkpoint
    print(f"{'='*60}")
    print(f"Starting Evaluation")
    print(f"{'='*60}\n")

    for idx, checkpoint_path in enumerate(checkpoints, 1):
        model_name, step = extract_checkpoint_info(checkpoint_path)
        print(f"\n{'#'*60}")
        print(f"# Checkpoint {idx}/{len(checkpoints)}: {model_name}")
        if step:
            print(f"# Step: {step}")
        print(f"{'#'*60}")

        try:
            evaluate_checkpoint(
                checkpoint_path=checkpoint_path,
                output_root=output_root,
                port=args.port,
                max_samples=args.max_samples,
                tensor_parallel_size=args.tensor_parallel_size,
                verbose=args.verbose,
                chat_template=args.chat_template,
                batch_size=args.batch_size,
            )
        except Exception as e:
            print(f"âŒ Error evaluating {checkpoint_path}: {e}")
            import traceback
            traceback.print_exc()
            continue

    # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
    print(f"\n{'#'*60}")
    print("# Creating summary table for all checkpoints")
    print(f"{'#'*60}\n")

    summary_table_file = output_root / "summary_table.csv"
    create_summary_table(output_root, summary_table_file)

    print(f"\n{'#'*60}")
    print("# Evaluation complete!")
    print(f"# Results saved to: {output_root}")
    print(f"{'#'*60}\n")


if __name__ == "__main__":
    main()
