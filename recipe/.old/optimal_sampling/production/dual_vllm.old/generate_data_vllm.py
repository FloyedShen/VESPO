#!/usr/bin/env python3
"""
åŸºäº vLLM çš„æ•°æ®ç”Ÿæˆç®¡çº¿

ä½¿ç”¨ EnhancedDualVLLMCoordinator é€šè¿‡ HTTP API è°ƒç”¨ vLLM æœåŠ¡å™¨ç”Ÿæˆæ•°æ®

ç‰¹æ€§:
- âœ… ä½¿ç”¨ vLLM HTTP APIï¼ˆæ— éœ€åŠ è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼‰
- âœ… æ”¯æŒ HuggingFace datasets
- âœ… åŒæç¤ºæ”¯æŒï¼ˆä¸åŒchat templateï¼‰
- âœ… ç¨³å®šæ€§æ£€æµ‹ï¼ˆå¯é€‰ï¼‰
- âœ… Trust regionçº¦æŸï¼ˆå¯é€‰ï¼‰
- âœ… æ‰¹å¤„ç† + å¼‚æ­¥å¹¶å‘
- âœ… æ–­ç‚¹ç»­ä¼ 
- âœ… å¤šç§è¾“å‡ºæ ¼å¼ï¼ˆJSONL/Parquetï¼‰

ä½¿ç”¨ç¤ºä¾‹:

    # å¯åŠ¨ä¸¤ä¸ª vLLM æœåŠ¡å™¨ï¼ˆä¸åŒç»ˆç«¯ï¼‰
    python -m vllm.entrypoints.openai.api_server \\
        --model Qwen/Qwen3-4B-Base --port 9000 --max-logprobs 20

    python -m vllm.entrypoints.openai.api_server \\
        --model Qwen/Qwen3-14B --port 9001 --max-logprobs 20

    # ç”Ÿæˆæ•°æ®
    python generate_data_vllm.py \\
        --theta_url http://localhost:9000 \\
        --t_url http://localhost:9001 \\
        --dataset agentica-org/DeepScaleR-Preview-Dataset \\
        --output generated_data.jsonl \\
        --num_samples 1000 \\
        --max_tokens 512 \\
        --batch_size 16 \\
        --enable_stability_check \\
        --save_diagnostics
"""

import asyncio
import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm
import numpy as np
from datasets import load_dataset, Dataset
import time

from coordinator_enhanced import EnhancedDualVLLMCoordinator
from config_enhanced import EnhancedCoordinatorConfig


class DatasetAdapter:
    """æ•°æ®é›†é€‚é…å™¨ - è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶æå–prompts"""

    def __init__(self, dataset_path: str, split: str = "train"):
        """
        Args:
            dataset_path: HuggingFace dataset name æˆ–æœ¬åœ°è·¯å¾„
            split: æ•°æ®é›†åˆ†å‰²
        """
        print(f"ğŸ“¦ åŠ è½½æ•°æ®é›†: {dataset_path}")
        try:
            self.dataset = load_dataset(dataset_path, split=split)
            print(f"âœ… æˆåŠŸåŠ è½½: {len(self.dataset)} æ¡æ•°æ®")
        except Exception as e:
            raise ValueError(f"Failed to load dataset: {e}")

        # æ£€æµ‹æ ¼å¼
        self._detect_format()

    def _detect_format(self):
        """è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†æ ¼å¼"""
        sample = self.dataset[0]
        self.columns = list(sample.keys())

        # æ£€æµ‹prompt/questionå­—æ®µ
        prompt_candidates = ["prompt", "question", "instruction", "input", "text", "query"]
        self.prompt_field = None
        for candidate in prompt_candidates:
            if candidate in self.columns:
                self.prompt_field = candidate
                break

        # æ£€æµ‹messageså­—æ®µï¼ˆOpenAIæ ¼å¼ï¼‰
        self.messages_field = "messages" if "messages" in self.columns else None

        print(f"ğŸ“‹ æ•°æ®é›†æ ¼å¼:")
        print(f"   - Columns: {self.columns}")
        print(f"   - Prompt field: {self.prompt_field}")
        print(f"   - Messages field: {self.messages_field}")

    def __len__(self):
        return len(self.dataset)

    def get_prompt(self, idx: int) -> str:
        """æå–çº¯æ–‡æœ¬ prompt"""
        sample = self.dataset[idx]

        # ä¼˜å…ˆä½¿ç”¨messagesæ ¼å¼
        if self.messages_field and self.messages_field in sample:
            messages = sample[self.messages_field]
            if isinstance(messages, list):
                # æå–æœ€åä¸€ä¸ªuseræ¶ˆæ¯
                for msg in reversed(messages):
                    if isinstance(msg, dict) and msg.get("role") == "user":
                        return msg.get("content", "")

        # ä½¿ç”¨promptå­—æ®µ
        if self.prompt_field and self.prompt_field in sample:
            content = sample[self.prompt_field]
            if isinstance(content, str):
                return content

        # Fallback: è¿”å›ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å­—æ®µ
        for value in sample.values():
            if isinstance(value, str) and len(value) > 0:
                return value

        raise ValueError(f"Cannot extract prompt from sample {idx}")

    def get_messages(self, idx: int) -> List[Dict[str, str]]:
        """æå–messagesæ ¼å¼ï¼ˆOpenAIï¼‰"""
        sample = self.dataset[idx]

        # å¦‚æœå·²æœ‰messagesæ ¼å¼
        if self.messages_field and self.messages_field in sample:
            messages = sample[self.messages_field]
            if isinstance(messages, list):
                return messages

        # å¦åˆ™ä»promptæ„é€ 
        prompt = self.get_prompt(idx)
        return [{"role": "user", "content": prompt}]

    def get_metadata(self, idx: int) -> Dict:
        """è·å–å…ƒæ•°æ®"""
        sample = self.dataset[idx]
        metadata = {"sample_idx": idx}

        # ä¿å­˜åŸå§‹æ•°æ®çš„å…¶ä»–å­—æ®µ
        for key, value in sample.items():
            if key != self.prompt_field and key != self.messages_field:
                # åªä¿å­˜ç®€å•ç±»å‹
                if isinstance(value, (str, int, float, bool)):
                    metadata[f"original_{key}"] = value

        return metadata


def create_dual_prompts(
    messages_list: List[List[Dict[str, str]]],
    use_base_template: bool = True,
    use_instruct_template: bool = True
) -> tuple[List[str], List[str]]:
    """
    åˆ›å»ºåŒæç¤ºï¼šBaseå’ŒInstructæ ¼å¼

    Args:
        messages_list: OpenAIæ ¼å¼çš„messagesåˆ—è¡¨
        use_base_template: æ˜¯å¦ä¸ºbaseæ¨¡å‹ä½¿ç”¨ç®€å•æ¨¡æ¿
        use_instruct_template: æ˜¯å¦ä¸ºinstructæ¨¡å‹ä½¿ç”¨chat template

    Returns:
        (prompts_theta, prompts_t)
    """
    prompts_theta = []
    prompts_t = []

    for messages in messages_list:
        # æå–useræ¶ˆæ¯
        user_content = ""
        for msg in messages:
            if msg.get("role") == "user":
                user_content = msg.get("content", "")
                break

        # Baseæ ¼å¼ï¼ˆç®€å•ï¼‰
        if use_base_template:
            prompt_theta = f"Question: {user_content}\n\nAnswer: "
        else:
            prompt_theta = user_content

        # Instructæ ¼å¼ï¼ˆQwen/ChatMLï¼‰
        if use_instruct_template:
            prompt_t = f"<|im_start|>user\n{user_content}<|im_end|>\n<|im_start|>assistant\n"
        else:
            prompt_t = user_content

        prompts_theta.append(prompt_theta)
        prompts_t.append(prompt_t)

    return prompts_theta, prompts_t


class CheckpointManager:
    """ç®¡ç†æ–­ç‚¹ç»­ä¼ """

    def __init__(self, output_path: str):
        self.output_path = Path(output_path)
        self.checkpoint_file = self.output_path.parent / f"{self.output_path.stem}.checkpoint"

    def load(self) -> Optional[Dict]:
        """åŠ è½½checkpoint"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to load checkpoint: {e}")
        return None

    def save(self, checkpoint: Dict):
        """ä¿å­˜checkpoint"""
        try:
            self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.checkpoint_file, 'w') as f:
                json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to save checkpoint: {e}")

    def remove(self):
        """åˆ é™¤checkpoint"""
        if self.checkpoint_file.exists():
            try:
                self.checkpoint_file.unlink()
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to remove checkpoint: {e}")


async def generate_data(args):
    """ä¸»æ•°æ®ç”Ÿæˆå‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸš€ vLLM Optimal Sampling æ•°æ®ç”Ÿæˆç®¡çº¿")
    print("="*80)

    # åŠ è½½æ•°æ®é›†
    adapter = DatasetAdapter(args.dataset, args.dataset_split)
    total_samples = len(adapter)

    # ç¡®å®šå¤„ç†èŒƒå›´
    if args.num_samples is None:
        args.num_samples = total_samples - args.start_idx
    else:
        args.num_samples = min(args.num_samples, total_samples - args.start_idx)

    end_idx = args.start_idx + args.num_samples

    print(f"\nğŸ“Š å¤„ç†èŒƒå›´:")
    print(f"   - Total samples in dataset: {total_samples}")
    print(f"   - Processing: {args.start_idx} â†’ {end_idx} ({args.num_samples} samples)")

    # Checkpointç®¡ç†
    checkpoint_mgr = CheckpointManager(args.output)
    checkpoint = checkpoint_mgr.load()

    start_from = args.start_idx
    if checkpoint:
        last_processed = checkpoint.get("last_processed_idx", args.start_idx - 1)
        if last_processed >= end_idx - 1:
            print(f"\nâœ… Already completed! (checkpoint shows idx={last_processed})")
            return

        start_from = last_processed + 1
        print(f"\nğŸ”„ Resuming from checkpoint: idx={start_from}")

    # é…ç½®Coordinator
    config = EnhancedCoordinatorConfig(
        theta_url=args.theta_url,
        t_url=args.t_url,
        theta_model_name=args.theta_model,
        t_model_name=args.t_model,
        top_k=args.top_k,
        force_first_token=args.force_first_token,
        constraint_to_target=args.constraint_to_target,
        target_top_p=args.target_top_p,
        enable_stability_check=args.enable_stability_check,
        stability_threshold_js=args.stability_threshold_js,
        stability_threshold_overlap=args.stability_threshold_overlap,
        auto_fallback=args.auto_fallback,
        enable_logging=args.verbose,
    )

    print(f"\nâš™ï¸  é…ç½®:")
    print(f"   - Î¸ URL: {config.theta_url}")
    print(f"   - t URL: {config.t_url}")
    print(f"   - Top-k: {config.top_k}")
    print(f"   - Force first token: {config.force_first_token}")
    print(f"   - Trust region: {config.constraint_to_target} (p={config.target_top_p})")
    print(f"   - Stability check: {config.enable_stability_check}")
    if config.enable_stability_check:
        print(f"      JS threshold: {config.stability_threshold_js}")
        print(f"      Overlap threshold: {config.stability_threshold_overlap}")

    # è¾“å‡ºæ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    mode = 'a' if checkpoint else 'w'
    f_out = open(output_path, mode)

    diag_file = None
    if args.save_diagnostics:
        diag_file = open(output_path.with_suffix('.diagnostics.jsonl'), mode)

    # å¯åŠ¨Coordinator
    print(f"\nğŸ”— è¿æ¥åˆ° vLLM æœåŠ¡å™¨...")
    async with EnhancedDualVLLMCoordinator(config) as coordinator:
        print(f"âœ… è¿æ¥æˆåŠŸ!")

        # æ‰¹å¤„ç†ç”Ÿæˆ
        processed_count = start_from - args.start_idx

        for batch_start in tqdm(
            range(start_from, end_idx, args.batch_size),
            desc="Generating",
            initial=processed_count // args.batch_size
        ):
            batch_end = min(batch_start + args.batch_size, end_idx)
            batch_indices = range(batch_start, batch_end)

            # æå–messages
            messages_list = []
            metadata_list = []
            for idx in batch_indices:
                try:
                    messages = adapter.get_messages(idx)
                    metadata = adapter.get_metadata(idx)
                    messages_list.append(messages)
                    metadata_list.append(metadata)
                except Exception as e:
                    print(f"\nâš ï¸  Warning: Failed to extract sample {idx}: {e}")
                    continue

            if not messages_list:
                continue

            # åˆ›å»ºåŒæç¤º
            prompts_theta, prompts_t = create_dual_prompts(
                messages_list,
                use_base_template=args.use_base_template,
                use_instruct_template=args.use_instruct_template
            )

            # ç”Ÿæˆ
            try:
                results = await coordinator.generate_batch_dual_prompts(
                    prompts_theta=prompts_theta,
                    prompts_t=prompts_t,
                    max_tokens=args.max_tokens,
                    temperature=args.temperature,
                    return_diagnostics=args.save_diagnostics,
                    show_progress=False
                )

                # ä¿å­˜ç»“æœ
                for i, result in enumerate(results):
                    if result.error:
                        print(f"\nâš ï¸  Error in sample {batch_start + i}: {result.error}")
                        continue

                    # æ„é€ è¾“å‡ºæ•°æ®ï¼ˆOpenAIæ ¼å¼ï¼‰
                    output_data = {
                        "messages": [
                            {"role": "user", "content": adapter.get_prompt(batch_start + i)},
                            {"role": "assistant", "content": result.generated_text[len(result.prompt):]}
                        ],
                        "metadata": metadata_list[i]
                    }

                    # æ·»åŠ alphaç­‰è¯Šæ–­ä¿¡æ¯
                    if args.save_diagnostics:
                        output_data["diagnostics"] = {
                            "alpha_mean": float(np.mean(result.alpha_history)),
                            "alpha_std": float(np.std(result.alpha_history)),
                            "alpha_first": float(result.alpha_history[0]) if result.alpha_history else None,
                        }

                        # å†™å…¥è¯Šæ–­æ–‡ä»¶
                        diag_data = {
                            "sample_idx": batch_start + i,
                            **output_data["diagnostics"]
                        }
                        diag_file.write(json.dumps(diag_data, ensure_ascii=False) + '\n')
                        diag_file.flush()

                    # å†™å…¥ä¸»è¾“å‡º
                    f_out.write(json.dumps(output_data, ensure_ascii=False) + '\n')
                    f_out.flush()

                    processed_count += 1

                # ä¿å­˜checkpoint
                checkpoint_mgr.save({
                    "last_processed_idx": batch_end - 1,
                    "processed_count": processed_count,
                    "timestamp": time.time()
                })

            except Exception as e:
                print(f"\nâŒ Error during generation for batch {batch_start}-{batch_end}: {e}")
                import traceback
                traceback.print_exc()
                continue

    # å…³é—­æ–‡ä»¶
    f_out.close()
    if diag_file:
        diag_file.close()

    # åˆ é™¤checkpoint
    checkpoint_mgr.remove()

    # ç»Ÿè®¡ä¿¡æ¯
    stats = coordinator.get_statistics()
    print(f"\n{'='*80}")
    print("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯")
    print("="*80)
    print(f"æ€»æ ·æœ¬æ•°: {processed_count}")
    print(f"æˆåŠŸç‡: {stats.get('success_rate', 0):.1%}")
    print(f"æ€»Tokens: {stats['total_tokens']}")
    print(f"é¦–Tokenå¼ºåˆ¶: {stats['first_token_forced']} æ¬¡")
    if config.enable_stability_check:
        print(f"ç¨³å®šæ€§æ£€æŸ¥: {stats['stability_checks']} æ¬¡")
        print(f"ç¨³å®šæ€§Fallback: {stats['stability_fallback']} æ¬¡ ({stats['stability_fallback']/max(stats['stability_checks'],1)*100:.1f}%)")

    print(f"\nâœ… å®Œæˆï¼è¾“å‡º: {output_path}")
    if args.save_diagnostics:
        print(f"ğŸ“Š è¯Šæ–­ä¿¡æ¯: {output_path.with_suffix('.diagnostics.jsonl')}")


def main():
    parser = argparse.ArgumentParser(description="vLLMæ•°æ®ç”Ÿæˆç®¡çº¿")

    # vLLMæœåŠ¡å™¨
    parser.add_argument("--theta_url", type=str, required=True,
                       help="Ï€_Î¸ (base) vLLMæœåŠ¡å™¨URL")
    parser.add_argument("--t_url", type=str, required=True,
                       help="Ï€_t (teacher) vLLMæœåŠ¡å™¨URL")
    parser.add_argument("--theta_model", type=str, default="Qwen/Qwen3-4B-Base",
                       help="Baseæ¨¡å‹åç§°")
    parser.add_argument("--t_model", type=str, default="Qwen/Qwen3-14B",
                       help="Teacheræ¨¡å‹åç§°")

    # æ•°æ®é›†
    parser.add_argument("--dataset", type=str, required=True,
                       help="HuggingFace dataset name")
    parser.add_argument("--dataset_split", type=str, default="train",
                       help="Dataset split")

    # å¤„ç†èŒƒå›´
    parser.add_argument("--num_samples", type=int, default=None,
                       help="Number of samples (default: all)")
    parser.add_argument("--start_idx", type=int, default=0,
                       help="Start index")
    parser.add_argument("--batch_size", type=int, default=16,
                       help="Batch size")

    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_tokens", type=int, default=512,
                       help="Maximum tokens to generate")
    parser.add_argument("--temperature", type=float, default=1.0,
                       help="Sampling temperature")

    # Coordinatoré…ç½®
    parser.add_argument("--top_k", type=int, default=20,
                       help="Top-k for approximation (max 20 for vLLM 0.11.0)")
    parser.add_argument("--force_first_token", action="store_true", default=True,
                       help="Force first token to use Ï€_t")
    parser.add_argument("--constraint_to_target", action="store_true", default=True,
                       help="Enable trust region constraint")
    parser.add_argument("--target_top_p", type=float, default=0.95,
                       help="Trust region top-p threshold")

    # ç¨³å®šæ€§æ£€æµ‹
    parser.add_argument("--enable_stability_check", action="store_true",
                       help="Enable stability detection")
    parser.add_argument("--stability_threshold_js", type=float, default=0.5,
                       help="JS divergence threshold")
    parser.add_argument("--stability_threshold_overlap", type=float, default=0.1,
                       help="Overlap probability mass threshold")
    parser.add_argument("--auto_fallback", action="store_true", default=True,
                       help="Auto fallback to Ï€_t when unstable")

    # Promptæ¨¡æ¿
    parser.add_argument("--use_base_template", action="store_true", default=True,
                       help="Use simple template for base model")
    parser.add_argument("--use_instruct_template", action="store_true", default=True,
                       help="Use chat template for instruct model")

    # è¾“å‡º
    parser.add_argument("--output", type=str, required=True,
                       help="Output file path (JSONL)")
    parser.add_argument("--save_diagnostics", action="store_true",
                       help="Save diagnostic information")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose logging")

    args = parser.parse_args()

    # è¿è¡Œ
    asyncio.run(generate_data(args))


if __name__ == "__main__":
    main()
