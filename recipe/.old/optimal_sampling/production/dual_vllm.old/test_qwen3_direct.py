#!/usr/bin/env python3
"""
ç›´æ¥ä½¿ç”¨ vLLM Python SDK æµ‹è¯• Qwen3-4B-Base + Qwen3-14B

ä¸ä½¿ç”¨ HTTP APIï¼Œç›´æ¥è°ƒç”¨ vLLM Python æ¥å£
"""

import asyncio
import numpy as np
from typing import List, Dict
from vllm import LLM, SamplingParams

from utils import (
    solve_kl_symmetry,
    compute_q_star,
    merge_top_k_candidates,
    sample_from_distribution,
    compute_diagnostics,
)


def test_direct_vllm():
    """ç›´æ¥ä½¿ç”¨ vLLM SDK æµ‹è¯•"""
    print("\n" + "="*70)
    print("ğŸ§ª Qwen3-4B-Base + Qwen3-14B ç›´æ¥æµ‹è¯•ï¼ˆvLLM SDKï¼‰")
    print("="*70)

    # åˆå§‹åŒ–ä¸¤ä¸ªæ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    print("  åŠ è½½ Base æ¨¡å‹ (4B)...")
    llm_theta = LLM(
        model="Qwen/Qwen3-4B-Base",
        gpu_memory_utilization=0.20,
        max_model_len=2048,
        dtype="auto",
        trust_remote_code=True,
    )

    print("  åŠ è½½ Teacher æ¨¡å‹ (14B)...")
    llm_t = LLM(
        model="Qwen/Qwen3-14B",
        gpu_memory_utilization=0.55,
        max_model_len=2048,
        dtype="auto",
        trust_remote_code=True,
    )

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

    # æµ‹è¯•æç¤º
    prompts_theta = [
        "Q: What is machine learning?\nA:",
    ]

    prompts_t = [
        "<|im_start|>user\nWhat is machine learning?<|im_end|>\n<|im_start|>assistant\n",
    ]

    # ç”Ÿæˆå‚æ•°
    sampling_params = SamplingParams(
        temperature=1.0,
        max_tokens=1,
        logprobs=20,  # è·å– top-20 logprobs
    )

    print("ğŸ“ æµ‹è¯•ç”Ÿæˆ...")
    print(f"   Base æ ¼å¼: {prompts_theta[0][:40]}...")
    print(f"   Instruct æ ¼å¼: {prompts_t[0][:40]}...")

    # ç”Ÿæˆ 50 ä¸ª token
    max_tokens = 50
    context_theta = prompts_theta[0]
    context_t = prompts_t[0]

    generated_tokens = []
    alpha_history = []

    for step in range(max_tokens):
        # ä»ä¸¤ä¸ªæ¨¡å‹è·å– logprobs
        outputs_theta = llm_theta.generate([context_theta], sampling_params)
        outputs_t = llm_t.generate([context_t], sampling_params)

        # æå– logprobs
        logprobs_theta_data = outputs_theta[0].outputs[0].logprobs[0]
        logprobs_t_data = outputs_t[0].outputs[0].logprobs[0]

        # è½¬æ¢ä¸º dict
        logprobs_theta = {token_id: logprob.logprob for token_id, logprob in logprobs_theta_data.items()}
        logprobs_t = {token_id: logprob.logprob for token_id, logprob in logprobs_t_data.items()}

        # åˆå¹¶ top-k
        candidates, probs_theta, probs_t = merge_top_k_candidates(
            logprobs_theta, logprobs_t
        )

        # è®¡ç®— Î±*
        if step == 0:
            # é¦– token å¼ºåˆ¶
            alpha_star = 1.0
            q_star = probs_t
        else:
            alpha_star = solve_kl_symmetry(probs_theta, probs_t)
            q_star = compute_q_star(probs_theta, probs_t, alpha_star)

        # é‡‡æ ·
        next_token = sample_from_distribution(q_star, candidates)

        generated_tokens.append(next_token)
        alpha_history.append(alpha_star)

        # è§£ç å¹¶æ›´æ–°ä¸Šä¸‹æ–‡
        token_str = llm_theta.get_tokenizer().decode([next_token])
        context_theta += token_str
        context_t += token_str

        if step < 5:
            print(f"  Step {step}: token={next_token}, Î±={alpha_star:.3f}, text='{token_str}'")

    # ç»“æœ
    print(f"\n{'='*70}")
    print("ğŸ“Š ç»“æœ")
    print("="*70)
    print(f"  ç”Ÿæˆ tokens: {len(generated_tokens)}")
    print(f"  å¹³å‡ Î±: {np.mean(alpha_history):.3f} Â± {np.std(alpha_history):.3f}")
    print(f"  é¦– Î±: {alpha_history[0]:.3f} (åº”ä¸º 1.0)")
    print(f"  Î± èŒƒå›´: [{np.min(alpha_history):.3f}, {np.max(alpha_history):.3f}]")

    print(f"\n  ç”Ÿæˆæ–‡æœ¬:")
    print(f"  {context_theta[:200]}...")

    print("\n" + "="*70)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    test_direct_vllm()
