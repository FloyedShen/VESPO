"""
æ•°æ®ç”Ÿæˆè„šæœ¬ (å¤šå¡å¹¶è¡Œç‰ˆæœ¬)

åŠŸèƒ½:
- âœ… æ‰¹é‡å¤„ç† (batch processing)
- âœ… æ•°æ®å¹¶è¡Œ (data parallel) - å¤šå¡å¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡
- âœ… æ¨¡å‹åˆ‡ç‰‡ (model parallelism) - å¤§æ¨¡å‹è‡ªåŠ¨åˆ†ç‰‡åˆ°å¤šGPU
- âœ… åŠ¨æ€é…ç½®ï¼šæ ¹æ®GPUæ•°é‡å’Œæ˜¾å­˜è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
- âœ… æ–­ç‚¹ç»­ä¼ 
- âœ… è‡ªåŠ¨æ•°æ®é›†æ ¼å¼æ£€æµ‹

ä½¿ç”¨ç¤ºä¾‹:

    # å°æ¨¡å‹ + æ•°æ®å¹¶è¡Œï¼ˆæ¨èç”¨äº7B-13Bæ¨¡å‹ï¼‰
    python generate_data_parallel.py \
        --model_theta Qwen/Qwen2.5-7B \
        --model_t Qwen/Qwen2.5-7B-Instruct \
        --dataset /path/to/DeepScaleR \
        --output generated_data.jsonl \
        --strategy data_parallel \
        --num_gpus 4 \
        --batch_size_per_gpu 8

    # å¤§æ¨¡å‹ + æ¨¡å‹åˆ‡ç‰‡ï¼ˆæ¨èç”¨äº70Bæ¨¡å‹ï¼‰
    python generate_data_parallel.py \
        --model_theta meta-llama/Llama-2-70b-hf \
        --model_t meta-llama/Llama-2-70b-chat-hf \
        --dataset /path/to/DeepScaleR \
        --output generated_data.jsonl \
        --strategy model_parallel \
        --num_gpus 4 \
        --load_in_4bit \
        --attn_implementation flash_attention_2

    # è‡ªåŠ¨ç­–ç•¥ï¼ˆæ ¹æ®GPUæ•°é‡å’Œæ˜¾å­˜è‡ªåŠ¨é€‰æ‹©ï¼‰
    python generate_data_parallel.py \
        --model_theta Qwen/Qwen2.5-7B \
        --model_t Qwen/Qwen2.5-7B-Instruct \
        --dataset /path/to/DeepScaleR \
        --output generated_data.jsonl \
        --strategy auto
"""

import os
import json
import argparse
import torch
import torch.multiprocessing as mp
from typing import List, Dict, Optional
from pathlib import Path
from tqdm import tqdm
from datasets import load_dataset, load_from_disk, Dataset, concatenate_datasets
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import time

from optimal_sampling_model import create_optimal_sampling_model, create_dual_prompts


# ============================================
# Checkpoint ç®¡ç†
# ============================================

class CheckpointManager:
    """ç®¡ç†æ–­ç‚¹ç»­ä¼ çš„checkpoint"""

    def __init__(self, output_path: str, rank: int = 0):
        """
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            rank: GPU rank (å¤šGPUæ—¶ä½¿ç”¨)
        """
        self.output_path = Path(output_path)
        self.rank = rank

        # Checkpoint æ–‡ä»¶è·¯å¾„
        if rank > 0:
            self.checkpoint_file = self.output_path.parent / f"{self.output_path.stem}_gpu{rank}.checkpoint"
        else:
            self.checkpoint_file = self.output_path.parent / f"{self.output_path.stem}.checkpoint"

    def load(self) -> Optional[Dict]:
        """åŠ è½½checkpoint"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r') as f:
                    checkpoint = json.load(f)
                return checkpoint
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to load checkpoint: {e}")
                return None
        return None

    def save(self, checkpoint: Dict):
        """ä¿å­˜checkpoint"""
        try:
            self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.checkpoint_file, 'w') as f:
                json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to save checkpoint: {e}")

    def remove(self):
        """åˆ é™¤checkpointï¼ˆä»»åŠ¡å®Œæˆåï¼‰"""
        if self.checkpoint_file.exists():
            try:
                self.checkpoint_file.unlink()
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to remove checkpoint: {e}")


# ============================================
# æ•°æ®é›†é€‚é…å™¨
# ============================================

class DatasetAdapter:
    """æ•°æ®é›†é€‚é…å™¨ - è‡ªåŠ¨æ£€æµ‹æ ¼å¼"""

    def __init__(self, dataset_path: str, split: str = "train"):
        """
        Args:
            dataset_path: HuggingFace dataset name æˆ–æœ¬åœ°è·¯å¾„
            split: æ•°æ®é›†åˆ†å‰²
        """
        self.dataset_path = dataset_path

        # å°è¯•åŠ è½½æ•°æ®é›†
        try:
            # å°è¯•ä½œä¸ºæœ¬åœ°è·¯å¾„åŠ è½½
            if Path(dataset_path).exists():
                self.dataset = load_from_disk(dataset_path)
                if isinstance(self.dataset, dict) and split in self.dataset:
                    self.dataset = self.dataset[split]
                print(f"âœ“ Loaded dataset from local path: {dataset_path}")
            else:
                # ä½œä¸ºHuggingFace datasetåŠ è½½
                self.dataset = load_dataset(dataset_path, split=split)
                print(f"âœ“ Loaded dataset from HuggingFace: {dataset_path}")
        except Exception as e:
            raise ValueError(f"Failed to load dataset from {dataset_path}: {e}")

        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        self._detect_format()

    def _detect_format(self):
        """è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†æ ¼å¼"""
        sample = self.dataset[0]
        self.columns = list(sample.keys())

        # æ£€æµ‹promptå­—æ®µ
        prompt_candidates = ["prompt", "question", "instruction", "input", "text", "query"]
        self.prompt_field = None
        for candidate in prompt_candidates:
            if candidate in self.columns:
                self.prompt_field = candidate
                break

        # æ£€æµ‹messageså­—æ®µ (OpenAIæ ¼å¼)
        self.messages_field = "messages" if "messages" in self.columns else None

        print(f"âœ“ Detected dataset format:")
        print(f"  Columns: {self.columns}")
        print(f"  Prompt field: {self.prompt_field}")
        print(f"  Messages field: {self.messages_field}")

    def __len__(self):
        return len(self.dataset)

    def get_prompt(self, idx: int) -> str:
        """æå–prompt"""
        sample = self.dataset[idx]

        # ä¼˜å…ˆä½¿ç”¨messagesæ ¼å¼
        if self.messages_field and self.messages_field in sample:
            messages = sample[self.messages_field]
            if isinstance(messages, list):
                # æå–æœ€åä¸€ä¸ªuseræ¶ˆæ¯
                for msg in reversed(messages):
                    if isinstance(msg, dict) and msg.get("role") == "user":
                        return msg.get("content", "")

        # ä½¿ç”¨promptå­—æ®µ
        if self.prompt_field and self.prompt_field in sample:
            content = sample[self.prompt_field]
            if isinstance(content, str):
                return content

        # Fallback: è¿”å›ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å­—æ®µ
        for value in sample.values():
            if isinstance(value, str) and len(value) > 0:
                return value

        raise ValueError(f"Cannot extract prompt from sample {idx}")

    def get_messages(self, idx: int) -> List[Dict[str, str]]:
        """
        æå–messagesæ ¼å¼çš„æ•°æ®

        Returns:
            List[Dict[str, str]]: OpenAI messages æ ¼å¼ [{"role": "user", "content": "..."}]
        """
        sample = self.dataset[idx]

        # ä¼˜å…ˆä½¿ç”¨messagesæ ¼å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.messages_field and self.messages_field in sample:
            messages = sample[self.messages_field]
            if isinstance(messages, list):
                return messages

        # å¦åˆ™ä»promptæ„é€ messages
        prompt = self.get_prompt(idx)
        return [{"role": "user", "content": prompt}]

    def get_metadata(self, idx: int) -> Dict:
        """è·å–å…ƒæ•°æ®"""
        sample = self.dataset[idx]
        metadata = {"sample_idx": idx}

        # ä¿å­˜åŸå§‹æ•°æ®çš„å…¶ä»–å­—æ®µ
        for key, value in sample.items():
            if key != self.prompt_field and key != self.messages_field:
                # åªä¿å­˜ç®€å•ç±»å‹
                if isinstance(value, (str, int, float, bool)):
                    metadata[f"original_{key}"] = value

        return metadata

    def to_openai_format(self, prompt: str, response: str, metadata: Dict = None) -> Dict:
        """è½¬æ¢ä¸ºOpenAI APIæ ¼å¼"""
        messages = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ]

        result = {"messages": messages}

        if metadata:
            result.update(metadata)

        return result

    def to_verl_format(self, prompt: str, response: str, metadata: Dict = None) -> Dict:
        """
        è½¬æ¢ä¸º verl è®­ç»ƒæ ¼å¼

        verl æ ¼å¼ç¤ºä¾‹:
        {
            "data_source": "optimal_sampling",
            "prompt": [{"role": "user", "content": "..."}],
            "ability": "general",
            "extra_info": {
                "alpha_mean": 0.65,
                "response": "...",
                ...
            }
        }
        """
        data = {
            "data_source": "optimal_sampling",
            "prompt": [{"role": "user", "content": prompt}],
            "ability": "general",  # å¯ä»¥æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®
            "extra_info": {
                "response": response,
                "sample_idx": metadata.get("sample_idx", -1) if metadata else -1,
            }
        }

        # æ·»åŠ å…¶ä»–å…ƒæ•°æ®
        if metadata:
            for key, value in metadata.items():
                if key not in ["sample_idx"]:
                    data["extra_info"][key] = value

        return data


# ============================================
# å¤šGPUæ•°æ®ç”Ÿæˆå™¨
# ============================================

def worker_process(
    rank: int,
    world_size: int,
    model_theta: str,
    model_t: str,
    dataset_path: str,
    dataset_split: str,
    output_path: str,
    start_idx: int,
    end_idx: int,
    batch_size: int,
    max_new_tokens: int,
    temperature: float,
    alpha_method: str,
    fixed_alpha: float,
    model_kwargs: Dict,
    save_diagnostics: bool,
    output_format: str = "jsonl",  # "jsonl", "parquet", or "both"
    print_samples: bool = False  # âœ¨ æ–°å¢ï¼šæ˜¯å¦æ‰“å°æ¯ä¸ªæ ·æœ¬
):
    """
    Workerè¿›ç¨‹ï¼šå¤„ç†åˆ†é…ç»™è¯¥GPUçš„æ•°æ®

    Args:
        rank: GPU rank (0, 1, 2, ...)
        world_size: æ€»GPUæ•°é‡
        å…¶ä»–å‚æ•°ï¼šæ¨¡å‹å’Œç”Ÿæˆé…ç½®
    """
    try:
        # è®¾ç½®CUDA device
        torch.cuda.set_device(rank)
        device = f"cuda:{rank}"

        print(f"[GPU {rank}] Initializing worker...")

        # åŠ è½½æ•°æ®é›†
        adapter = DatasetAdapter(dataset_path, dataset_split)

        # åˆ›å»ºæ¨¡å‹ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ï¼‰
        print(f"[GPU {rank}] Loading model...")
        model_kwargs["device"] = device
        model = create_optimal_sampling_model(
            model_theta=model_theta,
            model_t=model_t,
            alpha_method=alpha_method,
            fixed_alpha=fixed_alpha,
            # device=device,
            **model_kwargs
        )
        print(f"[GPU {rank}] Model loaded successfully")

        # è®¡ç®—è¯¥è¿›ç¨‹å¤„ç†çš„ç´¢å¼•èŒƒå›´
        total_samples = end_idx - start_idx
        samples_per_gpu = total_samples // world_size
        extra = total_samples % world_size

        # åˆ†é…æ ·æœ¬ï¼ˆå°½é‡å‡åŒ€ï¼‰
        if rank < extra:
            local_start = start_idx + rank * (samples_per_gpu + 1)
            local_end = local_start + samples_per_gpu + 1
        else:
            local_start = start_idx + extra * (samples_per_gpu + 1) + (rank - extra) * samples_per_gpu
            local_end = local_start + samples_per_gpu

        print(f"[GPU {rank}] Processing samples {local_start} to {local_end} ({local_end - local_start} total)")

        # âœ¨ Checkpoint ç®¡ç†
        checkpoint_mgr = CheckpointManager(output_path, rank=rank)
        checkpoint = checkpoint_mgr.load()

        # æ£€æŸ¥æ˜¯å¦ä» checkpoint æ¢å¤
        if checkpoint:
            last_processed_idx = checkpoint.get("last_processed_idx", local_start - 1)
            processed_count = checkpoint.get("processed_count", 0)
            resume_from = last_processed_idx + 1

            if resume_from >= local_end:
                print(f"[GPU {rank}] âœ“ Already completed! (checkpoint shows all samples processed)")
                return

            print(f"[GPU {rank}] ğŸ”„ Resuming from checkpoint:")
            print(f"[GPU {rank}]    Last processed: {last_processed_idx}")
            print(f"[GPU {rank}]    Processed count: {processed_count}")
            print(f"[GPU {rank}]    Resuming from: {resume_from}")

            # è°ƒæ•´èµ·å§‹ä½ç½®
            local_start = resume_from
        else:
            print(f"[GPU {rank}] Starting from scratch (no checkpoint found)")
            processed_count = 0

        # è¾“å‡ºæ–‡ä»¶ï¼ˆæ¯ä¸ªGPUç‹¬ç«‹æ–‡ä»¶ï¼‰
        output_file_base = Path(output_path).parent / f"{Path(output_path).stem}_gpu{rank}"

        # æ ¹æ®è¾“å‡ºæ ¼å¼å†³å®šæ–‡ä»¶æ‰©å±•å
        if output_format == "parquet":
            output_file = output_file_base.with_suffix('.parquet')
        else:
            output_file = output_file_base.with_suffix('.jsonl')

        diag_file = output_file_base.with_suffix('.diagnostics.jsonl') if save_diagnostics else None

        # æ”¶é›†æ‰€æœ‰æ•°æ®ï¼ˆç”¨äºparquetï¼‰
        all_data = []
        all_diag_data = []

        # å¦‚æœéœ€è¦jsonlï¼Œæ‰“å¼€æ–‡ä»¶
        f_out = None
        f_diag = None
        if output_format in ["jsonl", "both"]:
            # âœ¨ ä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
            mode = 'a' if checkpoint else 'w'
            f_out = open(output_file if output_format == "jsonl" else output_file_base.with_suffix('.jsonl'), mode)
            if save_diagnostics:
                f_diag = open(diag_file, mode)

        # æ‰¹å¤„ç†ç”Ÿæˆ
        for batch_start in tqdm(
            range(local_start, local_end, batch_size),
            desc=f"GPU {rank}",
            position=rank
        ):
            batch_end = min(batch_start + batch_size, local_end)
            batch_indices = range(batch_start, batch_end)

            # âœ¨ æå–messagesï¼ˆç”¨äºç”Ÿæˆdual promptsï¼‰
            messages_list = []
            metadata_list = []
            for idx in batch_indices:
                try:
                    messages = adapter.get_messages(idx)
                    metadata = adapter.get_metadata(idx)
                    messages_list.append(messages)
                    metadata_list.append(metadata)
                except Exception as e:
                    print(f"[GPU {rank}] Warning: Failed to extract messages at {idx}: {e}")
                    continue

            if not messages_list:
                continue

            # âœ¨ ä½¿ç”¨ create_dual_prompts ç”Ÿæˆä¸¤ä¸ªä¸åŒçš„ prompts
            # Ï€_Î¸ ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿ï¼ŒÏ€_t ä½¿ç”¨ chat template
            try:
                prompts_theta, prompts_t = create_dual_prompts(
                    messages_list,
                    model.tokenizer_theta,
                    model.tokenizer_t,
                    force_nlt_in_theta=True,  # Baseæ¨¡å‹ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿
                    add_generation_prompt=True,
                    add_think_token=False,
                )
            except Exception as e:
                print(f"[GPU {rank}] Warning: Failed to create dual prompts: {e}")
                # Fallback: ä½¿ç”¨ç®€å•çš„promptæå–
                prompts_theta = []
                prompts_t = []
                for messages in messages_list:
                    # ç®€å•æå–useræ¶ˆæ¯
                    user_content = ""
                    for msg in messages:
                        if msg.get("role") == "user":
                            user_content = msg.get("content", "")
                            break
                    prompts_theta.append(f"Question: {user_content}\n\nAnswer: ")
                    prompts_t.append(f"Question: {user_content}\n\nAnswer: ")

            # ç”Ÿæˆ
            try:
                outputs = model.generate(
                    prompts=prompts_theta,  # âœ¨ Ï€_Î¸ ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿
                    prompts_t=prompts_t,    # âœ¨ Ï€_t ä½¿ç”¨ chat template
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    return_diagnostics=save_diagnostics,
                    tqdm_desc=f"GPU {rank}, [{batch_start - local_start + 1}-{batch_end - local_start}]"
                )

                # ä¿å­˜ç»“æœ
                for i, response in enumerate(outputs.generated_texts):
                    # è·å–åŸå§‹ promptï¼ˆç”¨äºä¿å­˜ï¼‰
                    prompt = prompts_theta[i]  # ä¿å­˜ Î¸ çš„ promptï¼ˆæˆ–è€…å¯ä»¥ä¿å­˜ messagesï¼‰
                    # å‡†å¤‡å…ƒæ•°æ®ï¼ˆåŒ…å«alphaç­‰è¯Šæ–­ä¿¡æ¯ï¼‰
                    metadata_with_diag = metadata_list[i].copy()
                    if save_diagnostics:
                        metadata_with_diag["alpha_mean"] = outputs.alpha_values[i].mean().item()
                        metadata_with_diag["alpha_std"] = outputs.alpha_values[i].std().item()
                        metadata_with_diag["ess_ratio_mean"] = outputs.ess_ratios[i].mean().item()
                        metadata_with_diag["gpu_rank"] = rank

                    # Parquetæ ¼å¼ï¼ˆverlæ ¼å¼ï¼‰
                    if output_format in ["parquet", "both"]:
                        verl_data = adapter.to_verl_format(
                            prompt=prompt,
                            response=response,
                            metadata=metadata_with_diag
                        )
                        all_data.append(verl_data)

                    # JSONLæ ¼å¼ï¼ˆOpenAIæ ¼å¼ï¼‰
                    if output_format in ["jsonl", "both"]:
                        openai_data = adapter.to_openai_format(
                            prompt=prompt,
                            response=response,
                            metadata=metadata_list[i]
                        )
                        f_out.write(json.dumps(openai_data, ensure_ascii=False) + '\n')
                        f_out.flush()

                    # ä¿å­˜è¯Šæ–­ä¿¡æ¯
                    if save_diagnostics:
                        diag_data = {
                            "sample_idx": metadata_list[i].get("sample_idx"),
                            "alpha_mean": outputs.alpha_values[i].mean().item(),
                            "alpha_std": outputs.alpha_values[i].std().item(),
                            "ess_ratio_mean": outputs.ess_ratios[i].mean().item(),
                            "gpu_rank": rank
                        }
                        if output_format in ["jsonl", "both"]:
                            f_diag.write(json.dumps(diag_data, ensure_ascii=False) + '\n')
                            f_diag.flush()
                        all_diag_data.append(diag_data)

                    # âœ¨ æ‰“å°æ ·æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if print_samples:
                        sample_idx = metadata_list[i].get("sample_idx", -1)
                        print(f"\n{'='*80}")
                        print(f"[GPU {rank}] Sample #{sample_idx}")
                        print(f"{'='*80}")
                        print(f"ğŸ“ Prompt_\\theta:")
                        print(prompt)
                        print(f"ğŸ“ Prompt_t:")
                        print(prompts_t[i])
                        print(f"\nğŸ’¬ Response:")
                        print(response)
                        if save_diagnostics:
                            print(f"\nğŸ“Š Diagnostics:")
                            print(f"  - Alpha (Ï€_t weight): {metadata_with_diag.get('alpha_mean', 0):.3f} Â± {metadata_with_diag.get('alpha_std', 0):.3f}")
                            print(f"  - ESS Ratio: {metadata_with_diag.get('ess_ratio_mean', 0):.3f}")
                        print(f"{'='*80}\n")

                    # âœ¨ æ›´æ–°å·²å¤„ç†æ ·æœ¬è®¡æ•°
                    processed_count += 1

                # âœ¨ å¯¹äºparquetæ ¼å¼ï¼Œå®šæœŸä¿å­˜åˆ°ç£ç›˜ï¼ˆé˜²æ­¢OOMå’Œæ•°æ®ä¸¢å¤±ï¼‰
                # æ¯32ä¸ªbatchï¼ˆæˆ–ç´¯ç§¯åˆ°ä¸€å®šæ•°é‡ï¼‰ä¿å­˜ä¸€æ¬¡
                SAVE_INTERVAL_BATCHES = 16  # æ¯16ä¸ªbatchä¿å­˜ä¸€æ¬¡
                if output_format in ["parquet", "both"] and len(all_data) >= SAVE_INTERVAL_BATCHES * batch_size:
                    print(f"[GPU {rank}] Intermediate save: {len(all_data)} samples...")
                    # ä¿å­˜åˆ°ä¸´æ—¶parquetæ–‡ä»¶ï¼ˆä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼‰
                    parquet_file = output_file if output_format == "parquet" else output_file_base.with_suffix('.parquet')
                    if parquet_file.exists():
                        # è¿½åŠ æ¨¡å¼ï¼šè¯»å–å·²æœ‰æ•°æ®ï¼Œåˆå¹¶åé‡å†™ï¼ˆparquetä¸æ”¯æŒåŸç”Ÿè¿½åŠ ï¼‰
                        existing_dataset = Dataset.from_parquet(str(parquet_file))
                        new_dataset = Dataset.from_list(all_data)
                        combined = concatenate_datasets([existing_dataset, new_dataset])
                        combined.to_parquet(str(parquet_file))
                    else:
                        # é¦–æ¬¡ä¿å­˜
                        dataset = Dataset.from_list(all_data)
                        dataset.to_parquet(str(parquet_file))
                    print(f"[GPU {rank}] âœ“ Intermediate save completed")
                    # æ¸…ç©ºå†…å­˜
                    all_data = []

                # âœ¨ ä¿å­˜ checkpointï¼ˆæ¯ä¸ªbatchåï¼‰
                checkpoint_data = {
                    "last_processed_idx": batch_end - 1,
                    "processed_count": processed_count,
                    "timestamp": time.time()
                }
                checkpoint_mgr.save(checkpoint_data)

            except Exception as e:
                print(f"[GPU {rank}] Error during generation for batch {batch_start}-{batch_end}: {e}")
                import traceback
                traceback.print_exc()
                continue

        # å…³é—­jsonlæ–‡ä»¶
        if f_out:
            f_out.close()
        if f_diag:
            f_diag.close()

        # å†™å…¥parquetæ–‡ä»¶ï¼ˆæœ€åå‰©ä½™çš„æ•°æ®ï¼‰
        if output_format in ["parquet", "both"]:
            if all_data:
                print(f"[GPU {rank}] Writing final parquet data ({len(all_data)} samples)...")
                # è½¬æ¢ä¸ºDatasetå¹¶ä¿å­˜
                parquet_file = output_file if output_format == "parquet" else output_file_base.with_suffix('.parquet')
                if parquet_file.exists():
                    # è¿½åŠ åˆ°å·²æœ‰æ–‡ä»¶
                    existing_dataset = Dataset.from_parquet(str(parquet_file))
                    new_dataset = Dataset.from_list(all_data)
                    combined = concatenate_datasets([existing_dataset, new_dataset])
                    combined.to_parquet(str(parquet_file))
                else:
                    # é¦–æ¬¡ä¿å­˜ï¼ˆå¦‚æœä¸­é—´ä¿å­˜æ²¡è§¦å‘ï¼‰
                    dataset = Dataset.from_list(all_data)
                    dataset.to_parquet(str(parquet_file))
                print(f"[GPU {rank}] âœ“ Final parquet saved: {parquet_file}")
            else:
                print(f"[GPU {rank}] âœ“ All data already saved incrementally")

        # âœ¨ åˆ é™¤ checkpointï¼ˆä»»åŠ¡æˆåŠŸå®Œæˆï¼‰
        checkpoint_mgr.remove()
        print(f"[GPU {rank}] âœ“ Checkpoint removed")

        print(f"[GPU {rank}] âœ“ Completed! Output: {output_file}")

    except Exception as e:
        print(f"[GPU {rank}] FATAL ERROR: {e}")
        import traceback
        traceback.print_exc()


def merge_outputs(output_path: str, num_gpus: int, output_format: str = "jsonl", delete_temp: bool = True):
    """åˆå¹¶å¤šä¸ªGPUçš„è¾“å‡ºæ–‡ä»¶"""
    print("\n" + "="*60)
    print("Merging outputs from all GPUs...")
    print("="*60)

    output_file = Path(output_path)

    # åˆå¹¶ JSONL
    if output_format in ["jsonl", "both"]:
        merged_data = output_file.with_suffix('.jsonl')
        merged_diag = output_file.with_suffix('.diagnostics.jsonl')

        # åˆå¹¶ä¸»è¾“å‡º
        with open(merged_data, 'w') as f_out:
            for rank in range(num_gpus):
                temp_file = output_file.parent / f"{output_file.stem}_gpu{rank}.jsonl"
                if temp_file.exists():
                    with open(temp_file, 'r') as f_in:
                        for line in f_in:
                            f_out.write(line)
                    if delete_temp:
                        temp_file.unlink()

        # åˆå¹¶è¯Šæ–­ä¿¡æ¯
        diag_exists = False
        for rank in range(num_gpus):
            temp_diag = output_file.parent / f"{output_file.stem}_gpu{rank}.diagnostics.jsonl"
            if temp_diag.exists():
                diag_exists = True
                break

        if diag_exists:
            with open(merged_diag, 'w') as f_out:
                for rank in range(num_gpus):
                    temp_diag = output_file.parent / f"{output_file.stem}_gpu{rank}.diagnostics.jsonl"
                    if temp_diag.exists():
                        with open(temp_diag, 'r') as f_in:
                            for line in f_in:
                                f_out.write(line)
                        if delete_temp:
                            temp_diag.unlink()

        print(f"âœ“ Merged JSONL output: {merged_data}")
        if diag_exists:
            print(f"âœ“ Merged diagnostics: {merged_diag}")

    # åˆå¹¶ Parquet
    if output_format in ["parquet", "both"]:
        merged_parquet = output_file.with_suffix('.parquet')

        # æ”¶é›†æ‰€æœ‰GPUçš„æ•°æ®
        all_datasets = []
        for rank in range(num_gpus):
            temp_parquet = output_file.parent / f"{output_file.stem}_gpu{rank}.parquet"
            if temp_parquet.exists():
                try:
                    ds = Dataset.from_parquet(str(temp_parquet))
                    all_datasets.append(ds)
                    if delete_temp:
                        temp_parquet.unlink()
                except Exception as e:
                    print(f"Warning: Failed to load {temp_parquet}: {e}")

        if all_datasets:
            # åˆå¹¶æ‰€æœ‰dataset
            from datasets import concatenate_datasets
            merged_dataset = concatenate_datasets(all_datasets)
            merged_dataset.to_parquet(str(merged_parquet))
            print(f"âœ“ Merged Parquet output: {merged_parquet}")
            print(f"  Total samples: {len(merged_dataset)}")

    print("="*60 + "\n")


# ============================================
# ä¸»å‡½æ•°
# ============================================

def main():
    parser = argparse.ArgumentParser(description="Generate data with multi-GPU support")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_theta", type=str, required=True,
                       help="Path to Ï€_Î¸ model")
    parser.add_argument("--model_t", type=str, default=None,
                       help="Path to Ï€_t model (default: same as model_theta)")

    # æ•°æ®é›†
    parser.add_argument("--dataset", type=str, required=True,
                       help="HuggingFace dataset name or local path")
    parser.add_argument("--dataset_split", type=str, default="train",
                       help="Dataset split")

    # å¤šGPUé…ç½®
    parser.add_argument("--strategy", type=str, default="auto",
                       choices=["auto", "data_parallel", "model_parallel"],
                       help="Parallelization strategy")
    parser.add_argument("--num_gpus", type=int, default=None,
                       help="Number of GPUs to use (default: all available)")
    parser.add_argument("--batch_size_per_gpu", type=int, default=8,
                       help="Batch size per GPU (for data_parallel)")

    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--num_samples", type=int, default=None,
                       help="Number of samples to generate (default: all)")
    parser.add_argument("--start_idx", type=int, default=0,
                       help="Start index (for resuming)")
    parser.add_argument("--max_new_tokens", type=int, default=16384,
                       help="Maximum new tokens to generate")
    parser.add_argument("--temperature", type=float, default=1.0,
                       help="Sampling temperature")

    # Alphaæ–¹æ³•
    parser.add_argument("--alpha_method", type=str, default="ess_balance",
                       choices=["fixed", "kl_symmetry", "entropy", "ess_balance"],
                       help="Method to compute alpha")
    parser.add_argument("--fixed_alpha", type=float, default=0.5,
                       help="Fixed alpha value (when alpha_method=fixed)")

    # å¤§æ¨¡å‹åŠ é€Ÿå‚æ•°
    # parser.add_argument("--load_in_4bit", action="store_true",
    #                    help="Enable INT4 quantization")
    # parser.add_argument("--load_in_8bit", action="store_true",
    #                    help="Enable INT8 quantization")
    parser.add_argument("--attn_implementation", type=str, default="flash_attention_2",
                       choices=["eager", "flash_attention_2"],
                       help="Attention implementation")
    parser.add_argument("--dtype", type=str, default="float16",
                       choices=["float16", "bfloat16", "float32"],
                       help="Data type")

    # âœ¨ æ–°å¢ï¼šé‡‡æ ·æ§åˆ¶å‚æ•°
    parser.add_argument("--constraint_to_target", action="store_true", default=True,
                       help="Constrain q* to Ï€_t's support (default: True)")
    parser.add_argument("--no_constraint_to_target", dest="constraint_to_target",
                       action="store_false",
                       help="Disable constraint to target")
    parser.add_argument("--target_top_k", type=int, default=64,
                       help="Top-k for Ï€_t support constraint (default: 128, was 32)")
    parser.add_argument("--target_top_p", type=float, default=0.95,
                       help="Top-p for Ï€_t support constraint (default: 0.95)")
    parser.add_argument("--force_first_token", action="store_true", default=True,
                       help="Force first token to use Ï€_t (default: True)")
    parser.add_argument("--no_force_first_token", dest="force_first_token",
                       action="store_false",
                       help="Disable forcing first token")

    # è¾“å‡º
    parser.add_argument("--output", type=str, required=True,
                       help="Output file path")
    parser.add_argument("--output_format", type=str, default="jsonl",
                       choices=["jsonl", "parquet", "both"],
                       help="Output format: jsonl (OpenAI), parquet (verl), or both")
    parser.add_argument("--save_diagnostics", action="store_true",
                       help="Save diagnostic information")
    parser.add_argument("--print_samples", action="store_true",
                       help="Print each prompt and response during generation")
    parser.add_argument("--keep_temp_files", action="store_true",
                       help="Keep temporary per-GPU files")

    args = parser.parse_args()

    # æ£€æµ‹å¯ç”¨GPU
    if torch.cuda.is_available():
        available_gpus = torch.cuda.device_count()
        if args.num_gpus is None:
            args.num_gpus = available_gpus
        else:
            args.num_gpus = min(args.num_gpus, available_gpus)
    else:
        raise RuntimeError("No CUDA devices available!")

    print("\n" + "="*60)
    print("Multi-GPU Data Generation")
    print("="*60)
    print(f"GPUs available: {available_gpus}")
    print(f"GPUs to use: {args.num_gpus}")
    print(f"Strategy: {args.strategy}")
    print(f"Batch size per GPU: {args.batch_size_per_gpu}")
    print("="*60 + "\n")

    # å†³å®šç­–ç•¥
    if args.strategy == "auto":
        model_name = args.model_theta.lower()
        if "70b" in model_name or "65b" in model_name:
            strategy = "model_parallel"
        else:
            strategy = "data_parallel"
        print(f"Auto-selected strategy: {strategy}")
    else:
        strategy = args.strategy

    # è½¬æ¢dtype
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32
    }

    # æ„å»ºæ¨¡å‹kwargs
    model_kwargs = {
        "dtype": dtype_map[args.dtype],
        # âœ… æ–°å¢ï¼šä¼ é€’é‡‡æ ·æ§åˆ¶å‚æ•°
        "constraint_to_target": args.constraint_to_target,
        "target_top_k": args.target_top_k,
        "target_top_p": args.target_top_p,
        "force_target_for_first_token": args.force_first_token,
        "force_target_for_special_tokens": True,  # å§‹ç»ˆå¯ç”¨special tokenå¤„ç†
    }

    # if args.load_in_4bit:
    #     model_kwargs["load_in_4bit"] = True
    #     model_kwargs["bnb_4bit_compute_dtype"] = torch.bfloat16
    #     model_kwargs["bnb_4bit_use_double_quant"] = True
    #     model_kwargs["bnb_4bit_quant_type"] = "nf4"
    #
    # if args.load_in_8bit:
    #     model_kwargs["load_in_8bit"] = True

    if args.attn_implementation:
        model_kwargs["attn_implementation"] = args.attn_implementation

    # æ ¹æ®ç­–ç•¥é…ç½®device_map
    if strategy == "model_parallel":
        # æ¨¡å‹åˆ‡ç‰‡ï¼šè®©transformersè‡ªåŠ¨åˆ†é…
        model_kwargs["device_map"] = "auto"
        if args.num_gpus > 1:
            # é™åˆ¶æ¯ä¸ªGPUçš„æ˜¾å­˜ï¼ˆå‡åˆ†ï¼‰
            max_memory = {i: f"{75 // args.num_gpus}GB" for i in range(args.num_gpus)}
            model_kwargs["max_memory"] = max_memory

        print(f"\nâš™ï¸  Using MODEL PARALLEL strategy:")
        print(f"   Model will be split across {args.num_gpus} GPUs")
        print(f"   Using single process with device_map='auto'")
        print(f"   Memory limit per GPU: {max_memory if args.num_gpus > 1 else 'auto'}\n")

        # æ¨¡å‹åˆ‡ç‰‡æ¨¡å¼ï¼šå•è¿›ç¨‹å¤„ç†æ‰€æœ‰æ•°æ®
        # ä¸ä½¿ç”¨multiprocessingï¼Œå› ä¸ºæ¨¡å‹å·²ç»è·¨å¤šGPU
        adapter = DatasetAdapter(args.dataset, args.dataset_split)

        total_samples = len(adapter)
        if args.num_samples is None:
            args.num_samples = total_samples - args.start_idx
        else:
            args.num_samples = min(args.num_samples, total_samples - args.start_idx)

        # åˆ›å»ºæ¨¡å‹ï¼ˆè·¨GPUï¼‰
        print("Loading model (will be split across GPUs)...")
        model = create_optimal_sampling_model(
            model_theta=args.model_theta,
            model_t=args.model_t,
            alpha_method=args.alpha_method,
            fixed_alpha=args.fixed_alpha,
            **model_kwargs
        )
        print("âœ“ Model loaded and distributed")

        # å•è¿›ç¨‹æ‰¹å¤„ç†
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ”¶é›†æ•°æ®ï¼ˆç”¨äºparquetï¼‰
        all_data = []
        all_diag_data = []

        # æ‰“å¼€æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦jsonlï¼‰
        f_out = None
        f_diag = None
        if args.output_format in ["jsonl", "both"]:
            output_file_jsonl = output_path.with_suffix('.jsonl') if args.output_format == "both" else output_path
            f_out = open(output_file_jsonl, 'w')
            if args.save_diagnostics:
                f_diag = open(output_path.with_suffix('.diagnostics.jsonl'), 'w')

        for batch_start in tqdm(
            range(args.start_idx, args.start_idx + args.num_samples, args.batch_size_per_gpu),
            desc="Processing"
        ):
            batch_end = min(batch_start + args.batch_size_per_gpu, args.start_idx + args.num_samples)
            batch_indices = range(batch_start, batch_end)

            # âœ¨ æå–messagesï¼ˆç”¨äºç”Ÿæˆdual promptsï¼‰
            messages_list = []
            metadata_list = []
            for idx in batch_indices:
                try:
                    messages = adapter.get_messages(idx)
                    metadata = adapter.get_metadata(idx)
                    messages_list.append(messages)
                    metadata_list.append(metadata)
                except Exception as e:
                    print(f"Warning: Failed to extract messages at {idx}: {e}")
                    continue

            if not messages_list:
                continue

            # âœ¨ ä½¿ç”¨ create_dual_prompts ç”Ÿæˆä¸¤ä¸ªä¸åŒçš„ prompts
            # Ï€_Î¸ ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿ï¼ŒÏ€_t ä½¿ç”¨ chat template
            try:
                prompts_theta, prompts_t = create_dual_prompts(
                    messages_list,
                    model.tokenizer_theta,
                    model.tokenizer_t,
                    force_nlt_in_theta=True,  # Baseæ¨¡å‹ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿
                    add_generation_prompt=True
                )
            except Exception as e:
                print(f"Warning: Failed to create dual prompts: {e}")
                # Fallback: ä½¿ç”¨ç®€å•çš„promptæå–
                prompts_theta = []
                prompts_t = []
                for messages in messages_list:
                    # ç®€å•æå–useræ¶ˆæ¯
                    user_content = ""
                    for msg in messages:
                        if msg.get("role") == "user":
                            user_content = msg.get("content", "")
                            break
                    prompts_theta.append(f"Question: {user_content}\n\nAnswer: ")
                    prompts_t.append(f"Question: {user_content}\n\nAnswer: ")

            try:
                outputs = model.generate(
                    prompts=prompts_theta,  # âœ¨ Ï€_Î¸ ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿
                    prompts_t=prompts_t,    # âœ¨ Ï€_t ä½¿ç”¨ chat template
                    max_new_tokens=args.max_new_tokens,
                    temperature=args.temperature,
                    return_diagnostics=args.save_diagnostics
                )

                for i, response in enumerate(outputs.generated_texts):
                    # è·å–åŸå§‹ promptï¼ˆç”¨äºä¿å­˜ï¼‰
                    prompt = prompts_theta[i]  # ä¿å­˜ Î¸ çš„ prompt
                    # å‡†å¤‡å…ƒæ•°æ®ï¼ˆåŒ…å«alphaç­‰è¯Šæ–­ä¿¡æ¯ï¼‰
                    metadata_with_diag = metadata_list[i].copy()
                    if args.save_diagnostics:
                        metadata_with_diag["alpha_mean"] = outputs.alpha_values[i].mean().item()
                        metadata_with_diag["alpha_std"] = outputs.alpha_values[i].std().item()
                        metadata_with_diag["ess_ratio_mean"] = outputs.ess_ratios[i].mean().item()

                    # Parquetæ ¼å¼ï¼ˆverlæ ¼å¼ï¼‰
                    if args.output_format in ["parquet", "both"]:
                        verl_data = adapter.to_verl_format(
                            prompt=prompt,
                            response=response,
                            metadata=metadata_with_diag
                        )
                        all_data.append(verl_data)

                    # JSONLæ ¼å¼ï¼ˆOpenAIæ ¼å¼ï¼‰
                    if args.output_format in ["jsonl", "both"]:
                        openai_data = adapter.to_openai_format(
                            prompt=prompt,
                            response=response,
                            metadata=metadata_list[i]
                        )
                        f_out.write(json.dumps(openai_data, ensure_ascii=False) + '\n')
                        f_out.flush()

                    # ä¿å­˜è¯Šæ–­ä¿¡æ¯
                    if args.save_diagnostics:
                        diag_data = {
                            "sample_idx": metadata_list[i].get("sample_idx"),
                            "alpha_mean": outputs.alpha_values[i].mean().item(),
                            "alpha_std": outputs.alpha_values[i].std().item(),
                            "ess_ratio_mean": outputs.ess_ratios[i].mean().item()
                        }
                        if args.output_format in ["jsonl", "both"]:
                            f_diag.write(json.dumps(diag_data, ensure_ascii=False) + '\n')
                            f_diag.flush()
                        all_diag_data.append(diag_data)

                    # âœ¨ æ‰“å°æ ·æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if args.print_samples:
                        sample_idx = metadata_list[i].get("sample_idx", -1)
                        print(f"\n{'='*80}")
                        print(f"Sample #{sample_idx}")
                        print(f"{'='*80}")
                        print(f"ğŸ“ Prompt:")
                        print(prompt)
                        print(f"\nğŸ’¬ Response:")
                        print(response)
                        if args.save_diagnostics:
                            print(f"\nğŸ“Š Diagnostics:")
                            print(f"  - Alpha (Ï€_t weight): {metadata_with_diag.get('alpha_mean', 0):.3f} Â± {metadata_with_diag.get('alpha_std', 0):.3f}")
                            print(f"  - ESS Ratio: {metadata_with_diag.get('ess_ratio_mean', 0):.3f}")
                        print(f"{'='*80}\n")

            except Exception as e:
                print(f"Error during generation: {e}")
                import traceback
                traceback.print_exc()
                continue

        # å…³é—­jsonlæ–‡ä»¶
        if f_out:
            f_out.close()
        if f_diag:
            f_diag.close()

        # å†™å…¥parquetæ–‡ä»¶
        if args.output_format in ["parquet", "both"]:
            if all_data:
                print("Writing parquet file...")
                dataset = Dataset.from_list(all_data)
                output_file_parquet = output_path.with_suffix('.parquet') if args.output_format == "both" else output_path
                dataset.to_parquet(str(output_file_parquet))
                print(f"âœ“ Parquet saved: {output_file_parquet}")

        print(f"\nâœ“ Generation completed!")
        if args.output_format == "jsonl":
            print(f"Output: {output_path}")
        elif args.output_format == "parquet":
            print(f"Output: {output_path}")
        else:  # both
            print(f"JSONL output: {output_path.with_suffix('.jsonl')}")
            print(f"Parquet output: {output_path.with_suffix('.parquet')}")

    else:  # data_parallel
        # æ•°æ®å¹¶è¡Œï¼šæ¯ä¸ªGPUç‹¬ç«‹åŠ è½½æ¨¡å‹ï¼Œå¤„ç†ä¸åŒçš„æ•°æ®
        print(f"\nâš™ï¸  Using DATA PARALLEL strategy:")
        print(f"   {args.num_gpus} independent processes, each with full model")
        print(f"   Data will be split across GPUs\n")

        # åŠ è½½æ•°æ®é›†ï¼ˆåªä¸ºäº†è·å–å¤§å°ï¼‰
        adapter = DatasetAdapter(args.dataset, args.dataset_split)
        total_samples = len(adapter)

        if args.num_samples is None:
            args.num_samples = total_samples - args.start_idx
        else:
            args.num_samples = min(args.num_samples, total_samples - args.start_idx)

        end_idx = args.start_idx + args.num_samples

        # ä¸ºæ¯ä¸ªGPUæŒ‡å®šdevice
        model_kwargs["device"] = "cuda"  # Will be set by worker

        # å¯åŠ¨å¤šè¿›ç¨‹
        mp.set_start_method('spawn', force=True)

        processes = []
        for rank in range(args.num_gpus):
            p = mp.Process(
                target=worker_process,
                args=(
                    rank,
                    args.num_gpus,
                    args.model_theta,
                    args.model_t,
                    args.dataset,
                    args.dataset_split,
                    args.output,
                    args.start_idx,
                    end_idx,
                    args.batch_size_per_gpu,
                    args.max_new_tokens,
                    args.temperature,
                    args.alpha_method,
                    args.fixed_alpha,
                    model_kwargs,
                    args.save_diagnostics,
                    args.output_format,  # âœ¨ ä¼ é€’output_format
                    args.print_samples   # âœ¨ æ–°å¢ï¼šä¼ é€’print_samples
                )
            )
            p.start()
            processes.append(p)

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        for p in processes:
            p.join()

        # åˆå¹¶è¾“å‡º
        merge_outputs(args.output, args.num_gpus, output_format=args.output_format, delete_temp=not args.keep_temp_files)

    print("\n" + "="*60)
    print("âœ… All done!")
    print("="*60)


if __name__ == "__main__":
    main()
