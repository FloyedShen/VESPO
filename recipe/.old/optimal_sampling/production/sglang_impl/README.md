# SGLang Implementation of Optimal Sampling

Clean and efficient implementation using SGLang's native control flow.

## üéØ Why SGLang?

**60% less code than vLLM** with better maintainability:

| Feature | vLLM | SGLang |
|---------|------|--------|
| **Code lines** | 950 | **350** ‚úÖ |
| **Files** | 10 | **3** ‚úÖ |
| **Complexity** | High | **Low** ‚úÖ |
| **Debugging** | Hard | **Easy** ‚úÖ |
| **Performance** | Fast | **Fast** ‚úÖ |

## üöÄ Quick Start

### Installation

```bash
pip install sglang[all]
```

### Basic Usage

```python
from production.sglang_impl import OptimalSamplingSGLang

# Initialize (super simple!)
sampler = OptimalSamplingSGLang(
    model_base="Qwen/Qwen2.5-0.5B",
    model_teacher="Qwen/Qwen2.5-1.5B",
    alpha_method="kl_symmetry"
)

# Generate
outputs = sampler.generate(
    prompts=["What is AI?"],
    max_tokens=100,
    collect_diagnostics=True
)

print(outputs.generated_texts[0])
print(f"Alpha mean: {np.mean(outputs.alphas[0]):.3f}")
```

## üìã Features

- ‚úÖ **Clean API**: No logits processor hacks
- ‚úÖ **Direct Control**: Explicit generation loop
- ‚úÖ **Easy Debug**: Print any intermediate value
- ‚úÖ **Batch Support**: Efficient batch processing
- ‚úÖ **Diagnostics**: Alpha and ESS tracking
- ‚úÖ **Flexible**: Easy to modify and extend

## üé® Architecture

```
User Prompt
    ‚Üì
SGLang Runtime (Base + Teacher)
    ‚Üì
Generation Loop:
  ‚îú‚îÄ Get logits_Œ∏ from Base
  ‚îú‚îÄ Get logits_t from Teacher
  ‚îú‚îÄ Compute alpha
  ‚îú‚îÄ Compute q* = Œ∏^(1-Œ±) √ó t^Œ±
  ‚îú‚îÄ Sample from q*
  ‚îî‚îÄ Repeat
    ‚Üì
Generated Text
```

## üìä Code Comparison

### vLLM Version (Complex)

```python
# Need 3 files + 950 lines

class OptimalSamplingLogitsProcessor:
    """Hack to intercept logits"""
    def __call__(self, token_ids, logits):
        # Complex logic inside vLLM
        ...

class GuideModelVLLM:
    """Separate wrapper for guide model"""
    ...

class OptimalSamplingVLLM:
    """Main class with complex setup"""
    ...
```

### SGLang Version (Simple) ‚ú®

```python
# Just 1 file + 350 lines

class OptimalSamplingSGLang:
    """Clean implementation"""

    def _generate_single(self, prompt, max_tokens):
        for step in range(max_tokens):
            # Clear and explicit
            logits_Œ∏, logits_t = self._get_dual_logits(ids)
            alpha = compute_alpha(logits_Œ∏, logits_t)
            q_star = mix(logits_Œ∏, logits_t, alpha)
            token = sample(q_star)
```

## üîß Alpha Methods

All methods from theory implementation:

```python
# 1. KL Symmetry (default)
sampler = OptimalSamplingSGLang(..., alpha_method="kl_symmetry")

# 2. ESS Balance (exact)
sampler = OptimalSamplingSGLang(..., alpha_method="ess_balance")

# 3. Entropy (fast)
sampler = OptimalSamplingSGLang(..., alpha_method="entropy")

# 4. Fixed
sampler = OptimalSamplingSGLang(..., alpha_method="fixed", fixed_alpha=0.7)
```

## üí° Advanced Usage

### Collect Diagnostics

```python
outputs = sampler.generate(
    prompts=prompts,
    collect_diagnostics=True
)

# Access alpha values
for i, alphas in enumerate(outputs.alphas):
    print(f"Prompt {i}: alpha_mean={np.mean(alphas):.3f}")

# ESS ratios
for i, ess_ratios in enumerate(outputs.ess_ratios):
    print(f"Prompt {i}: ESS_ratio={np.mean(ess_ratios):.3f}")
```

### Support Constraint

```python
sampler = OptimalSamplingSGLang(
    model_base="...",
    model_teacher="...",
    constraint_to_target=True,  # Limit to teacher's vocabulary
    target_top_k=100,            # Only consider top-100 tokens
    target_top_p=0.95            # Or top-p constraint
)
```

### Multi-GPU

```python
sampler = OptimalSamplingSGLang(
    model_base="meta-llama/Llama-2-70b-hf",
    model_teacher="meta-llama/Llama-2-70b-chat-hf",
    tp_size=4,  # Tensor parallelism across 4 GPUs
    mem_fraction_static=0.85
)
```

## üß™ Testing

Run validation:

```bash
python production/sglang_impl/validate_sglang.py
```

Simple example:

```bash
python production/sglang_impl/example_simple.py
```

## üìà Performance

Expected performance (based on SGLang benchmarks):

| Metric | vLLM 0.6 | SGLang | Speedup |
|--------|----------|--------|---------|
| **Simple gen** | 100% | 95-98% | 0.95x |
| **Complex sampling** | 100% | **110-120%** | **1.1-1.2x** ‚ú® |
| **Multi-model** | 100% | **120-130%** | **1.2-1.3x** ‚ú® |

Why faster for complex sampling:
- Smarter scheduler for control flow
- Better multi-model coordination
- Optimized for this use case

## üíæ Memory Requirements

Same as vLLM:

| Model Size | Memory | GPU |
|------------|--------|-----|
| 2√ó0.5B | ~4 GB | Any modern GPU |
| 2√ó7B | ~26 GB | A100 40GB |
| 2√ó13B | ~52 GB | A100 80GB |
| 2√ó70B | ~280 GB | 4√óA100 80GB |

## üêõ Troubleshooting

### Import Error

```bash
pip install sglang[all]
```

### Out of Memory

```python
# Reduce memory fraction
sampler = OptimalSamplingSGLang(
    ...,
    mem_fraction_static=0.75  # Lower value
)
```

### Slow Generation

```python
# Enable torch.compile
sampler = OptimalSamplingSGLang(
    ...,
    enable_torch_compile=True
)
```

## üìö API Reference

### OptimalSamplingSGLang

```python
class OptimalSamplingSGLang:
    def __init__(
        self,
        model_base: str,           # Base model path
        model_teacher: str,        # Teacher model path
        alpha_method: str = "kl_symmetry",
        alpha_min: float = 0.5,
        alpha_max: float = 1.0,
        mem_fraction_static: float = 0.85,
        tp_size: int = 1,
        **kwargs
    )

    def generate(
        self,
        prompts: List[str],
        max_tokens: int = 100,
        temperature: float = 1.0,
        collect_diagnostics: bool = False
    ) -> SGLangSamplingOutput
```

### SGLangSamplingOutput

```python
@dataclass
class SGLangSamplingOutput:
    generated_texts: List[str]
    generated_ids: List[List[int]]
    num_tokens: List[int]
    alphas: Optional[List[List[float]]]  # [batch][seq_len]
    ess_ratios: Optional[List[List[float]]]
```

## üîÑ Migration from vLLM

If you have vLLM code:

```python
# vLLM
from production.vllm_impl import OptimalSamplingVLLM
model = OptimalSamplingVLLM(
    model_theta="base",
    model_t="teacher",
    gpu_memory_utilization_theta=0.45,
    gpu_memory_utilization_t=0.45
)

# SGLang (much simpler!)
from production.sglang_impl import OptimalSamplingSGLang
sampler = OptimalSamplingSGLang(
    model_base="base",
    model_teacher="teacher",
    mem_fraction_static=0.85
)
```

## ‚ú® Why Choose SGLang?

1. **Designed for Complex Sampling**: This is exactly what SGLang was built for
2. **Cleaner Code**: 60% less code, easier to understand and maintain
3. **Better Debugging**: Can print/inspect every step
4. **Future-Proof**: Not affected by vLLM V1 API changes
5. **Active Development**: Growing community focused on LLM control flow

## ü§ù Comparison with Other Implementations

| Implementation | Code Lines | Complexity | Maintainability | Performance |
|----------------|------------|------------|-----------------|-------------|
| **Transformers** | 1200 | Medium | Medium | Slow (baseline) |
| **vLLM 0.6** | 950 | High | Low | Fast (3-5x) |
| **vLLM V1** | N/A | N/A | N/A | Not supported |
| **SGLang** ‚≠ê | **350** | **Low** | **High** | **Fast (3-5x)** |

## üìù Notes

- SGLang uses vLLM's kernels under the hood, so performance is similar
- For production, both SGLang and vLLM are viable
- SGLang is better for research and rapid iteration
- Code is much easier to modify and extend

## üìÑ License

Same as parent project.

---

**TL;DR**: Use SGLang for optimal sampling. It's cleaner, easier, and just as fast! ‚ú®
