"""
OptimalSamplingModel: æœ€ä¼˜é‡‡æ ·åˆ†å¸ƒçš„å®ç°

æ”¯æŒç‰¹æ€§:
- ä¸¤ç§backend: transformers å’Œ VLLM
- å¤šç§alphaè®¡ç®—æ–¹æ³•: fixed, kl_symmetry, entropy
- q* åˆ†å¸ƒè®¡ç®—å’Œé‡‡æ ·
- å®Œæ•´çš„è¯Šæ–­ä¿¡æ¯ (ESS, KLæ•£åº¦ç­‰)

æ ¸å¿ƒæ¦‚å¿µ:
- Ï€_Î¸: Baseæ¨¡å‹ï¼ˆå¦‚Llama-2-7bï¼‰
- Ï€_t: Teacheræ¨¡å‹ï¼ˆå¦‚Llama-2-7b-chatï¼Œé€šå¸¸æ˜¯Instructæ¨¡å‹ï¼‰
- q*: æœ€ä¼˜æ··åˆåˆ†å¸ƒ q*(x) = Ï€_Î¸(x)^(1-Î±) Ã— Ï€_t(x)^Î±
- Î±: **Teacheræ¨¡å‹çš„æƒé‡** (Î±=0â†’Base, Î±=1â†’Teacher, Î±>0.5â†’æ›´æ¥è¿‘Teacher)

ä½¿ç”¨ç¤ºä¾‹:
    model = OptimalSamplingModel(
        model_theta_path="meta-llama/Llama-2-7b-hf",        # Base model
        model_t_path="meta-llama/Llama-2-7b-chat-hf",       # Teacher/Instruct model
        backend="transformers",
        alpha_method="kl_symmetry"
    )

    outputs = model.generate(
        prompts=["Hello, how are you?"],
        max_new_tokens=100,
        temperature=1.0
    )

    # é€šå¸¸æœŸæœ› alpha > 0.5ï¼Œå› ä¸ºTeacheræ¨¡å‹è´¨é‡æ›´é«˜
    print(f"Average Î±: {outputs.alpha_values.mean():.3f}")
"""

import torch
import torch.nn.functional as F
from typing import List, Dict, Optional, Union, Literal
from dataclasses import dataclass
import numpy as np
from tqdm import tqdm


@dataclass
class SamplingOutput:
    """é‡‡æ ·è¾“å‡ºç»“æœ"""
    generated_texts: List[str]  # decodeåçš„æ–‡æœ¬ï¼ˆå¦‚æœskip_decode=Falseï¼‰
    generated_ids: torch.Tensor  # ç”Ÿæˆçš„token IDs [batch, seq_len]
    alpha_values: torch.Tensor  # Alphaå€¼ [batch, seq_len]
    ess_ratios: torch.Tensor    # ESSæ¯”ä¾‹ [batch, seq_len]
    diagnostics: Dict[str, any]  # è¯Šæ–­ä¿¡æ¯
    logits: Optional[Dict[str, torch.Tensor]] = None  # âœ¨ æ–°å¢ï¼šæ¯ä¸€æ­¥çš„logits {"theta": [...], "t": [...]}
    q_star_probs: Optional[torch.Tensor] = None  # âœ¨ æ–°å¢ï¼šq*æ¦‚ç‡åˆ†å¸ƒ [batch, seq_len, vocab_size]


class AlphaComputer:
    """Alphaå‚æ•°è®¡ç®—å™¨ï¼ˆå¢å¼ºæ•°å€¼ç¨³å®šæ€§ç‰ˆæœ¬ï¼‰

    âœ… é‡è¦ï¼šAlphaè¯­ä¹‰å®šä¹‰
    ==================
    Î± è¡¨ç¤º **Teacheræ¨¡å‹ (Ï€_t)** çš„æƒé‡ï¼š
    - Î± = 0 â†’ å®Œå…¨ä½¿ç”¨ Baseæ¨¡å‹ (Ï€_Î¸)
    - Î± = 1 â†’ å®Œå…¨ä½¿ç”¨ Teacheræ¨¡å‹ (Ï€_t)
    - Î± > 0.5 â†’ æ›´æ¥è¿‘ Teacherï¼ˆç¬¦åˆç›´è§‰ï¼Œå› ä¸ºTeacherè´¨é‡æ›´é«˜ï¼‰

    æ··åˆå…¬å¼ï¼šq*(x) = Ï€_Î¸(x)^(1-Î±) Ã— Ï€_t(x)^Î±
    """

    def __init__(self, method: str = "kl_symmetry", fixed_alpha: float = 0.5,
                 tol: float = 1e-6, max_iter: int = 12,
                 constraint_to_target: bool = False,
                 target_top_k: int = -1,
                 target_top_p: float = 1.0):
        """
        Args:
            method: alphaè®¡ç®—æ–¹æ³• ["fixed", "kl_symmetry", "reverse_kl_symmetry", "ess_balance", "entropy"]
            fixed_alpha: å½“method="fixed"æ—¶ä½¿ç”¨çš„å›ºå®šå€¼ï¼ˆÎ±=0.5è¡¨ç¤ºå‡åŒ€æ··åˆï¼‰
            tol: KLå¯¹ç§°/ESSå¹³è¡¡æ±‚è§£çš„å®¹å·®
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
            constraint_to_target: æ˜¯å¦é™åˆ¶åœ¨Ï€_tçš„supportä¸Šï¼ˆæ¨èTrueï¼‰
            target_top_k: Ï€_tçš„top-ké™åˆ¶ï¼ˆ-1è¡¨ç¤ºä¸é™åˆ¶ï¼‰
            target_top_p: Ï€_tçš„top-pé™åˆ¶ï¼ˆ1.0è¡¨ç¤ºä¸é™åˆ¶ï¼‰

        Note:
            Î±è¡¨ç¤ºTeacher (Ï€_t)çš„æƒé‡ã€‚é€šå¸¸æœŸæœ›Î± > 0.5ï¼Œå› ä¸ºTeacheræ¨¡å‹è´¨é‡æ›´é«˜ã€‚
        """
        self.method = method
        self.fixed_alpha = fixed_alpha
        self.tol = tol
        self.max_iter = max_iter
        self.eps = 1e-10

        # âœ… æ–°å¢ï¼šSupportçº¦æŸ
        self.constraint_to_target = constraint_to_target
        self.target_top_k = target_top_k
        self.target_top_p = target_top_p

    def compute(self, probs_theta: torch.Tensor, probs_t: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—alphaï¼ˆâœ¨ æ”¹è¿›ï¼šä¸å†æ–½åŠ supportçº¦æŸï¼Œä¿æŒalphaè®¡ç®—çº¯ç²¹ï¼‰

        Args:
            probs_theta: [batch, vocab_size] å½“å‰ç­–ç•¥æ¦‚ç‡
            probs_t: [batch, vocab_size] ç›®æ ‡ç­–ç•¥æ¦‚ç‡

        Returns:
            alpha: [batch] alphaå€¼

        Note:
            âœ¨ æ–°æ”¹è¿›ï¼šSupportçº¦æŸç°åœ¨åœ¨q*è®¡ç®—ä¹‹åæ–½åŠ ï¼Œä¸å½±å“alphaè®¡ç®—ã€‚
            è¿™æ ·å¯ä»¥é¿å…çº¦æŸè®©ä¸¤ä¸ªåˆ†å¸ƒå˜å¾—ç›¸ä¼¼ï¼Œå¯¼è‡´alphaåå°çš„é—®é¢˜ã€‚
        """
        # âœ… ç§»é™¤ï¼šä¸å†åœ¨alphaè®¡ç®—æ—¶æ–½åŠ constraint
        # åŸå› ï¼šconstraintä¼šè®©probs_thetaå’Œprobs_tå˜å¾—æ›´ç›¸ä¼¼ï¼Œå½±å“alphaè®¡ç®—
        # æ–°æ–¹æ¡ˆï¼šå…ˆç®—alphaï¼Œå†å¯¹q*æ–½åŠ constraint

        if self.method == "fixed":
            return self._fixed(probs_theta)
        elif self.method == "kl_symmetry":
            return self._kl_symmetry(probs_theta, probs_t)
        elif self.method == "reverse_kl_symmetry":
            return self._reverse_kl_symmetry(probs_theta, probs_t)
        elif self.method == "ess_balance":
            return self._ess_balance(probs_theta, probs_t)
        elif self.method == "entropy":
            return self._entropy(probs_theta, probs_t)
        else:
            raise ValueError(f"Unknown alpha method: {self.method}")

    def _apply_support_constraint(self, probs_theta: torch.Tensor, probs_t: torch.Tensor) -> tuple:
        """
        é™åˆ¶åœ¨Ï€_tçš„supportä¸Šï¼ˆåªåœ¨Ï€_tè®¤ä¸ºåˆç†çš„tokenä¸Šæ··åˆï¼‰

        è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ•°å€¼ç¨³å®šæŠ€å·§ï¼š
        - é¿å…Base modelçš„å¼‚å¸¸token
        - åªåœ¨Instruct modelæ”¯æŒçš„ç©ºé—´ä¸Šåšæ··åˆ
        - å¤§å¹…æå‡æ•°å€¼ç¨³å®šæ€§

        Args:
            probs_theta: [batch, vocab_size]
            probs_t: [batch, vocab_size]

        Returns:
            (probs_theta_masked, probs_t_masked): çº¦æŸåçš„æ¦‚ç‡åˆ†å¸ƒ
        """
        batch_size, vocab_size = probs_t.shape

        # åˆ›å»ºmaskï¼ˆæ ‡è®°Ï€_tæ”¯æŒçš„tokenï¼‰
        mask = torch.ones_like(probs_t, dtype=torch.bool)

        # Top-k çº¦æŸ
        if self.target_top_k > 0:
            k = min(self.target_top_k, vocab_size)
            _, top_k_indices = torch.topk(probs_t, k=k, dim=-1)

            # åˆ›å»ºtop-k mask
            mask_k = torch.zeros_like(probs_t, dtype=torch.bool)
            mask_k.scatter_(-1, top_k_indices, True)
            mask = mask & mask_k

        # Top-p çº¦æŸ
        if self.target_top_p < 1.0:
            sorted_probs, sorted_indices = torch.sort(probs_t, descending=True, dim=-1)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)

            # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
            indices_to_keep = cumsum_probs <= self.target_top_p
            # è‡³å°‘ä¿ç•™ç¬¬ä¸€ä¸ªtoken
            indices_to_keep[..., 0] = True

            # åˆ›å»ºtop-p mask
            mask_p = torch.zeros_like(probs_t, dtype=torch.bool)
            mask_p.scatter_(-1, sorted_indices, indices_to_keep)
            mask = mask & mask_p

        # åº”ç”¨mask
        probs_theta_masked = probs_theta * mask.float()
        probs_t_masked = probs_t * mask.float()

        # é‡æ–°å½’ä¸€åŒ–
        probs_theta_masked = probs_theta_masked / (probs_theta_masked.sum(dim=-1, keepdim=True) + self.eps)
        probs_t_masked = probs_t_masked / (probs_t_masked.sum(dim=-1, keepdim=True) + self.eps)

        return probs_theta_masked, probs_t_masked

    def apply_constraint_to_q_star(self, q_star: torch.Tensor, probs_t: torch.Tensor) -> torch.Tensor:
        """
        âœ¨ æ–°æ–¹æ³•ï¼šå¯¹q*æ–½åŠ supportçº¦æŸï¼ˆé™åˆ¶åœ¨Ï€_tçš„supportä¸Šï¼‰

        è¿™æ˜¯æ”¹è¿›ç‰ˆçš„çº¦æŸåº”ç”¨æ–¹å¼ï¼š
        - å…ˆåœ¨å®Œæ•´ç©ºé—´è®¡ç®—alphaï¼ˆé¿å…çº¦æŸå½±å“alphaï¼‰
        - è®¡ç®—å®Œæ•´çš„q*
        - æœ€åå¯¹q*æ–½åŠ çº¦æŸï¼ˆåªä¿ç•™Ï€_tæ”¯æŒçš„tokenï¼‰

        Args:
            q_star: [batch, vocab_size] q*æ¦‚ç‡åˆ†å¸ƒ
            probs_t: [batch, vocab_size] Ï€_tæ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äºç¡®å®šsupportï¼‰

        Returns:
            q_star_constrained: [batch, vocab_size] çº¦æŸåçš„q*åˆ†å¸ƒ
        """
        if not self.constraint_to_target or (self.target_top_k <= 0 and self.target_top_p >= 1.0):
            # ä¸éœ€è¦çº¦æŸï¼Œç›´æ¥è¿”å›
            return q_star

        batch_size, vocab_size = probs_t.shape

        # åˆ›å»ºmaskï¼ˆæ ‡è®°Ï€_tæ”¯æŒçš„tokenï¼‰
        mask = torch.ones_like(probs_t, dtype=torch.bool)

        # Top-k çº¦æŸ
        if self.target_top_k > 0:
            k = min(self.target_top_k, vocab_size)
            _, top_k_indices = torch.topk(probs_t, k=k, dim=-1)

            # åˆ›å»ºtop-k mask
            mask_k = torch.zeros_like(probs_t, dtype=torch.bool)
            mask_k.scatter_(-1, top_k_indices, True)
            mask = mask & mask_k

        # Top-p çº¦æŸ
        if self.target_top_p < 1.0:
            sorted_probs, sorted_indices = torch.sort(probs_t, descending=True, dim=-1)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)

            # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
            indices_to_keep = cumsum_probs <= self.target_top_p
            # è‡³å°‘ä¿ç•™ç¬¬ä¸€ä¸ªtoken
            indices_to_keep[..., 0] = True

            # åˆ›å»ºtop-p mask
            mask_p = torch.zeros_like(probs_t, dtype=torch.bool)
            mask_p.scatter_(-1, sorted_indices, indices_to_keep)
            mask = mask & mask_p

        # åº”ç”¨maskåˆ°q*
        q_star_masked = q_star * mask.float()

        # é‡æ–°å½’ä¸€åŒ–
        q_star_masked = q_star_masked / (q_star_masked.sum(dim=-1, keepdim=True) + self.eps)

        return q_star_masked


    def _fixed(self, probs_theta: torch.Tensor) -> torch.Tensor:
        """å›ºå®šalpha"""
        batch_size = probs_theta.shape[0]
        return torch.full((batch_size,), self.fixed_alpha,
                         device=probs_theta.device, dtype=probs_theta.dtype)

    def _kl_symmetry(self, probs_theta: torch.Tensor, probs_t: torch.Tensor) -> torch.Tensor:
        """
        äºŒåˆ†æ³•æ±‚è§£KLå¯¹ç§°æ¡ä»¶ï¼ˆå¢å¼ºæ•°å€¼ç¨³å®šæ€§ç‰ˆæœ¬ï¼‰

        ç›®æ ‡: D_KL(q||Ï€_Î¸) = D_KL(q||Ï€_t)
        ç­‰ä»·äº: E_q[log(Ï€_t/Ï€_Î¸)] = 0

        æ”¹è¿›ï¼š
        - âœ… Clampè¾“å…¥æ¦‚ç‡é¿å…log(0)
        - âœ… åœ¨log-spaceè®¡ç®—ï¼Œé¿å…æ•°å€¼æº¢å‡º
        - âœ… æ£€æµ‹æç«¯æƒ…å†µå¹¶æå‰è¿”å›
        - âœ… ä½¿ç”¨logsumexpç¡®ä¿æ•°å€¼ç¨³å®š
        - âœ¨ æ–°å¢ï¼šæ”¶æ•›æ£€æµ‹å’Œæ™ºèƒ½fallback

        æ³¨æ„: è¿™æ˜¯ESSå¹³è¡¡æ¡ä»¶çš„ä¸€é˜¶è¿‘ä¼¼
        """
        batch_size = probs_theta.shape[0]
        device = probs_theta.device

        # âœ… æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤1ï¼šClampè¾“å…¥æ¦‚ç‡
        probs_theta = torch.clamp(probs_theta, min=self.eps, max=1.0)
        probs_t = torch.clamp(probs_t, min=self.eps, max=1.0)

        # âœ¨ å¢å¼º1ï¼šæ£€æµ‹åˆ†å¸ƒæ˜¯å¦å‡ ä¹ç›¸åŒ
        max_diff = (probs_theta - probs_t).abs().max()
        if max_diff < 1e-6:
            # ä¸¤ä¸ªåˆ†å¸ƒå‡ ä¹å®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›0.5
            return torch.full((batch_size,), 0.5, device=device, dtype=probs_theta.dtype)

        # äºŒåˆ†æœç´¢
        alpha_low = torch.full((batch_size,), 0.0, device=device)
        alpha_high = torch.full((batch_size,), 1.0, device=device)

        # âœ… é¢„è®¡ç®—logæ¦‚ç‡ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        log_probs_theta = torch.log(probs_theta)
        log_probs_t = torch.log(probs_t)
        log_ratio = log_probs_t - log_probs_theta  # log(Ï€_t/Ï€_Î¸)

        # âœ¨ å¢å¼º2ï¼šè·Ÿè¸ªæ”¶æ•›çŠ¶æ€
        converged = torch.zeros(batch_size, dtype=torch.bool, device=device)

        for iteration in range(self.max_iter):
            alpha_mid = (alpha_low + alpha_high) / 2

            # âœ… åœ¨log-spaceè®¡ç®— q_alpha
            # âœ… ä¿®æ”¹ï¼šlog q = (1-Î±) log Ï€_Î¸ + Î± log Ï€_t ï¼ˆÎ±ç°åœ¨æ˜¯Ï€_tçš„æƒé‡ï¼‰
            alpha_expanded = alpha_mid.unsqueeze(-1)
            log_q_unnormalized = (1 - alpha_expanded) * log_probs_theta + alpha_expanded * log_probs_t

            # âœ… ä½¿ç”¨logsumexpå½’ä¸€åŒ–ï¼ˆæ•°å€¼ç¨³å®šï¼‰
            log_q = log_q_unnormalized - torch.logsumexp(log_q_unnormalized, dim=-1, keepdim=True)
            q_alpha = torch.exp(log_q)

            # è®¡ç®— Î”(Î±) = E_q[log(Ï€_t/Ï€_Î¸)]
            delta = (q_alpha * log_ratio).sum(dim=-1)

            # âœ¨ å¢å¼º3ï¼šæ£€æŸ¥deltaæ˜¯å¦æœ‰æ•ˆï¼ˆNaN/Infæ£€æµ‹ï¼‰
            invalid = torch.isnan(delta) | torch.isinf(delta)
            if invalid.any():
                # å¯¹æ— æ•ˆçš„æ ·æœ¬ï¼Œæ ‡è®°ä¸ºå·²æ”¶æ•›å¹¶ä½¿ç”¨entropy fallback
                converged = converged | invalid
                delta = torch.where(invalid, torch.tensor(0.0, device=device), delta)

            # âœ… ä¿®æ”¹ï¼šæ›´æ–°åŒºé—´ï¼ˆåè½¬ä¸ç­‰å¼ï¼Œå› ä¸ºÎ±å«ä¹‰å˜äº†ï¼‰
            # delta > 0 â†’ qåå‘Ï€_t â†’ éœ€è¦å‡å°Î±ï¼ˆå‡å°Ï€_tæƒé‡ï¼‰
            mask = delta > 0
            alpha_high = torch.where(mask, alpha_mid, alpha_high)  # åè½¬ï¼
            alpha_low = torch.where(mask, alpha_low, alpha_mid)    # åè½¬ï¼

            # âœ¨ å¢å¼º4ï¼šæ£€æŸ¥æ”¶æ•›
            width = alpha_high - alpha_low
            newly_converged = width < self.tol
            converged = converged | newly_converged

            if converged.all():
                break

        alpha_result = (alpha_low + alpha_high) / 2

        # # âœ¨ å¢å¼º5ï¼šå¯¹æœªæ”¶æ•›çš„æ ·æœ¬ä½¿ç”¨entropy fallback
        # if not converged.all():
        #     num_not_converged = (~converged).sum().item()
        #     if num_not_converged > 0:
        #         print(f"âš ï¸  Warning: {num_not_converged}/{batch_size} samples did not converge in KL symmetry")
        #         print(f"   Using entropy-based fallback for these samples")
        #
        #         # Entropy-based alpha
        #         h_theta = -(probs_theta * torch.log(probs_theta + self.eps)).sum(dim=-1)
        #         h_t = -(probs_t * torch.log(probs_t + self.eps)).sum(dim=-1)
        #         alpha_entropy = h_theta / (h_theta + h_t + self.eps)
        #
        #         # å¯¹æœªæ”¶æ•›çš„æ ·æœ¬ä½¿ç”¨entropy fallback
        #         alpha_result = torch.where(converged, alpha_result, alpha_entropy)

        # âœ… æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤6ï¼šæœ€ç»ˆclampå’ŒNaNæ£€æŸ¥
        alpha_result = torch.clamp(alpha_result, min=0.0, max=1.0)

        # âœ¨ å¢å¼º6ï¼šæœ€ç»ˆNaNæ£€æŸ¥
        if torch.isnan(alpha_result).any():
            nan_count = torch.isnan(alpha_result).sum().item()
            print(f"âŒ CRITICAL: Alpha has {nan_count} NaN after KL symmetry computation")
            print(f"   Falling back to Î±=0.75 for all NaN positions")
            alpha_result = torch.where(torch.isnan(alpha_result),
                                        torch.tensor(0.75, device=device),
                                        alpha_result)

        return alpha_result

    def _reverse_kl_symmetry(self, probs_theta: torch.Tensor, probs_t: torch.Tensor) -> torch.Tensor:
        """
        åå‘KLå¯¹ç§°æ¡ä»¶ï¼ˆå¢å¼ºæ•°å€¼ç¨³å®šæ€§ç‰ˆæœ¬ï¼‰

        ç›®æ ‡: D_KL(Ï€_Î¸||q) = D_KL(Ï€_t||q)
        ç­‰ä»·äº: E_{Ï€_Î¸}[log q] = E_{Ï€_t}[log q]
        æˆ–: Î£ (Ï€_Î¸ - Ï€_t) log q = 0

        ç†è®ºå¯¹æ¯”ï¼š
        - å‰å‘KL (kl_symmetry): D_KL(q||Ï€_Î¸) = D_KL(q||Ï€_t)
          * Mode-seekingï¼ˆæ¨¡å¼è¿½è¸ªï¼‰
          * qå€¾å‘äºé›†ä¸­åœ¨Ï€çš„å•ä¸€æ¨¡å¼ä¸Š
          * ä»qé‡‡æ ·çš„è§†è§’ï¼ˆç¬¦åˆImportance Samplingï¼‰

        - åå‘KL (reverse_kl_symmetry): D_KL(Ï€_Î¸||q) = D_KL(Ï€_t||q)
          * Mode-coveringï¼ˆæ¨¡å¼è¦†ç›–ï¼‰
          * qå€¾å‘äºè¦†ç›–Ï€çš„æ‰€æœ‰æ¨¡å¼
          * ä»Ï€é‡‡æ ·çš„è§†è§’ï¼ˆæ›´æ¢ç´¢æ€§ï¼‰

        æ³¨æ„ï¼šåå‘KLå¯¹ç§°ä¸æ˜¯ESSå¹³è¡¡çš„ç›´æ¥è¿‘ä¼¼ï¼Œ
        è€Œæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‡†åˆ™ã€‚å®ƒä¼šäº§ç”Ÿæ›´åˆ†æ•£ã€entropyæ›´é«˜çš„q*ã€‚
        """
        batch_size = probs_theta.shape[0]
        device = probs_theta.device

        # âœ… æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤1ï¼šClampè¾“å…¥æ¦‚ç‡
        probs_theta = torch.clamp(probs_theta, min=self.eps, max=1.0)
        probs_t = torch.clamp(probs_t, min=self.eps, max=1.0)

        # âœ… æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤2ï¼šæ£€æµ‹æç«¯æƒ…å†µ
        max_ratio = (probs_theta / probs_t).max(dim=-1)[0]
        min_ratio = (probs_theta / probs_t).min(dim=-1)[0]
        nearly_identical = (max_ratio < 1.1) & (min_ratio > 0.9)

        if nearly_identical.any():
            alpha_result = torch.full((batch_size,), 0.5, device=device, dtype=probs_theta.dtype)
            if nearly_identical.all():
                return alpha_result

        # äºŒåˆ†æœç´¢
        alpha_low = torch.full((batch_size,), 0.1, device=device)
        alpha_high = torch.full((batch_size,), 0.9, device=device)

        # âœ… é¢„è®¡ç®—logæ¦‚ç‡ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        log_probs_theta = torch.log(probs_theta)
        log_probs_t = torch.log(probs_t)

        for iteration in range(self.max_iter):
            alpha_mid = (alpha_low + alpha_high) / 2

            # âœ… åœ¨log-spaceè®¡ç®— q_alpha
            # âœ… ä¿®æ”¹ï¼šlog q = (1-Î±) log Ï€_Î¸ + Î± log Ï€_t ï¼ˆÎ±ç°åœ¨æ˜¯Ï€_tçš„æƒé‡ï¼‰
            alpha_expanded = alpha_mid.unsqueeze(-1)
            log_q_unnormalized = (1 - alpha_expanded) * log_probs_theta + alpha_expanded * log_probs_t

            # âœ… ä½¿ç”¨logsumexpå½’ä¸€åŒ–ï¼ˆæ•°å€¼ç¨³å®šï¼‰
            log_q = log_q_unnormalized - torch.logsumexp(log_q_unnormalized, dim=-1, keepdim=True)

            # è®¡ç®— Î”(Î±) = E_{Ï€_Î¸}[log q] - E_{Ï€_t}[log q]
            #           = Î£ Ï€_Î¸(x) log q(x) - Î£ Ï€_t(x) log q(x)
            #           = Î£ (Ï€_Î¸(x) - Ï€_t(x)) log q(x)
            delta = ((probs_theta - probs_t) * log_q).sum(dim=-1)

            # âœ… æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤3ï¼šæ£€æŸ¥deltaæ˜¯å¦æœ‰æ•ˆ
            invalid = torch.isnan(delta) | torch.isinf(delta)
            if invalid.any():
                alpha_mid = torch.where(invalid, torch.tensor(0.5, device=device), alpha_mid)
                delta = torch.where(invalid, torch.tensor(0.0, device=device), delta)

            # âœ… ä¿®æ”¹ï¼šæ›´æ–°åŒºé—´ï¼ˆåè½¬ä¸ç­‰å¼ï¼Œå› ä¸ºÎ±å«ä¹‰å˜äº†ï¼‰
            # å½“ delta > 0 æ—¶ï¼Œè¯´æ˜ E_{Ï€_Î¸}[log q] > E_{Ï€_t}[log q]
            # æ„å‘³ç€qåœ¨Ï€_Î¸è®¤ä¸ºå¯èƒ½çš„åŒºåŸŸç»™äºˆäº†æ›´é«˜çš„æ¦‚ç‡
            # éœ€è¦å¢å¤§Ï€_tçš„æƒé‡ï¼Œå³å¢å¤§Î±ï¼ˆæ–°å®šä¹‰ï¼‰
            mask = delta > 0
            alpha_low = torch.where(mask, alpha_mid, alpha_low)   # åè½¬ï¼
            alpha_high = torch.where(mask, alpha_high, alpha_mid) # åè½¬ï¼

            # æ£€æŸ¥æ”¶æ•›
            if (alpha_high - alpha_low).max() < self.tol:
                break

        alpha_result = (alpha_low + alpha_high) / 2

        # âœ… æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤4ï¼šæœ€ç»ˆclamp
        alpha_result = torch.clamp(alpha_result, min=0.1, max=0.9)

        # å¤„ç†ä¹‹å‰æ£€æµ‹åˆ°çš„nearly_identicalæƒ…å†µ
        if nearly_identical.any():
            alpha_result = torch.where(nearly_identical, torch.tensor(0.5, device=device), alpha_result)

        return alpha_result

    def _ess_balance(self, probs_theta: torch.Tensor, probs_t: torch.Tensor) -> torch.Tensor:
        """
        äºŒåˆ†æ³•æ±‚è§£ESSå¹³è¡¡æ¡ä»¶ (ç²¾ç¡®æ¡ä»¶ï¼Œå¸¦æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤)

        ç›®æ ‡: ESS_Î¸(q) = ESS_t(q)
        ç­‰ä»·äº: Î£(Ï€_Î¸Â²/q) = Î£(Ï€_tÂ²/q)
        æˆ–: Î£(Ï€_Î¸Â²/q) - Î£(Ï€_tÂ²/q) = 0

        è¿™æ˜¯ç†è®ºä¸Šçš„ç²¾ç¡®æ¡ä»¶ï¼ŒKLå¯¹ç§°åªæ˜¯å®ƒçš„ä¸€é˜¶è¿‘ä¼¼ã€‚
        æ ¹æ® theory/proof_final.md:589-592ï¼Œä¸¤è€…å·®å¼‚é€šå¸¸ < 2%ã€‚

        åŒ…å«6å±‚æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤ï¼ˆè§ESS_BALANCE_STABILITY.mdï¼‰ã€‚

        æ³¨æ„ï¼š
        - Î±=0æ—¶ q=Ï€_Î¸ (åªç”¨Base)ï¼ŒÎ±=1æ—¶ q=Ï€_t (åªç”¨Teacher)
        - äºŒåˆ†æœç´¢æ‰¾åˆ°ä½¿ESSå¹³è¡¡çš„Î±å€¼
        """
        batch_size = probs_theta.shape[0]
        device = probs_theta.device

        # ========================================
        # ç¬¬1æ­¥ï¼šæ£€æŸ¥è¾¹ç•Œæ¡ä»¶
        # ========================================
        # è®¡ç®— Î±=0 (åªç”¨Base) å’Œ Î±=1 (åªç”¨Teacher) æ—¶çš„ ESS å’Œ sum_sq
        alpha_zero = torch.zeros(batch_size, device=device)
        alpha_one = torch.ones(batch_size, device=device)

        q_0 = self._geometric_mean(probs_theta, probs_t, alpha_zero)
        sum_theta_sq_0 = ((probs_theta ** 2) / (q_0 + self.eps)).sum(dim=-1)
        sum_t_sq_0 = ((probs_t ** 2) / (q_0 + self.eps)).sum(dim=-1)
        ess_theta_0 = 1.0 / (sum_theta_sq_0 + self.eps)
        ess_t_0 = 1.0 / (sum_t_sq_0 + self.eps)
        delta_0 = sum_t_sq_0 - sum_theta_sq_0

        q_1 = self._geometric_mean(probs_theta, probs_t, alpha_one)
        sum_theta_sq_1 = ((probs_theta ** 2) / (q_1 + self.eps)).sum(dim=-1)
        sum_t_sq_1 = ((probs_t ** 2) / (q_1 + self.eps)).sum(dim=-1)
        ess_theta_1 = 1.0 / (sum_theta_sq_1 + self.eps)
        ess_t_1 = 1.0 / (sum_t_sq_1 + self.eps)
        delta_1 = sum_t_sq_1 - sum_theta_sq_1

        # ========================================
        # ç¬¬2æ­¥ï¼šå¤šé‡æ£€æŸ¥ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å›é€€
        # ========================================
        need_fallback = (
            (delta_0 * delta_1 > 0) |           # é›¶ç‚¹ä¸å­˜åœ¨ï¼ˆåŒå·ï¼‰
            (ess_theta_0 < 0.01) | (ess_t_0 < 0.01) |  # ESSè¿‡å°
            (ess_theta_1 < 0.01) | (ess_t_1 < 0.01) |
            (sum_theta_sq_0 > 100) | (sum_t_sq_0 > 100) |  # sum_sqè¿‡å¤§
            (sum_theta_sq_1 > 100) | (sum_t_sq_1 > 100)
        )

        # ========================================
        # ç¬¬3æ­¥ï¼šè‡ªé€‚åº”æœç´¢èŒƒå›´
        # ========================================
        # å¦‚æœæ£€æµ‹åˆ°é—®é¢˜ï¼Œç¼©å°æœç´¢èŒƒå›´åˆ°[0.2, 0.8]
        alpha_low = torch.where(need_fallback,
                               torch.full((batch_size,), 0.2, device=device),
                               torch.zeros(batch_size, device=device))
        alpha_high = torch.where(need_fallback,
                                torch.full((batch_size,), 0.8, device=device),
                                torch.ones(batch_size, device=device))

        # ========================================
        # ç¬¬4æ­¥ï¼šäºŒåˆ†æœç´¢
        # ========================================
        for _ in range(self.max_iter):
            alpha_mid = (alpha_low + alpha_high) / 2

            # è®¡ç®— q_alpha
            q_alpha = self._geometric_mean(probs_theta, probs_t, alpha_mid)

            # è®¡ç®— ESS å·®å€¼
            sum_theta_sq = ((probs_theta ** 2) / (q_alpha + self.eps)).sum(dim=-1)
            sum_t_sq = ((probs_t ** 2) / (q_alpha + self.eps)).sum(dim=-1)
            delta = sum_t_sq - sum_theta_sq

            # æ›´æ–°åŒºé—´
            mask = delta < 0
            alpha_low = torch.where(mask, alpha_mid, alpha_low)
            alpha_high = torch.where(mask, alpha_high, alpha_mid)

            # æ£€æŸ¥æ”¶æ•›
            if (alpha_high - alpha_low).max() < self.tol:
                break

        alpha_result = (alpha_low + alpha_high) / 2

        # ========================================
        # ç¬¬5æ­¥ï¼šå¯¹æœ‰é—®é¢˜çš„æ ·æœ¬å›é€€åˆ°KLå¯¹ç§°
        # ========================================
        if need_fallback.any():
            alpha_fallback = self._kl_symmetry(probs_theta, probs_t)
            alpha_result = torch.where(need_fallback, alpha_fallback, alpha_result)

        # ========================================
        # ç¬¬6æ­¥ï¼šæœ€ç»ˆé™åˆ¶åˆ°[0.1, 0.9]
        # ========================================
        return torch.clamp(alpha_result, 0.1, 0.9)

    def _entropy(self, probs_theta: torch.Tensor, probs_t: torch.Tensor) -> torch.Tensor:
        """ç†µå…¬å¼å¿«é€Ÿè¿‘ä¼¼

        å¯å‘å¼ï¼š
        - h_theta é«˜ â†’ Baseæ¨¡å‹ä¸ç¡®å®š â†’ åº”è¯¥æ›´ä¾èµ–Teacher â†’ Î±åº”è¯¥é«˜
        - h_t é«˜ â†’ Teacheræ¨¡å‹ä¸ç¡®å®š â†’ åº”è¯¥æ›´ä¾èµ–Base â†’ Î±åº”è¯¥ä½

        å› æ­¤ï¼šÎ± = h_theta / (h_theta + h_t)

        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç»éªŒå…¬å¼ï¼Œä¸æ˜¯ç†è®ºæ¨å¯¼çš„ç²¾ç¡®è§£ã€‚
        """
        h_theta = -(probs_theta * torch.log(probs_theta + self.eps)).sum(dim=-1)
        h_t = -(probs_t * torch.log(probs_t + self.eps)).sum(dim=-1)
        alpha = h_theta / (h_theta + h_t + self.eps)
        return torch.clamp(alpha, 0.0, 1.0)

    def _geometric_mean(self, p1: torch.Tensor, p2: torch.Tensor,
                       alpha: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å‡ ä½•å¹³å‡

        âœ… é‡è¦æ”¹åŠ¨ï¼šÎ± ç°åœ¨è¡¨ç¤º p2 (é€šå¸¸æ˜¯ Ï€_t/Teacher) çš„æƒé‡
        - Î± = 0 â†’ å®Œå…¨ä½¿ç”¨ p1 (Ï€_Î¸/Base)
        - Î± = 1 â†’ å®Œå…¨ä½¿ç”¨ p2 (Ï€_t/Teacher)
        - Î± > 0.5 â†’ æ›´æ¥è¿‘ Teacherï¼ˆç¬¦åˆç›´è§‰ï¼‰

        å…¬å¼ï¼šq = p1^(1-Î±) Ã— p2^Î±
        """
        if alpha.dim() == 1:
            alpha = alpha.unsqueeze(-1)
        # âœ… ä¿®æ”¹ï¼šÎ± ç°åœ¨æ˜¯ p2 çš„æƒé‡
        log_q = (1 - alpha) * torch.log(p1 + self.eps) + alpha * torch.log(p2 + self.eps)
        return F.softmax(log_q, dim=-1)


class DiagnosticComputer:
    """è¯Šæ–­ä¿¡æ¯è®¡ç®—å™¨ï¼ˆå¢å¼ºæ•°å€¼ç¨³å®šæ€§ç‰ˆæœ¬ï¼‰"""

    def __init__(self):
        self.eps = 1e-10

    def compute(self, probs_theta: torch.Tensor, probs_t: torch.Tensor,
                q_star: torch.Tensor, alpha: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—è¯Šæ–­ä¿¡æ¯ï¼ˆå¸¦æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤ï¼‰

        Returns:
            dict with keys: ess_theta, ess_t, ess_ratio, kl_theta, kl_t, kl_diff
        """
        # âœ… æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤ï¼šClamp æ‰€æœ‰æ¦‚ç‡åˆ†å¸ƒ
        probs_theta = torch.clamp(probs_theta, min=self.eps, max=1.0)
        probs_t = torch.clamp(probs_t, min=self.eps, max=1.0)
        q_star = torch.clamp(q_star, min=self.eps, max=1.0)

        # âœ… æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(probs_theta).any() or torch.isnan(probs_t).any() or torch.isnan(q_star).any():
            # è¿”å›å…¨ nan çš„è¯Šæ–­ä¿¡æ¯
            batch_size = probs_theta.shape[0]
            nan_tensor = torch.full((batch_size,), float('nan'), device=probs_theta.device)
            return {
                "alpha": alpha,
                "ess_theta": nan_tensor,
                "ess_t": nan_tensor,
                "ess_ratio": nan_tensor,
                "kl_theta": nan_tensor,
                "kl_t": nan_tensor,
                "kl_diff": nan_tensor,
            }

        # ESS (Effective Sample Size)
        # ESS = 1 / Î£(pÂ²/q)
        sum_theta_sq = ((probs_theta ** 2) / q_star).sum(dim=-1)
        sum_t_sq = ((probs_t ** 2) / q_star).sum(dim=-1)

        # âœ… é¿å…é™¤ä»¥ 0 æˆ–è¿‡å¤§çš„å€¼
        sum_theta_sq = torch.clamp(sum_theta_sq, min=self.eps, max=1e6)
        sum_t_sq = torch.clamp(sum_t_sq, min=self.eps, max=1e6)

        ess_theta = 1.0 / sum_theta_sq
        ess_t = 1.0 / sum_t_sq
        ess_ratio = ess_theta / (ess_t + self.eps)

        # KLæ•£åº¦ï¼šD_KL(q||p) = Î£ q(x) log(q(x)/p(x))
        # âœ… ä½¿ç”¨ log-space è®¡ç®—ï¼Œé¿å… log(0)
        log_q = torch.log(q_star)
        log_probs_theta = torch.log(probs_theta)
        log_probs_t = torch.log(probs_t)

        kl_theta = (q_star * (log_q - log_probs_theta)).sum(dim=-1)
        kl_t = (q_star * (log_q - log_probs_t)).sum(dim=-1)

        # âœ… æœ€ç»ˆæ£€æŸ¥ï¼šæ›¿æ¢ä»»ä½•å‰©ä½™çš„ inf/nan
        ess_theta = torch.where(torch.isfinite(ess_theta), ess_theta, torch.zeros_like(ess_theta))
        ess_t = torch.where(torch.isfinite(ess_t), ess_t, torch.zeros_like(ess_t))
        ess_ratio = torch.where(torch.isfinite(ess_ratio), ess_ratio, torch.ones_like(ess_ratio))
        kl_theta = torch.where(torch.isfinite(kl_theta), kl_theta, torch.zeros_like(kl_theta))
        kl_t = torch.where(torch.isfinite(kl_t), kl_t, torch.zeros_like(kl_t))

        return {
            "alpha": alpha,
            "ess_theta": ess_theta,
            "ess_t": ess_t,
            "ess_ratio": ess_ratio,
            "kl_theta": kl_theta,
            "kl_t": kl_t,
            "kl_diff": (kl_theta - kl_t).abs(),
        }


class OptimalSamplingModel:
    """æœ€ä¼˜é‡‡æ ·æ¨¡å‹"""

    def __init__(
        self,
        model_theta_path: str,
        model_t_path: Optional[str] = None,
        alpha_method: str = "kl_symmetry",
        fixed_alpha: float = 0.5,
        alpha_tol: float = 1e-6,
        constraint_to_target: bool = False,
        target_top_k: int = -1,
        target_top_p: float = 1.0,
        force_target_for_special_tokens: bool = True,
        force_target_for_first_token: bool = True,
        device: str = "cuda",
        dtype: torch.dtype = torch.float16,
        **kwargs
    ):
        """
        Args:
            model_theta_path: Ï€_Î¸ æ¨¡å‹è·¯å¾„
            model_t_path: Ï€_t æ¨¡å‹è·¯å¾„ (å¦‚æœä¸ºNone, åˆ™ä½¿ç”¨model_theta_path)
            alpha_method: alphaè®¡ç®—æ–¹æ³• ["fixed", "kl_symmetry", "reverse_kl_symmetry", "entropy", "ess_balance"]
            fixed_alpha: å›ºå®šalphaå€¼ (å½“alpha_method="fixed"æ—¶)
            alpha_tol: KLå¯¹ç§°æ±‚è§£å®¹å·®
            constraint_to_target: âœ¨ æ˜¯å¦é™åˆ¶åœ¨Ï€_tçš„supportä¸Šï¼ˆæ¨èTrueï¼Œæå‡æ•°å€¼ç¨³å®šæ€§ï¼‰
            target_top_k: âœ¨ Ï€_tçš„top-ké™åˆ¶ï¼ˆ-1è¡¨ç¤ºä¸é™åˆ¶ï¼‰
            target_top_p: âœ¨ Ï€_tçš„top-pé™åˆ¶ï¼ˆ1.0è¡¨ç¤ºä¸é™åˆ¶ï¼‰
            force_target_for_special_tokens: âœ¨ å¯¹special tokenså¼ºåˆ¶ä½¿ç”¨Ï€_tï¼ˆä»tokenizerè·å–ï¼Œæ¨èTrueï¼‰
            force_target_for_first_token: âœ¨ å¼ºåˆ¶ç¬¬ä¸€ä¸ªtokenä½¿ç”¨Ï€_tï¼ˆæ¨èTrueï¼‰
            device: è®¾å¤‡
            dtype: æ•°æ®ç±»å‹
            **kwargs: ä¼ é€’ç»™transformersçš„é¢å¤–å‚æ•°ï¼ˆå¦‚load_in_4bit, attn_implementationç­‰ï¼‰
        """
        self.device = device
        self.dtype = dtype

        print(device, dtype)

        # å­˜å‚¨special tokenå¤„ç†å‚æ•°
        self.force_target_for_special_tokens = force_target_for_special_tokens
        self.force_target_for_first_token = force_target_for_first_token

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä»…æ”¯æŒtransformers backendï¼‰
        self._init_transformers(model_theta_path, model_t_path, **kwargs)

        # åˆå§‹åŒ–alphaè®¡ç®—å™¨ï¼ˆå¸¦supportçº¦æŸï¼‰
        self.alpha_computer = AlphaComputer(
            method=alpha_method,
            fixed_alpha=fixed_alpha,
            tol=alpha_tol,
            constraint_to_target=constraint_to_target,
            target_top_k=target_top_k,
            target_top_p=target_top_p
        )

        # åˆå§‹åŒ–è¯Šæ–­è®¡ç®—å™¨
        self.diagnostic_computer = DiagnosticComputer()

        print(f"âœ“ OptimalSamplingModel initialized")
        print(f"  Backend: transformers")
        print(f"  Alpha method: {alpha_method}")
        if constraint_to_target:
            print(f"  âœ¨ Support constraint: ENABLED")
            if target_top_k > 0:
                print(f"     - Target top-k: {target_top_k}")
            if target_top_p < 1.0:
                print(f"     - Target top-p: {target_top_p}")
        if force_target_for_special_tokens and not self.same_model:
            print(f"  âœ¨ Special token handling: ENABLED")
            print(f"     - For special tokens from tokenizer config, use Ï€_t directly")
        if force_target_for_first_token:
            print(f"  âœ¨ First token forcing: ENABLED")
            print(f"     - First token will use Ï€_t directly")
        print(f"  Ï€_Î¸: {model_theta_path}")
        print(f"  Ï€_t: {model_t_path or model_theta_path}")

    def _init_transformers(self, model_theta_path: str, model_t_path: Optional[str], **kwargs):
        """åˆå§‹åŒ–transformers backendï¼ˆæ”¯æŒä¸åŒtokenizerï¼‰"""
        from transformers import AutoModelForCausalLM, AutoTokenizer

        # âœ… ç§»é™¤kwargsä¸­çš„torch_dtypeï¼ˆé¿å…ä¸self.dtypeå†²çªï¼‰
        kwargs.pop('torch_dtype', None)

        # âœ… é»˜è®¤å¯ç”¨ Flash Attention 2ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼‰
        if "attn_implementation" not in kwargs:
            try:
                import flash_attn
                kwargs["attn_implementation"] = "flash_attention_2"
                print("âœ… Flash Attention 2 auto-enabled (detected flash_attn package)")
            except ImportError:
                print("â„¹ï¸  Flash Attention 2 not available, using default attention")
                print("   Install with: pip install flash-attn --no-build-isolation")

        # âœ¨ æ‰“å°ä¼ é€’çš„å‚æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if kwargs:
            print(f"\nğŸ“¦ Model loading parameters:")
            for key, value in kwargs.items():
                if key in ["attn_implementation", "load_in_4bit", "load_in_8bit", "device_map"]:
                    print(f"   - {key}: {value}")
                elif key.startswith("bnb_"):
                    print(f"   - {key}: {value}")
            print()

        # ========================================
        # åŠ è½½tokenizerï¼ˆä¸¤ä¸ªæ¨¡å‹å¯èƒ½æœ‰ä¸åŒçš„tokenizerï¼‰
        # ========================================
        print(f"Loading tokenizer for Ï€_Î¸ from {model_theta_path}...")
        self.tokenizer_theta = AutoTokenizer.from_pretrained(model_theta_path)
        if self.tokenizer_theta.pad_token is None:
            self.tokenizer_theta.pad_token = self.tokenizer_theta.eos_token

        # åŠ è½½ Ï€_Î¸
        print(f"Loading Ï€_Î¸ from {model_theta_path}...")
        self.model_theta = AutoModelForCausalLM.from_pretrained(
            model_theta_path,
            torch_dtype=self.dtype,
            device_map=self.device if self.device != "cuda" else "auto",
            **kwargs
        )
        self.model_theta.eval()

        # åŠ è½½ Ï€_t å’Œå®ƒçš„tokenizer
        if model_t_path is None or model_t_path == model_theta_path:
            print(f"Using Ï€_Î¸ as Ï€_t (same model)")
            self.model_t = self.model_theta
            self.tokenizer_t = self.tokenizer_theta
            self.same_model = True
            self.same_tokenizer = True
        else:
            print(f"Loading tokenizer for Ï€_t from {model_t_path}...")
            self.tokenizer_t = AutoTokenizer.from_pretrained(model_t_path)
            if self.tokenizer_t.pad_token is None:
                self.tokenizer_t.pad_token = self.tokenizer_t.eos_token

            print(f"Loading Ï€_t from {model_t_path}...")
            self.model_t = AutoModelForCausalLM.from_pretrained(
                model_t_path,
                torch_dtype=self.dtype,
                device_map=self.device if self.device != "cuda" else "auto",
                **kwargs
            )
            self.model_t.eval()
            self.same_model = False

            # æ£€æŸ¥tokenizeræ˜¯å¦ç›¸åŒ
            self.same_tokenizer = self._check_tokenizer_compatibility()

        # ä¿æŒå‘åå…¼å®¹ï¼šself.tokenizeræŒ‡å‘Î¸çš„tokenizer
        self.tokenizer = self.tokenizer_theta

        # å¦‚æœtokenizerä¸åŒï¼Œå»ºç«‹vocabularyæ˜ å°„
        if not self.same_tokenizer:
            print("\nâš ï¸ different tokenizer detectedï¼Œbuild vocabulary mapping...")
            self._build_vocab_mapping()

        # æ£€æµ‹special tokensï¼ˆÏ€_tæœ‰ä½†Ï€_Î¸å¯èƒ½æ²¡è§è¿‡çš„ï¼‰
        if self.force_target_for_special_tokens and not self.same_model:
            self._detect_special_tokens()
        else:
            self.special_token_mask = None

    def _check_tokenizer_compatibility(self) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªtokenizeræ˜¯å¦ç›¸åŒ/å…¼å®¹"""
        # æ–¹æ³•1ï¼šæ£€æŸ¥vocab size
        if len(self.tokenizer_theta) != len(self.tokenizer_t):
            print(f"  Tokenizer vocab size: Î¸={len(self.tokenizer_theta)}, t={len(self.tokenizer_t)}")
            return False

        # æ–¹æ³•2ï¼šæ£€æŸ¥ç‰¹æ®Štoken
        # if (self.tokenizer_theta.eos_token_id != self.tokenizer_t.eos_token_id or
        #     self.tokenizer_theta.bos_token_id != self.tokenizer_t.bos_token_id):
        #     print(f"  Tokenizer special token different: Î¸(eos={self.tokenizer_theta.eos_token_id}, bos={self.tokenizer_theta.bos_token_id}), ")
        #     return False

        # æ–¹æ³•3ï¼šé‡‡æ ·æ£€æŸ¥ä¸€äº›token
        sample_tokens = ["hello", "world", "the", "a", "is"]
        for token_str in sample_tokens:
            id_theta = self.tokenizer_theta.encode(token_str, add_special_tokens=False)
            id_t = self.tokenizer_t.encode(token_str, add_special_tokens=False)
            if id_theta != id_t:
                print(f"  Tokenizer encode diff: '{token_str}' -> Î¸={id_theta}, t={id_t}")
                return False

        print("  âœ“ Tokenizers are compatible")
        return True

    def _build_vocab_mapping(self):
        """
        å»ºç«‹ä¸¤ä¸ªvocabularyä¹‹é—´çš„æ˜ å°„

        æ ¸å¿ƒæ€æƒ³ï¼š
        1. å¯¹äºæ¯ä¸ªÎ¸çš„token IDï¼Œæ‰¾åˆ°tä¸­å¯¹åº”çš„token string
        2. å»ºç«‹ IDæ˜ å°„: vocab_map_theta_to_t[id_theta] = id_t
        3. åœ¨è®¡ç®—q*æ—¶ï¼Œéœ€è¦å¯¹é½ä¸¤ä¸ªæ¨¡å‹çš„logits
        """
        vocab_size_theta = len(self.tokenizer_theta)
        vocab_size_t = len(self.tokenizer_t)

        print(f"  Building vocab mapping: Î¸({vocab_size_theta}) -> t({vocab_size_t})")

        # æ˜ å°„: theta_id -> t_id
        self.vocab_map_theta_to_t = {}
        self.vocab_map_t_to_theta = {}

        # å¯¹äºÎ¸çš„æ¯ä¸ªtokenï¼Œæ‰¾åˆ°tä¸­çš„å¯¹åº”
        unmapped_count = 0
        for id_theta in range(vocab_size_theta):
            try:
                # Decode token
                token_str = self.tokenizer_theta.decode([id_theta], skip_special_tokens=False)

                # Encodeåˆ°tçš„vocabulary
                ids_t = self.tokenizer_t.encode(token_str, add_special_tokens=False)

                if len(ids_t) == 1:
                    # 1å¯¹1æ˜ å°„
                    self.vocab_map_theta_to_t[id_theta] = ids_t[0]
                elif len(ids_t) > 1:
                    # 1å¯¹å¤šæ˜ å°„ï¼ˆÎ¸çš„ä¸€ä¸ªtokenå¯¹åº”tçš„å¤šä¸ªtokenï¼‰
                    # ç®€åŒ–ï¼šå–ç¬¬ä¸€ä¸ª
                    self.vocab_map_theta_to_t[id_theta] = ids_t[0]
                else:
                    # æ— æ³•æ˜ å°„
                    unmapped_count += 1

            except Exception as e:
                unmapped_count += 1

        # åå‘æ˜ å°„
        for id_t in range(vocab_size_t):
            try:
                token_str = self.tokenizer_t.decode([id_t], skip_special_tokens=False)
                ids_theta = self.tokenizer_theta.encode(token_str, add_special_tokens=False)

                if len(ids_theta) == 1:
                    self.vocab_map_t_to_theta[id_t] = ids_theta[0]
                elif len(ids_theta) > 1:
                    self.vocab_map_t_to_theta[id_t] = ids_theta[0]

            except Exception:
                pass

        mapped_ratio = len(self.vocab_map_theta_to_t) / vocab_size_theta
        print(f"  âœ“ Mapped {len(self.vocab_map_theta_to_t)}/{vocab_size_theta} tokens ({mapped_ratio:.1%})")

        if unmapped_count > vocab_size_theta * 0.1:
            print(f"  âš ï¸  è­¦å‘Š: {unmapped_count} tokensæ— æ³•æ˜ å°„ ({unmapped_count/vocab_size_theta:.1%})")
            print(f"     è¿™å¯èƒ½å¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™")

        # å­˜å‚¨vocab sizeä»¥ä¾¿åç»­ä½¿ç”¨
        self.vocab_size_theta = vocab_size_theta
        self.vocab_size_t = vocab_size_t

    def _detect_special_tokens(self):
        """
        æ£€æµ‹special tokensï¼ˆä»tokenizer configä¸­è·å–ï¼‰

        ç­–ç•¥ï¼š
        1. è·å–Ï€_tçš„æ‰€æœ‰special tokensï¼ˆä»tokenizer.all_special_tokensï¼‰
        2. è·å–Ï€_Î¸çš„æ‰€æœ‰special tokens
        3. æ‰¾åˆ°æ‰€æœ‰special tokensï¼ˆä¿å®ˆç­–ç•¥ï¼šä½¿ç”¨å¹¶é›†ï¼‰
        4. åˆ›å»ºmaskæ ‡è®°è¿™äº›token ID

        è¿™äº›tokenåœ¨Base modelä¸­å¯èƒ½æ²¡è§è¿‡ï¼ˆå¦‚<|im_start|>ç­‰ï¼‰ï¼Œ
        åº”è¯¥ç›´æ¥ä½¿ç”¨Ï€_tçš„æ¦‚ç‡ã€‚
        """
        print("\n" + "="*60)
        print("Detecting special tokens...")
        print("="*60)

        # âœ… ä½¿ç”¨æ¨¡å‹çš„å®é™…vocab sizeï¼Œè€Œä¸æ˜¯tokenizerçš„
        vocab_size_theta_tokenizer = len(self.tokenizer_theta)
        vocab_size_t_tokenizer = len(self.tokenizer_t)

        # ä»æ¨¡å‹configè·å–å®é™…çš„vocab size
        if hasattr(self.model_theta.config, 'vocab_size'):
            vocab_size_theta = self.model_theta.config.vocab_size
        else:
            vocab_size_theta = vocab_size_theta_tokenizer

        if hasattr(self.model_t.config, 'vocab_size'):
            vocab_size_t = self.model_t.config.vocab_size
        else:
            vocab_size_t = vocab_size_t_tokenizer

        # ä½¿ç”¨è¾ƒå¤§çš„vocab sizeï¼ˆç¡®ä¿èƒ½å®¹çº³æ‰€æœ‰å¯èƒ½çš„tokenï¼‰
        vocab_size = max(vocab_size_theta, vocab_size_t)

        print(f"Tokenizer vocab size: Î¸={vocab_size_theta_tokenizer}, t={vocab_size_t_tokenizer}")
        print(f"Model vocab size: Î¸={vocab_size_theta}, t={vocab_size_t}")
        print(f"Using vocab size: {vocab_size}")

        # åˆå§‹åŒ–maskï¼ˆFalseè¡¨ç¤ºæ­£å¸¸tokenï¼ŒTrueè¡¨ç¤ºspecial tokenï¼‰
        self.special_token_mask = torch.zeros(vocab_size, dtype=torch.bool)

        # è·å–Ï€_tçš„special tokens
        special_tokens_t = set()
        if hasattr(self.tokenizer_t, 'all_special_tokens'):
            for token in self.tokenizer_t.all_special_tokens:
                try:
                    token_ids = self.tokenizer_t.encode(token, add_special_tokens=False)
                    special_tokens_t.update(token_ids)
                except:
                    pass

        # è·å–Ï€_Î¸çš„special tokens
        special_tokens_theta = set()
        if hasattr(self.tokenizer_theta, 'all_special_tokens'):
            for token in self.tokenizer_theta.all_special_tokens:
                try:
                    token_ids = self.tokenizer_theta.encode(token, add_special_tokens=False)
                    special_tokens_theta.update(token_ids)
                except:
                    pass

        # æ–¹æ¡ˆï¼šä¿å®ˆç­–ç•¥ - æ‰€æœ‰special tokenséƒ½å¼ºåˆ¶ä½¿ç”¨Ï€_t
        # åŸå› ï¼šå³ä½¿Î¸å’Œtéƒ½æœ‰EOSï¼ŒBase modelå¯¹EOSçš„å¤„ç†å¯èƒ½ä¸å¤Ÿå¥½
        all_special_tokens = special_tokens_theta | special_tokens_t

        print(f"Ï€_Î¸ special tokens: {len(special_tokens_theta)} unique IDs")
        print(f"Ï€_t special tokens: {len(special_tokens_t)} unique IDs")
        print(f"All special tokens: {len(all_special_tokens)} unique IDs")

        # æ ‡è®°è¿™äº›token
        for token_id in all_special_tokens:
            if token_id < vocab_size:
                self.special_token_mask[token_id] = True

        # æ‰“å°ä¸€äº›ç¤ºä¾‹
        print(f"\nSpecial token examples:")
        count = 0
        for token_id in sorted(list(all_special_tokens))[:10]:
            if token_id < vocab_size:
                try:
                    token_str_theta = self.tokenizer_theta.decode([token_id])
                    token_str_t = self.tokenizer_t.decode([token_id])
                    in_theta = token_id in special_tokens_theta
                    in_t = token_id in special_tokens_t
                    print(f"  ID {token_id}: Î¸='{token_str_theta}' ({'âœ“' if in_theta else 'âœ—'}), "
                          f"t='{token_str_t}' ({'âœ“' if in_t else 'âœ—'})")
                    count += 1
                except:
                    pass

        if len(all_special_tokens) > 10:
            print(f"  ... and {len(all_special_tokens) - count} more")

        print(f"\nâœ“ Special token mask created: {self.special_token_mask.sum().item()} tokens marked")
        print("="*60 + "\n")

    @torch.no_grad()
    def generate(
        self,
        prompts: List[str],
        prompts_t: Optional[List[str]] = None,  # âœ… æ–°å¢ï¼šÏ€_tçš„promptï¼ˆå¯é€‰ï¼‰
        max_new_tokens: int = 100,
        temperature: float = 1.0,
        top_p: float = 1.0,
        top_k: int = -1,
        return_diagnostics: bool = True,
        skip_decode: bool = False,  # âœ¨ æ–°å¢ï¼šè·³è¿‡decodeï¼Œè¿”å›ç©ºçš„generated_texts
        return_logits: bool = False,  # âœ¨ æ–°å¢ï¼šè¿”å›æ¯æ­¥çš„logits
        return_q_star_probs: bool = False,  # âœ¨ æ–°å¢ï¼šè¿”å›q*æ¦‚ç‡åˆ†å¸ƒ
        **kwargs
    ) -> SamplingOutput:
        """
        ä½¿ç”¨q*é‡‡æ ·ç”Ÿæˆæ–‡æœ¬

        Args:
            prompts: Ï€_Î¸çš„è¾“å…¥promptsåˆ—è¡¨
            prompts_t: Ï€_tçš„è¾“å…¥promptsåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
                      å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨promptsï¼ˆé»˜è®¤è¡Œä¸ºï¼Œå‘åå…¼å®¹ï¼‰
                      å¦‚æœæä¾›ï¼Œåˆ™ä¸¤ä¸ªæ¨¡å‹çœ‹åˆ°ä¸åŒçš„è¾“å…¥
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleus samplingå‚æ•°
            top_k: top-k samplingå‚æ•°
            return_diagnostics: æ˜¯å¦è¿”å›è¯Šæ–­ä¿¡æ¯
            skip_decode: æ˜¯å¦è·³è¿‡å†…éƒ¨decodeï¼ˆé»˜è®¤Falseï¼Œå¦‚æœTrueåˆ™generated_textsä¸ºç©ºåˆ—è¡¨ï¼‰
            return_logits: æ˜¯å¦è¿”å›logitsï¼ˆé»˜è®¤Falseï¼‰
            return_q_star_probs: æ˜¯å¦è¿”å›q*æ¦‚ç‡åˆ†å¸ƒï¼ˆé»˜è®¤Falseï¼‰

        Returns:
            SamplingOutputå¯¹è±¡

        Examples:
            # åœºæ™¯1: ä¸¤ä¸ªæ¨¡å‹çœ‹ç›¸åŒçš„promptï¼ˆé»˜è®¤ï¼‰
            >>> outputs = model.generate(prompts=["Hello"])

            # åœºæ™¯2: ä¸¤ä¸ªæ¨¡å‹çœ‹ä¸åŒçš„prompt
            >>> outputs = model.generate(
            ...     prompts=["Answer briefly: What is AI?"],  # Ï€_Î¸çœ‹ç®€æ´ç‰ˆ
            ...     prompts_t=["Answer in detail: What is AI?"]  # Ï€_tçœ‹è¯¦ç»†ç‰ˆ
            ... )

            # åœºæ™¯3: è¿”å›logitså¹¶è·³è¿‡decodeï¼ˆå¤–éƒ¨decodeï¼‰
            >>> outputs = model.generate(
            ...     prompts=["Hello"],
            ...     skip_decode=True,
            ...     return_logits=True,
            ...     return_q_star_probs=True
            ... )
            >>> # outputs.generated_texts ä¸ºç©ºåˆ—è¡¨
            >>> # outputs.logits åŒ…å« {"theta": [...], "t": [...]}
            >>> # åœ¨å¤–éƒ¨decode
            >>> decoded = model.tokenizer.batch_decode(outputs.generated_ids, skip_special_tokens=True)
        """
        return self._generate_transformers(
            prompts, prompts_t, max_new_tokens, temperature, top_p, top_k,
            return_diagnostics, skip_decode, return_logits, return_q_star_probs, **kwargs
        )

    def _generate_transformers(
        self,
        prompts: List[str],
        prompts_t: Optional[List[str]],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        return_diagnostics: bool,
        skip_decode: bool,  # âœ¨ æ–°å¢å‚æ•°
        return_logits: bool,  # âœ¨ æ–°å¢å‚æ•°
        return_q_star_probs: bool,  # âœ¨ æ–°å¢å‚æ•°
        use_kv_cache: bool = True,  # âœ… æ–°å¢å‚æ•°
        stopping_criteria: Optional[any] = None,  # âœ… æ”¯æŒè‡ªå®šä¹‰åœæ­¢æ¡ä»¶
        **kwargs
    ) -> SamplingOutput:
        """
        Transformers backendçš„ç”Ÿæˆï¼ˆæ”¯æŒåŒpromptï¼‰

        Args:
            prompts: Ï€_Î¸çš„prompts
            prompts_t: Ï€_tçš„promptsï¼ˆå¯é€‰ï¼‰
            skip_decode: æ˜¯å¦è·³è¿‡decode
            return_logits: æ˜¯å¦è¿”å›logits
            return_q_star_probs: æ˜¯å¦è¿”å›q*æ¦‚ç‡
            use_kv_cache: æ˜¯å¦ä½¿ç”¨KV cacheåŠ é€Ÿï¼ˆé»˜è®¤Trueï¼‰
            stopping_criteria: transformers.StoppingCriteriaListå¯¹è±¡
        """
        batch_size = len(prompts)

        # ========================================
        # ç¬¬1æ­¥ï¼šåˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä¸åŒçš„prompt
        # ========================================
        use_different_prompts = (prompts_t is not None) and (prompts_t != prompts)

        if use_different_prompts:
            if len(prompts_t) != batch_size:
                raise ValueError(f"prompts_té•¿åº¦({len(prompts_t)})å¿…é¡»ä¸promptsé•¿åº¦({batch_size})ç›¸åŒ")

        # ========================================
        # ç¬¬2æ­¥ï¼šTokenizeï¼ˆå¯èƒ½ä¸åŒï¼‰
        # ========================================
        # Tokenize Ï€_Î¸çš„prompt
        inputs_theta = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048  # âœ… é˜²æ­¢è¶…é•¿è¾“å…¥
        )
        input_ids_theta = inputs_theta["input_ids"].to(self.model_theta.device)
        attention_mask_theta = inputs_theta["attention_mask"].to(self.model_theta.device)

        # Tokenize Ï€_tçš„promptï¼ˆå¦‚æœä¸åŒï¼‰
        if use_different_prompts:
            # æ£€æŸ¥tokenizeræ˜¯å¦ç›¸åŒ
            if self.same_model:
                # åŒä¸€ä¸ªæ¨¡å‹ï¼Œtokenizerè‚¯å®šç›¸åŒ
                inputs_t = self.tokenizer(
                    prompts_t,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048
                )
            else:
                inputs_t = self.tokenizer_t(
                    prompts_t,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048
                )

            input_ids_t = inputs_t["input_ids"].to(self.model_t.device)
            attention_mask_t = inputs_t["attention_mask"].to(self.model_t.device)
        else:
            # ä½¿ç”¨ç›¸åŒçš„promptï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
            input_ids_t = input_ids_theta
            attention_mask_t = attention_mask_theta

        # ========================================
        # å‡†å¤‡å­˜å‚¨
        # ========================================
        all_generated_ids = []
        all_alpha_values = []
        all_ess_ratios = []
        all_diagnostics = []

        # âœ¨ æ–°å¢ï¼šå­˜å‚¨ logits å’Œ q_star_probs
        all_logits_theta = [] if return_logits else None
        all_logits_t = [] if return_logits else None
        all_q_star_probs = [] if return_q_star_probs else None

        # âœ… åœæ­¢æ¡ä»¶ï¼šä¸ºæ¯ä¸ªæ ·æœ¬ç»´æŠ¤finishedçŠ¶æ€
        finished = torch.zeros(batch_size, dtype=torch.bool, device=self.model_theta.device)
        eos_token_id = self.tokenizer_theta.eos_token_id  # ä½¿ç”¨Î¸çš„EOSï¼ˆç”Ÿæˆtokenåœ¨Î¸çš„vocabï¼‰

        if use_kv_cache:
            # ========================================
            # âœ… ä½¿ç”¨KV cacheçš„é«˜æ•ˆå®ç°ï¼ˆæ”¯æŒåŒpromptï¼‰
            # ========================================

            # Prefillé˜¶æ®µï¼šå¤„ç†å®Œæ•´promptï¼Œåˆå§‹åŒ–KV cache
            # Ï€_Î¸ ä½¿ç”¨ input_ids_theta
            outputs_theta = self.model_theta(
                input_ids=input_ids_theta,
                attention_mask=attention_mask_theta,
                use_cache=True  # âœ… å¯ç”¨KV cache
            )
            past_key_values_theta = outputs_theta.past_key_values
            logits_theta = outputs_theta.logits[:, -1, :]

            # Ï€_t ä½¿ç”¨ input_ids_tï¼ˆå¯èƒ½ä¸åŒï¼‰
            if not self.same_model:
                outputs_t = self.model_t(
                    input_ids=input_ids_t,
                    attention_mask=attention_mask_t,
                    use_cache=True
                )
                past_key_values_t = outputs_t.past_key_values
                logits_t = outputs_t.logits[:, -1, :]
            else:
                past_key_values_t = None
                logits_t = logits_theta

            # Decodeé˜¶æ®µï¼šé€tokenç”Ÿæˆï¼Œå¤ç”¨KV cache
            for step in tqdm(range(max_new_tokens), desc=kwargs['tqdm_desc']):
                # âœ… å¯¹é½logitså¹¶è®¡ç®—æ¦‚ç‡ï¼ˆå¤„ç†ä¸åŒtokenizerï¼‰
                probs_theta, probs_t = self._align_logits(logits_theta, logits_t, temperature)

                # âœ… å¼ºåˆ¶ç¬¬ä¸€ä¸ªtokenä½¿ç”¨Ï€_t
                if step == 0 and self.force_target_for_first_token:
                    # ç¬¬ä¸€ä¸ªtokenç›´æ¥ä½¿ç”¨Ï€_tï¼Œä¸è¿›è¡Œæ··åˆ
                    q_star = probs_t
                    # âœ… ä¿®æ”¹ï¼šÎ±=1 è¡¨ç¤ºå®Œå…¨ä½¿ç”¨Ï€_tï¼ˆTeacherï¼‰
                    alpha = torch.ones(batch_size, device=probs_theta.device)
                else:
                    # åç»­tokenæ­£å¸¸è®¡ç®—alphaå’Œq*
                    # âœ¨ æ”¹è¿›ï¼šå…ˆè®¡ç®—alphaï¼Œå†è®¡ç®—q*ï¼Œæœ€åæ–½åŠ çº¦æŸ
                    alpha = self.alpha_computer.compute(probs_theta, probs_t)
                    q_star = self._compute_q_star(probs_theta, probs_t, alpha)

                    # âœ¨ æ–°å¢ï¼šå¯¹q*æ–½åŠ supportçº¦æŸï¼ˆé™åˆ¶åœ¨Ï€_tçš„supportä¸Šï¼‰
                    # è¿™æ ·å¯ä»¥é¿å…çº¦æŸå½±å“alphaè®¡ç®—
                    q_star = self.alpha_computer.apply_constraint_to_q_star(q_star, probs_t)

                # åº”ç”¨ top-p / top-k
                if top_p < 1.0 or top_k > 0:
                    q_star = self._apply_sampling_filters(q_star, top_p, top_k)

                # âœ… é‡‡æ ·å‰å®‰å…¨æ£€æŸ¥
                if torch.isnan(q_star).any() or torch.isinf(q_star).any() or (q_star < 0).any():
                    print(f"âš ï¸  Warning: Invalid q_star at step {step}, using \\pi_t fallback")
                    # q_star = torch.ones_like(q_star) / q_star.size(-1)
                    q_star = probs_t

                # ä» q* é‡‡æ ·
                try:
                    next_tokens = torch.multinomial(q_star, num_samples=1).squeeze(-1)
                except RuntimeError as e:
                    print(f"âš ï¸  Sampling failed at step {step}: {e}")
                    print(f"   q_star stats: min={q_star.min():.6f}, max={q_star.max():.6f}, sum={q_star.sum(dim=-1)}")
                    # å›é€€åˆ°argmax
                    next_tokens = q_star.argmax(dim=-1)

                # âœ… æ£€æŸ¥æœ¬æ­¥æ˜¯å¦æœ‰æ ·æœ¬ç”ŸæˆEOS
                if eos_token_id is not None:
                    finished = finished | (next_tokens == eos_token_id)

                # âœ… å¯¹å·²å®Œæˆçš„æ ·æœ¬ï¼Œä½¿ç”¨pad tokenï¼ˆé˜²æ­¢ç»§ç»­ç”Ÿæˆï¼‰
                if self.tokenizer_theta.pad_token_id is not None:
                    next_tokens = torch.where(finished,
                                             torch.tensor(self.tokenizer_theta.pad_token_id, device=next_tokens.device),
                                             next_tokens)

                # è®¡ç®—è¯Šæ–­ä¿¡æ¯
                if return_diagnostics:
                    diag = self.diagnostic_computer.compute(probs_theta, probs_t, q_star, alpha)
                    all_alpha_values.append(alpha.cpu())
                    all_ess_ratios.append(diag["ess_ratio"].cpu())
                    all_diagnostics.append({k: v.cpu() for k, v in diag.items()})

                # âœ¨ æ–°å¢ï¼šæ”¶é›† logits å’Œ q_star_probs
                if return_logits:
                    all_logits_theta.append(logits_theta.cpu())
                    all_logits_t.append(logits_t.cpu())
                if return_q_star_probs:
                    all_q_star_probs.append(q_star.cpu())

                # ä¿å­˜ç”Ÿæˆçš„token
                all_generated_ids.append(next_tokens.unsqueeze(-1))

                # âœ… æ£€æŸ¥åœæ­¢æ¡ä»¶
                if stopping_criteria is not None:
                    generated_so_far = torch.cat(all_generated_ids, dim=-1)
                    current_input_ids = torch.cat([input_ids_theta, generated_so_far], dim=-1)
                    if stopping_criteria(current_input_ids, None):
                        break

                # âœ… æ‰€æœ‰æ ·æœ¬éƒ½å®Œæˆæ—¶åœæ­¢
                if finished.all():
                    break

                # æ›´æ–°attention_maskï¼ˆå„è‡ªç‹¬ç«‹ï¼‰
                attention_mask_theta = torch.cat([
                    attention_mask_theta,
                    torch.ones((batch_size, 1), device=attention_mask_theta.device)
                ], dim=-1)

                if use_different_prompts:
                    attention_mask_t = torch.cat([
                        attention_mask_t,
                        torch.ones((batch_size, 1), device=attention_mask_t.device)
                    ], dim=-1)
                else:
                    attention_mask_t = attention_mask_theta

                # âœ… Forwardæ–°tokenï¼ˆä½¿ç”¨past_key_valueså’Œå„è‡ªçš„attention_maskï¼‰
                outputs_theta = self.model_theta(
                    input_ids=next_tokens.unsqueeze(-1),  # âœ… åªä¼ å…¥æ–°tokenï¼ˆç›¸åŒï¼‰
                    attention_mask=attention_mask_theta,   # âœ… ä½¿ç”¨å„è‡ªçš„mask
                    past_key_values=past_key_values_theta,  # âœ… ä½¿ç”¨cache
                    use_cache=True
                )
                past_key_values_theta = outputs_theta.past_key_values
                logits_theta = outputs_theta.logits[:, -1, :]

                if not self.same_model:
                    outputs_t = self.model_t(
                        input_ids=next_tokens.unsqueeze(-1),
                        attention_mask=attention_mask_t,     # âœ… ä½¿ç”¨å„è‡ªçš„mask
                        past_key_values=past_key_values_t,
                        use_cache=True
                    )
                    past_key_values_t = outputs_t.past_key_values
                    logits_t = outputs_t.logits[:, -1, :]
                else:
                    logits_t = logits_theta

        else:
            # ========================================
            # åŸå§‹å®ç°ï¼ˆä¸ä½¿ç”¨KV cacheï¼Œç”¨äºå¯¹æ¯”/è°ƒè¯•ï¼Œæ”¯æŒåŒpromptï¼‰
            # ========================================
            current_ids_theta = input_ids_theta
            current_attention_mask_theta = attention_mask_theta

            current_ids_t = input_ids_t
            current_attention_mask_t = attention_mask_t

            for step in range(max_new_tokens):
                # è·å– Ï€_Î¸ çš„logits
                outputs_theta = self.model_theta(
                    input_ids=current_ids_theta,
                    attention_mask=current_attention_mask_theta,
                    use_cache=False
                )
                logits_theta = outputs_theta.logits[:, -1, :]

                # è·å– Ï€_t çš„logits
                if self.same_model:
                    logits_t = logits_theta
                else:
                    outputs_t = self.model_t(
                        input_ids=current_ids_t,
                        attention_mask=current_attention_mask_t,
                        use_cache=False
                    )
                    logits_t = outputs_t.logits[:, -1, :]

                # âœ… å¯¹é½logitså¹¶è®¡ç®—æ¦‚ç‡ï¼ˆå¤„ç†ä¸åŒtokenizerï¼‰
                probs_theta, probs_t = self._align_logits(logits_theta, logits_t, temperature)

                # âœ… å¼ºåˆ¶ç¬¬ä¸€ä¸ªtokenä½¿ç”¨Ï€_t
                if step == 0 and self.force_target_for_first_token:
                    # ç¬¬ä¸€ä¸ªtokenç›´æ¥ä½¿ç”¨Ï€_tï¼Œä¸è¿›è¡Œæ··åˆ
                    q_star = probs_t
                    # âœ… ä¿®æ”¹ï¼šÎ±=1 è¡¨ç¤ºå®Œå…¨ä½¿ç”¨Ï€_tï¼ˆTeacherï¼‰
                    alpha = torch.ones(batch_size, device=probs_theta.device)
                else:
                    # åç»­tokenæ­£å¸¸è®¡ç®—alphaå’Œq*
                    # âœ¨ æ”¹è¿›ï¼šå…ˆè®¡ç®—alphaï¼Œå†è®¡ç®—q*ï¼Œæœ€åæ–½åŠ çº¦æŸ
                    alpha = self.alpha_computer.compute(probs_theta, probs_t)
                    q_star = self._compute_q_star(probs_theta, probs_t, alpha)

                    # âœ¨ æ–°å¢ï¼šå¯¹q*æ–½åŠ supportçº¦æŸï¼ˆé™åˆ¶åœ¨Ï€_tçš„supportä¸Šï¼‰
                    # è¿™æ ·å¯ä»¥é¿å…çº¦æŸå½±å“alphaè®¡ç®—
                    q_star = self.alpha_computer.apply_constraint_to_q_star(q_star, probs_t)

                # åº”ç”¨ top-p / top-k
                if top_p < 1.0 or top_k > 0:
                    q_star = self._apply_sampling_filters(q_star, top_p, top_k)

                # âœ… é‡‡æ ·å‰å®‰å…¨æ£€æŸ¥
                if torch.isnan(q_star).any() or torch.isinf(q_star).any() or (q_star < 0).any():
                    print(f"âš ï¸  Warning: Invalid q_star at step {step}, using q_t fallback")
                    # q_star = torch.ones_like(q_star) / q_star.size(-1)
                    q_star = probs_t

                # ä» q* é‡‡æ ·
                try:
                    next_tokens = torch.multinomial(q_star, num_samples=1).squeeze(-1)
                except RuntimeError as e:
                    print(f"âš ï¸  Sampling failed at step {step}: {e}")
                    print(f"   q_star stats: min={q_star.min():.6f}, max={q_star.max():.6f}, sum={q_star.sum(dim=-1)}")
                    # å›é€€åˆ°argmax
                    next_tokens = q_star.argmax(dim=-1)

                # âœ… æ£€æŸ¥æœ¬æ­¥æ˜¯å¦æœ‰æ ·æœ¬ç”ŸæˆEOS
                if eos_token_id is not None:
                    finished = finished | (next_tokens == eos_token_id)

                # âœ… å¯¹å·²å®Œæˆçš„æ ·æœ¬ï¼Œä½¿ç”¨pad tokenï¼ˆé˜²æ­¢ç»§ç»­ç”Ÿæˆï¼‰
                if self.tokenizer_theta.pad_token_id is not None:
                    next_tokens = torch.where(finished,
                                             torch.tensor(self.tokenizer_theta.pad_token_id, device=next_tokens.device),
                                             next_tokens)

                # è®¡ç®—è¯Šæ–­ä¿¡æ¯
                if return_diagnostics:
                    diag = self.diagnostic_computer.compute(probs_theta, probs_t, q_star, alpha)
                    all_alpha_values.append(alpha.cpu())
                    all_ess_ratios.append(diag["ess_ratio"].cpu())
                    all_diagnostics.append({k: v.cpu() for k, v in diag.items()})

                # âœ¨ æ–°å¢ï¼šæ”¶é›† logits å’Œ q_star_probs
                if return_logits:
                    all_logits_theta.append(logits_theta.cpu())
                    all_logits_t.append(logits_t.cpu())
                if return_q_star_probs:
                    all_q_star_probs.append(q_star.cpu())

                # æ›´æ–°åºåˆ—ï¼ˆå„è‡ªç‹¬ç«‹ï¼‰
                all_generated_ids.append(next_tokens.unsqueeze(-1))
                current_ids_theta = torch.cat([current_ids_theta, next_tokens.unsqueeze(-1)], dim=-1)
                current_attention_mask_theta = torch.cat([
                    current_attention_mask_theta,
                    torch.ones((batch_size, 1), device=current_attention_mask_theta.device)
                ], dim=-1)

                current_ids_t = torch.cat([current_ids_t, next_tokens.unsqueeze(-1)], dim=-1)
                current_attention_mask_t = torch.cat([
                    current_attention_mask_t,
                    torch.ones((batch_size, 1), device=current_attention_mask_t.device)
                ], dim=-1)

                # âœ… æ£€æŸ¥åœæ­¢æ¡ä»¶
                if stopping_criteria is not None:
                    if stopping_criteria(current_ids_theta, None):
                        break

                # âœ… æ‰€æœ‰æ ·æœ¬éƒ½å®Œæˆæ—¶åœæ­¢
                if finished.all():
                    break

        # ç»„è£…ç»“æœ
        generated_ids = torch.cat(all_generated_ids, dim=-1)

        # âœ¨ æ–°å¢ï¼šæ ¹æ® skip_decode å†³å®šæ˜¯å¦ decode
        if skip_decode:
            generated_texts = []  # è¿”å›ç©ºåˆ—è¡¨
        else:
            generated_texts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        # ç»„è£…è¯Šæ–­ä¿¡æ¯
        diagnostics = {}
        if return_diagnostics and all_diagnostics:
            alpha_values = torch.stack(all_alpha_values, dim=1)  # [batch, seq_len]
            ess_ratios = torch.stack(all_ess_ratios, dim=1)

            # èšåˆæ‰€æœ‰stepçš„è¯Šæ–­ä¿¡æ¯
            for key in all_diagnostics[0].keys():
                values = torch.stack([d[key] for d in all_diagnostics], dim=1)
                diagnostics[key] = values
        else:
            alpha_values = torch.zeros((batch_size, 0))
            ess_ratios = torch.zeros((batch_size, 0))

        # âœ¨ æ–°å¢ï¼šç»„è£… logits å’Œ q_star_probs
        logits = None
        if return_logits and all_logits_theta:
            logits = {
                "theta": torch.stack(all_logits_theta, dim=1),  # [batch, seq_len, vocab_size]
                "t": torch.stack(all_logits_t, dim=1)
            }

        q_star_probs = None
        if return_q_star_probs and all_q_star_probs:
            q_star_probs = torch.stack(all_q_star_probs, dim=1)  # [batch, seq_len, vocab_size]

        return SamplingOutput(
            generated_texts=generated_texts,
            generated_ids=generated_ids,
            alpha_values=alpha_values,
            ess_ratios=ess_ratios,
            diagnostics=diagnostics,
            logits=logits,
            q_star_probs=q_star_probs
        )

    def _align_logits(
        self,
        logits_theta: torch.Tensor,
        logits_t: torch.Tensor,
        temperature: float = 1.0
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        å¯¹é½ä¸¤ä¸ªæ¨¡å‹çš„logitsï¼ˆå½“tokenizerä¸åŒæ—¶ï¼‰

        ç­–ç•¥ï¼š
        1. å¦‚æœtokenizerç›¸åŒï¼šç›´æ¥ä½¿ç”¨
        2. å¦‚æœtokenizerä¸åŒï¼š
           - å°†Î¸çš„logitsæ˜ å°„åˆ°ç»Ÿä¸€ç©ºé—´ï¼ˆä½¿ç”¨Î¸çš„vocabularyï¼‰
           - å°†tçš„logitsä¹Ÿæ˜ å°„åˆ°Î¸çš„vocabulary
           - è¿”å›å¯¹é½åçš„æ¦‚ç‡åˆ†å¸ƒ

        Args:
            logits_theta: [batch, vocab_size_theta]
            logits_t: [batch, vocab_size_t]
            temperature: æ¸©åº¦

        Returns:
            (probs_theta, probs_t_aligned): ä¸¤ä¸ªåœ¨ç›¸åŒvocabularyä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ
        """
        if self.same_tokenizer:
            # Tokenizerç›¸åŒï¼Œç›´æ¥è®¡ç®—æ¦‚ç‡
            probs_theta = F.softmax(logits_theta / temperature, dim=-1)
            probs_t = F.softmax(logits_t / temperature, dim=-1)
            return probs_theta, probs_t

        # Tokenizerä¸åŒï¼Œéœ€è¦å¯¹é½
        batch_size = logits_theta.shape[0]

        # Î¸çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆä¿æŒä¸å˜ï¼‰
        probs_theta = F.softmax(logits_theta / temperature, dim=-1)

        # å°†tçš„æ¦‚ç‡æ˜ å°„åˆ°Î¸çš„vocabulary
        probs_t_aligned = torch.zeros_like(probs_theta)

        # å¯¹äºÎ¸çš„æ¯ä¸ªtokenï¼Œæ‰¾åˆ°tä¸­çš„å¯¹åº”tokenå¹¶å¤åˆ¶æ¦‚ç‡
        for id_theta, id_t in self.vocab_map_theta_to_t.items():
            probs_t_aligned[:, id_theta] = F.softmax(logits_t / temperature, dim=-1)[:, id_t]

        # é‡æ–°å½’ä¸€åŒ–ï¼ˆå› ä¸ºå¯èƒ½æœ‰unmapped tokensï¼‰
        probs_t_aligned = probs_t_aligned / (probs_t_aligned.sum(dim=-1, keepdim=True) + 1e-10)

        return probs_theta, probs_t_aligned

    def _map_token_id(self, token_id: int, from_model: str = "theta") -> int:
        """
        å°†token IDä»ä¸€ä¸ªvocabularyæ˜ å°„åˆ°å¦ä¸€ä¸ª

        Args:
            token_id: æºtoken ID
            from_model: "theta" æˆ– "t"

        Returns:
            ç›®æ ‡token ID
        """
        if self.same_tokenizer:
            return token_id

        if from_model == "theta":
            # Î¸ -> t
            return self.vocab_map_theta_to_t.get(token_id, self.tokenizer_t.unk_token_id)
        else:
            # t -> Î¸
            return self.vocab_map_t_to_theta.get(token_id, self.tokenizer_theta.unk_token_id)

    def _compute_q_star(self, probs_theta: torch.Tensor, probs_t: torch.Tensor,
                        alpha: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—q*åˆ†å¸ƒï¼ˆæ”¯æŒå¯¹special tokensç›´æ¥ä½¿ç”¨Ï€_tï¼‰

        âœ… é‡è¦æ”¹åŠ¨ï¼šÎ± ç°åœ¨è¡¨ç¤º Ï€_t (Teacher) çš„æƒé‡ï¼
        - Î± = 0 â†’ å®Œå…¨ä½¿ç”¨ Ï€_Î¸ (Base)
        - Î± = 1 â†’ å®Œå…¨ä½¿ç”¨ Ï€_t (Teacher)
        - Î± > 0.5 â†’ æ›´æ¥è¿‘ Teacherï¼ˆç¬¦åˆç›´è§‰ï¼‰

        æ ¸å¿ƒæ”¹è¿›ï¼š
        å¯¹äºä»tokenizer configæ£€æµ‹åˆ°çš„special tokensï¼ˆå¦‚EOSã€chat templateç­‰ï¼‰ï¼Œ
        è¿™äº›tokenåœ¨Base modelä¸­å¯èƒ½æ²¡è§è¿‡æˆ–å¤„ç†ä¸å¥½ã€‚
        å¯¹è¿™äº›tokenï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨Ï€_tçš„æ¦‚ç‡ï¼Œä¸è¿›è¡Œå‡ ä½•å¹³å‡ã€‚

        æ•°å­¦ä¸Šç­‰ä»·äºï¼šå¯¹special tokenè®¾ç½®Î±=1ï¼ˆå®Œå…¨ä½¿ç”¨Ï€_tï¼‰

        æ³¨æ„ï¼šprobs_thetaå’Œprobs_tåº”è¯¥å·²ç»é€šè¿‡_align_logitså¯¹é½åˆ°ç›¸åŒvocabulary
        """
        eps = 1e-10

        # ========================================
        # âœ¨ å¢å¼º1: è¾“å…¥éªŒè¯å’ŒNaNæ—©æœŸæ£€æµ‹
        # ========================================
        if torch.isnan(probs_theta).any():
            nan_count = torch.isnan(probs_theta).sum().item()
            print(f"âš ï¸  Warning: probs_theta contains {nan_count} NaN values before q* computation")
            probs_theta = torch.where(torch.isnan(probs_theta),
                                       torch.tensor(eps, device=probs_theta.device),
                                       probs_theta)

        if torch.isnan(probs_t).any():
            nan_count = torch.isnan(probs_t).sum().item()
            print(f"âš ï¸  Warning: probs_t contains {nan_count} NaN values before q* computation")
            probs_t = torch.where(torch.isnan(probs_t),
                                   torch.tensor(eps, device=probs_t.device),
                                   probs_t)

        if torch.isnan(alpha).any():
            nan_positions = torch.where(torch.isnan(alpha))
            print(f"âš ï¸  Warning: alpha contains NaN at positions {nan_positions}")
            print(f"   Falling back to Î±=0.5 for NaN positions")
            alpha = torch.where(torch.isnan(alpha),
                                torch.tensor(0.5, device=alpha.device),
                                alpha)

        # ========================================
        # âœ¨ å¢å¼º2: ç¡®ä¿è¾“å…¥åœ¨æœ‰æ•ˆèŒƒå›´å†…
        # ========================================
        probs_theta = torch.clamp(probs_theta, min=eps, max=1.0)
        probs_t = torch.clamp(probs_t, min=eps, max=1.0)
        alpha = torch.clamp(alpha, min=0.0, max=1.0)

        if alpha.dim() == 1:
            alpha = alpha.unsqueeze(-1)  # [batch, 1]

        # è®¡ç®—logæ¦‚ç‡ï¼ˆæ·»åŠ epsé¿å…log(0)ï¼Œå³ä½¿clampä¹‹åä¹Ÿä¿é™©ï¼‰
        log_probs_theta = torch.log(probs_theta + eps)
        log_probs_t = torch.log(probs_t + eps)

        # ========================================
        # âœ¨ å¢å¼º3: éªŒè¯logæ¦‚ç‡çš„æœ‰æ•ˆæ€§ï¼ˆåªæ£€æŸ¥NaNï¼Œ-infæ˜¯æ­£å¸¸çš„ï¼‰
        # ========================================
        # æ³¨æ„ï¼šlog(æå°æ¦‚ç‡)ä¼šäº§ç”Ÿ-infï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼Œsoftmaxä¼šæ­£ç¡®å¤„ç†
        # åªæœ‰NaNæ‰æ˜¯çœŸæ­£çš„é—®é¢˜
        if torch.isnan(log_probs_theta).any():
            nan_count = torch.isnan(log_probs_theta).sum().item()
            print(f"âŒ Error: log_probs_theta has {nan_count} NaN after log operation")
            # å¼ºåˆ¶ä¿®å¤ï¼šå°†NaNæ›¿æ¢ä¸ºä¸€ä¸ªå®‰å…¨çš„logå€¼
            log_probs_theta = torch.where(torch.isnan(log_probs_theta),
                                           torch.tensor(-23.0, device=log_probs_theta.device),
                                           log_probs_theta)

        if torch.isnan(log_probs_t).any():
            nan_count = torch.isnan(log_probs_t).sum().item()
            print(f"âŒ Error: log_probs_t has {nan_count} NaN after log operation")
            log_probs_t = torch.where(torch.isnan(log_probs_t),
                                       torch.tensor(-23.0, device=log_probs_t.device),
                                       log_probs_t)

        if self.force_target_for_special_tokens and self.special_token_mask is not None:
            # ä½¿ç”¨special token mask
            # special_token_mask: [vocab_size] bool tensor
            # Trueè¡¨ç¤ºspecial tokenï¼Œåº”è¯¥ä½¿ç”¨Ï€_t

            # å°†maskç§»åˆ°æ­£ç¡®çš„deviceå¹¶æ‰©å±•åˆ°batch
            device = probs_theta.device
            special_mask = self.special_token_mask.to(device)  # [vocab_size]
            special_mask = special_mask.unsqueeze(0)  # [1, vocab_size]

            # è®¡ç®—ä¸¤ç§æƒ…å†µçš„log q
            # âœ… ä¿®æ”¹ï¼šÎ± ç°åœ¨æ˜¯ Ï€_t çš„æƒé‡
            # æ­£å¸¸token: log q = (1-Î±) log Ï€_Î¸ + Î± log Ï€_t
            # Special token: log q = log Ï€_t ï¼ˆÎ±=1ï¼‰

            log_q_normal = (1 - alpha) * log_probs_theta + alpha * log_probs_t
            log_q_special = log_probs_t  # ç›´æ¥ä½¿ç”¨Ï€_t

            # ä½¿ç”¨maské€‰æ‹©
            log_q = torch.where(special_mask, log_q_special, log_q_normal)
        else:
            # åŸå§‹æ–¹æ³•ï¼šç»Ÿä¸€çš„alpha
            # âœ… ä¿®æ”¹ï¼šÎ± ç°åœ¨æ˜¯ Ï€_t çš„æƒé‡
            log_q = (1 - alpha) * log_probs_theta + alpha * log_probs_t

        # ========================================
        # âœ¨ å¢å¼º4: æ£€æŸ¥log_qçš„æœ‰æ•ˆæ€§ï¼ˆåªæ£€æŸ¥NaNï¼‰
        # ========================================
        # æ³¨æ„ï¼šlog_qå¯èƒ½åŒ…å«-infï¼ˆè¡¨ç¤ºæå°æ¦‚ç‡ï¼‰ï¼Œè¿™æ˜¯æ­£å¸¸çš„
        # softmaxä¼šå°†-infæ­£ç¡®å¤„ç†ä¸º0æ¦‚ç‡
        if torch.isnan(log_q).any():
            nan_count = torch.isnan(log_q).sum().item()
            print(f"âŒ Error: log_q has {nan_count} NaN after geometric mean")
            print(f"   Alpha value: {alpha.squeeze() if alpha.dim() > 1 else alpha}")
            print(f"   Falling back to probs_t (Teacher model)")
            return probs_t.clone()

        # å½’ä¸€åŒ–
        q_star = F.softmax(log_q, dim=-1)

        # ========================================
        # âœ¨ å¢å¼º5: æœ€ç»ˆéªŒè¯q*çš„æœ‰æ•ˆæ€§
        # ========================================
        if torch.isnan(q_star).any():
            nan_count = torch.isnan(q_star).sum().item()
            print(f"âŒ CRITICAL: q_star has {nan_count} NaN after softmax!")
            print(f"   This should not happen if log_q was finite")
            print(f"   Falling back to q_t distribution")
            # q_star = torch.ones_like(q_star) / q_star.size(-1)
            q_star = probs_t.clone()

        return q_star

    def _apply_sampling_filters(self, probs: torch.Tensor, top_p: float, top_k: int) -> torch.Tensor:
        """åº”ç”¨top-på’Œtop-kè¿‡æ»¤"""
        # Top-k
        if top_k > 0:
            top_k_probs, top_k_indices = torch.topk(probs, k=min(top_k, probs.size(-1)), dim=-1)
            probs = torch.zeros_like(probs)
            probs.scatter_(-1, top_k_indices, top_k_probs)

        # Top-p (nucleus sampling)
        if top_p < 1.0:
            sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)

            # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token
            sorted_indices_to_remove = cumsum_probs > top_p
            # ä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡é˜ˆå€¼çš„token
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = False

            # åˆ›å»ºmask
            indices_to_remove = sorted_indices_to_remove.scatter(
                -1, sorted_indices, sorted_indices_to_remove
            )
            probs = probs.masked_fill(indices_to_remove, 0.0)

        # é‡æ–°å½’ä¸€åŒ–
        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-10)
        return probs

    # ============================================
    # Speculative Decoding æ ¸å¿ƒæ–¹æ³•
    # ============================================

    @torch.no_grad()
    def _draft_k_tokens_simple(
        self,
        current_ids_theta: torch.Tensor,
        current_ids_t: torch.Tensor,
        attention_mask_theta: torch.Tensor,
        attention_mask_t: torch.Tensor,
        k: int,
        temperature: float = 1.0,
        use_different_prompts: bool = False
    ) -> tuple:
        """
        ä½¿ç”¨Ï€_Î¸ç®€å•åœ°ç”Ÿæˆkä¸ªdraft tokensï¼ˆä¸ä½¿ç”¨past_kvï¼Œä¿è¯æ­£ç¡®æ€§ï¼‰

        Returns:
            draft_tokens: [batch, k]
            all_probs_theta: [batch, k, vocab]
            all_probs_t: [batch, k, vocab]
        """
        batch_size = current_ids_theta.shape[0]
        device = current_ids_theta.device

        draft_tokens = []
        all_probs_theta = []
        all_probs_t = []

        for i in range(k):
            # Forward Ï€_Î¸
            outputs_theta = self.model_theta(
                input_ids=current_ids_theta,
                attention_mask=attention_mask_theta,
                use_cache=False  # ä¸ä½¿ç”¨cacheï¼Œç®€å•ç›´æ¥
            )
            logits_theta = outputs_theta.logits[:, -1, :]

            # Forward Ï€_t
            if not self.same_model:
                outputs_t = self.model_t(
                    input_ids=current_ids_t,
                    attention_mask=attention_mask_t,
                    use_cache=False
                )
                logits_t = outputs_t.logits[:, -1, :]
            else:
                logits_t = logits_theta

            # è®¡ç®—æ¦‚ç‡
            probs_theta = F.softmax(logits_theta / temperature, dim=-1)
            probs_t = F.softmax(logits_t / temperature, dim=-1)

            # ä»Ï€_Î¸é‡‡æ ·draft token
            next_token = torch.multinomial(probs_theta, num_samples=1)  # [batch, 1]

            draft_tokens.append(next_token)
            all_probs_theta.append(probs_theta)
            all_probs_t.append(probs_t)

            # æ›´æ–°åºåˆ—
            current_ids_theta = torch.cat([current_ids_theta, next_token], dim=-1)
            if use_different_prompts:
                current_ids_t = torch.cat([current_ids_t, next_token], dim=-1)
            else:
                current_ids_t = current_ids_theta

            attention_mask_theta = torch.cat([
                attention_mask_theta,
                torch.ones((batch_size, 1), device=device)
            ], dim=-1)
            if use_different_prompts:
                attention_mask_t = torch.cat([
                    attention_mask_t,
                    torch.ones((batch_size, 1), device=device)
                ], dim=-1)
            else:
                attention_mask_t = attention_mask_theta

        draft_tokens = torch.cat(draft_tokens, dim=-1)  # [batch, k]
        all_probs_theta = torch.stack(all_probs_theta, dim=1)  # [batch, k, vocab]
        all_probs_t = torch.stack(all_probs_t, dim=1)  # [batch, k, vocab]

        return draft_tokens, all_probs_theta, all_probs_t

    def _verify_and_accept_batch(
        self,
        input_ids_theta: torch.Tensor,
        input_ids_t: torch.Tensor,
        draft_tokens: torch.Tensor,
        draft_probs: torch.Tensor,
        attention_mask_theta: torch.Tensor,
        attention_mask_t: torch.Tensor,
        past_key_values_theta,
        past_key_values_t,
        temperature: float,
        use_different_prompts: bool
    ) -> tuple[list, list, list, any, any, torch.Tensor, torch.Tensor]:
        """
        æ‰¹é‡éªŒè¯draft tokenså¹¶å†³å®šæ¥å—/æ‹’ç»

        æ ¸å¿ƒé€»è¾‘ï¼š
        1. æ‰¹é‡forward: ä¸€æ¬¡æ€§å¤„ç†kä¸ªdraft tokens
        2. å¯¹æ¯ä¸ªä½ç½®i:
           - è®¡ç®—çœŸå®çš„q*(i)
           - è®¡ç®—æ¥å—æ¦‚ç‡: min(1, q*[draft[i]] / q_draft[draft[i]])
           - æ¥å—æˆ–æ‹’ç»ï¼ˆä»ä¿®æ­£åˆ†å¸ƒé‡é‡‡æ ·ï¼‰
        3. é‡åˆ°ç¬¬ä¸€ä¸ªæ‹’ç»å°±åœæ­¢

        Returns:
            accepted_tokens: æ¥å—çš„tokenåˆ—è¡¨
            alpha_values: å¯¹åº”çš„alphaå€¼åˆ—è¡¨
            diagnostics: è¯Šæ–­ä¿¡æ¯åˆ—è¡¨
            updated_past_kv_theta: æ›´æ–°åçš„Ï€_Î¸ KV cache
            updated_past_kv_t: æ›´æ–°åçš„Ï€_t KV cache
            final_attention_mask_theta: æœ€ç»ˆçš„attention mask (Î¸)
            final_attention_mask_t: æœ€ç»ˆçš„attention mask (t)
        """
        batch_size, k = draft_tokens.shape
        device = draft_tokens.device

        # ========================================
        # Step 1: æ‰¹é‡forwardï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
        # ========================================
        # å°†kä¸ªdraft tokenæ‹¼æ¥åˆ°è¾“å…¥åé¢
        extended_ids_theta = torch.cat([input_ids_theta, draft_tokens], dim=-1)
        extended_mask_theta = torch.cat([
            attention_mask_theta,
            torch.ones((batch_size, k), device=device)
        ], dim=-1)

        # Forward Ï€_Î¸ï¼ˆæ‰¹é‡å¤„ç†kä¸ªtokenï¼‰
        outputs_theta = self.model_theta(
            input_ids=extended_ids_theta,
            attention_mask=extended_mask_theta,
            use_cache=True
        )
        logits_theta_all = outputs_theta.logits  # [batch, seq_len + k, vocab_size]

        # Forward Ï€_tï¼ˆæ‰¹é‡å¤„ç†kä¸ªtokenï¼‰
        if not self.same_model:
            extended_ids_t = torch.cat([input_ids_t, draft_tokens], dim=-1)
            extended_mask_t = torch.cat([
                attention_mask_t,
                torch.ones((batch_size, k), device=device)
            ], dim=-1)

            outputs_t = self.model_t(
                input_ids=extended_ids_t,
                attention_mask=extended_mask_t,
                use_cache=True
            )
            logits_t_all = outputs_t.logits
        else:
            logits_t_all = logits_theta_all
            extended_mask_t = extended_mask_theta

        # ========================================
        # Step 2: é€ä¸ªéªŒè¯æ¯ä¸ªdraft token
        # ========================================
        accepted_tokens = []
        alpha_values = []
        diagnostics_list = []

        # è·Ÿè¸ªå½“å‰ä½ç½®çš„past_kvå’Œattention_mask
        current_past_kv_theta = past_key_values_theta
        current_past_kv_t = past_key_values_t
        current_mask_theta = attention_mask_theta
        current_mask_t = attention_mask_t

        for i in range(k):
            # è·å–ç¬¬iä¸ªä½ç½®çš„logitsï¼ˆå¯¹åº”draft_token[i]ä¹‹å‰çš„çŠ¶æ€ï¼‰
            # å¦‚æœextended_ids = [A, B, C, D, E], draft_tokens = [D, E]
            # logits[seq_len-1] é¢„æµ‹ D (draft[0])
            # logits[seq_len] é¢„æµ‹ E (draft[1])
            # ç”¨è´Ÿç´¢å¼•ï¼šdraft[i] çš„logitsæ˜¯ logits[-(k-i+1)]
            logits_theta_i = logits_theta_all[:, -(k - i + 1), :]
            logits_t_i = logits_t_all[:, -(k - i + 1), :]

            # è®¡ç®—æ¦‚ç‡
            probs_theta_i, probs_t_i = self._align_logits(logits_theta_i, logits_t_i, temperature)

            # è®¡ç®—Î±å’Œq*
            alpha = self.alpha_computer.compute(probs_theta_i, probs_t_i)
            q_star = self._compute_q_star(probs_theta_i, probs_t_i, alpha)
            q_star = self.alpha_computer.apply_constraint_to_q_star(q_star, probs_t_i)

            # è·å–draft tokenå’Œå¯¹åº”çš„draftæ¦‚ç‡
            draft_token_i = draft_tokens[:, i]  # [batch]
            q_draft_i = draft_probs[:, i, :]    # [batch, vocab_size]

            # ========================================
            # æ ¸å¿ƒï¼šSpeculative SamplingéªŒè¯
            # ========================================
            # æ¥å—æ¦‚ç‡: accept_prob = min(1, q*(draft) / q_draft(draft))
            q_star_at_draft = q_star.gather(-1, draft_token_i.unsqueeze(-1)).squeeze(-1)  # [batch]
            q_draft_at_draft = q_draft_i.gather(-1, draft_token_i.unsqueeze(-1)).squeeze(-1)  # [batch]

            accept_prob = torch.clamp(
                q_star_at_draft / (q_draft_at_draft + 1e-10),
                max=1.0
            )

            # é‡‡æ ·å†³å®šæ˜¯å¦æ¥å—
            accept = torch.rand_like(accept_prob) < accept_prob

            if accept.all():
                # æ‰€æœ‰æ ·æœ¬éƒ½æ¥å—è¿™ä¸ªdraft token
                accepted_tokens.append(draft_token_i)
                alpha_values.append(alpha)

                # è®¡ç®—è¯Šæ–­ä¿¡æ¯
                diag = self.diagnostic_computer.compute(probs_theta_i, probs_t_i, q_star, alpha)
                diagnostics_list.append(diag)

                # æ›´æ–°past_kvå’Œattention_maskï¼ˆè¿™é‡Œéœ€è¦é€æ­¥æ›´æ–°ï¼‰
                # ç”±äºæˆ‘ä»¬å·²ç»æœ‰äº†å®Œæ•´çš„forwardç»“æœï¼Œå¯ä»¥æˆªå–å¯¹åº”çš„past_kv
                # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨æœ€ç»ˆçš„past_kvï¼ˆå› ä¸ºæˆ‘ä»¬æ‰¹é‡forwardäº†ï¼‰
                current_mask_theta = torch.cat([
                    current_mask_theta,
                    torch.ones((batch_size, 1), device=device)
                ], dim=-1)
                if use_different_prompts:
                    current_mask_t = torch.cat([
                        current_mask_t,
                        torch.ones((batch_size, 1), device=device)
                    ], dim=-1)
                else:
                    current_mask_t = current_mask_theta

            else:
                # è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬æ‹’ç» â†’ éœ€è¦é‡é‡‡æ ·
                # ä¿®æ­£åˆ†å¸ƒï¼šq'(x) = max(0, q*(x) - q_draft(x)) / Z
                corrected_probs = torch.clamp(q_star - q_draft_i, min=0.0)
                corrected_probs = corrected_probs / (corrected_probs.sum(dim=-1, keepdim=True) + 1e-10)

                # é‡é‡‡æ ·
                resampled_token = torch.multinomial(corrected_probs, num_samples=1).squeeze(-1)

                # å¯¹æ¥å—çš„æ ·æœ¬ç”¨draftï¼Œå¯¹æ‹’ç»çš„æ ·æœ¬ç”¨resampled
                final_token = torch.where(accept, draft_token_i, resampled_token)

                accepted_tokens.append(final_token)
                alpha_values.append(alpha)

                diag = self.diagnostic_computer.compute(probs_theta_i, probs_t_i, q_star, alpha)
                diagnostics_list.append(diag)

                # æ›´æ–°mask
                current_mask_theta = torch.cat([
                    current_mask_theta,
                    torch.ones((batch_size, 1), device=device)
                ], dim=-1)
                if use_different_prompts:
                    current_mask_t = torch.cat([
                        current_mask_t,
                        torch.ones((batch_size, 1), device=device)
                    ], dim=-1)
                else:
                    current_mask_t = current_mask_theta

                # âŒ æ‹’ç»ï¼šåœæ­¢éªŒè¯åç»­draft tokens
                break

        # ========================================
        # Step 3: æ›´æ–°past_kvï¼ˆincrementally forward accepted tokensï¼‰
        # ========================================
        # ä»past_key_valueså¼€å§‹ï¼Œé€ä¸ªforward accepted tokens
        if len(accepted_tokens) > 0:
            current_past_kv_theta = past_key_values_theta
            current_past_kv_t = past_key_values_t

            for token in accepted_tokens:
                # Forward Ï€_Î¸ one token at a time
                outputs_theta = self.model_theta(
                    input_ids=token.unsqueeze(-1),
                    attention_mask=None,  # attention_maskå·²ç»åœ¨current_mask_thetaä¸­ç´¯ç§¯äº†
                    past_key_values=current_past_kv_theta,
                    use_cache=True
                )
                current_past_kv_theta = outputs_theta.past_key_values

                if not self.same_model:
                    outputs_t = self.model_t(
                        input_ids=token.unsqueeze(-1),
                        attention_mask=None,
                        past_key_values=current_past_kv_t,
                        use_cache=True
                    )
                    current_past_kv_t = outputs_t.past_key_values
                else:
                    current_past_kv_t = current_past_kv_theta

            final_past_kv_theta = current_past_kv_theta
            final_past_kv_t = current_past_kv_t
        else:
            final_past_kv_theta = past_key_values_theta
            final_past_kv_t = past_key_values_t

        return (
            accepted_tokens,
            alpha_values,
            diagnostics_list,
            final_past_kv_theta,
            final_past_kv_t,
            current_mask_theta,
            current_mask_t
        )

    @torch.no_grad()
    def generate_speculative(
        self,
        prompts: List[str],
        prompts_t: Optional[List[str]] = None,
        max_new_tokens: int = 100,
        k: int = 4,  # æ¯æ¬¡é¢„æµ‹kä¸ªdraft tokens
        temperature: float = 1.0,
        top_p: float = 1.0,
        top_k: int = -1,
        return_diagnostics: bool = True,
        skip_decode: bool = False,
        return_logits: bool = False,
        return_q_star_probs: bool = False,
        stopping_criteria: Optional[any] = None,
        **kwargs
    ) -> SamplingOutput:
        """
        ä½¿ç”¨Speculative DecodingåŠ é€Ÿçš„ç”Ÿæˆæ–¹æ³•

        æ ¸å¿ƒæ€æƒ³ï¼š
        1. ç”¨Ï€_Î¸å¿«é€Ÿç”Ÿæˆkä¸ªdraft tokensï¼ˆautoregressiveï¼‰
        2. ç”¨Ï€_Î¸å’ŒÏ€_tæ‰¹é‡éªŒè¯è¿™kä¸ªtokensï¼ˆå¹¶è¡Œforwardï¼‰
        3. æ¥å—/æ‹’ç»æ¯ä¸ªtokenï¼Œä¸¥æ ¼ä¿è¯æœ€ç»ˆåˆ†å¸ƒ = q*

        Args:
            prompts: Ï€_Î¸çš„è¾“å…¥prompts
            prompts_t: Ï€_tçš„è¾“å…¥promptsï¼ˆå¯é€‰ï¼Œæ”¯æŒåŒpromptï¼‰
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            k: æ¯æ¬¡é¢„æµ‹çš„draft tokenæ•°é‡ï¼ˆæ¨è4-8ï¼‰
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleus sampling
            top_k: top-k sampling
            return_diagnostics: æ˜¯å¦è¿”å›è¯Šæ–­ä¿¡æ¯
            skip_decode: æ˜¯å¦è·³è¿‡decode
            return_logits: æ˜¯å¦è¿”å›logits
            return_q_star_probs: æ˜¯å¦è¿”å›q*æ¦‚ç‡
            stopping_criteria: åœæ­¢æ¡ä»¶

        Returns:
            SamplingOutputå¯¹è±¡ï¼ˆä¸generateæ–¹æ³•ç›¸åŒï¼‰

        Note:
            - æ•°å­¦ä¸Šä¸¥æ ¼ä¿è¯ï¼šé‡‡æ ·åˆ†å¸ƒ = q*ï¼ˆé€šè¿‡speculative samplingéªŒè¯ï¼‰
            - é¢„æœŸåŠ é€Ÿï¼š1.5-3xï¼ˆå–å†³äºæ¥å—ç‡å’Œkå€¼ï¼‰
            - æ¥å—ç‡é€šå¸¸åœ¨40-70%ï¼ˆÏ€_Î¸å’ŒÏ€_tè¶Šæ¥è¿‘ï¼Œæ¥å—ç‡è¶Šé«˜ï¼‰
        """
        batch_size = len(prompts)

        # ========================================
        # å‡†å¤‡è¾“å…¥
        # ========================================
        use_different_prompts = (prompts_t is not None) and (prompts_t != prompts)

        if use_different_prompts:
            if len(prompts_t) != batch_size:
                raise ValueError(f"prompts_té•¿åº¦({len(prompts_t)})å¿…é¡»ä¸promptsé•¿åº¦({batch_size})ç›¸åŒ")

        # Tokenize
        inputs_theta = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        )
        input_ids_theta = inputs_theta["input_ids"].to(self.model_theta.device)
        attention_mask_theta = inputs_theta["attention_mask"].to(self.model_theta.device)

        if use_different_prompts:
            if self.same_model:
                inputs_t = self.tokenizer(prompts_t, return_tensors="pt", padding=True, truncation=True, max_length=2048)
            else:
                inputs_t = self.tokenizer_t(prompts_t, return_tensors="pt", padding=True, truncation=True, max_length=2048)
            input_ids_t = inputs_t["input_ids"].to(self.model_t.device)
            attention_mask_t = inputs_t["attention_mask"].to(self.model_t.device)
        else:
            input_ids_t = input_ids_theta
            attention_mask_t = attention_mask_theta

        # ========================================
        # å‡†å¤‡å­˜å‚¨
        # ========================================
        all_generated_ids = []
        all_alpha_values = []
        all_ess_ratios = []
        all_diagnostics = []

        all_logits_theta = [] if return_logits else None
        all_logits_t = [] if return_logits else None
        all_q_star_probs = [] if return_q_star_probs else None

        finished = torch.zeros(batch_size, dtype=torch.bool, device=self.model_theta.device)
        eos_token_id = self.tokenizer_theta.eos_token_id

        # ========================================
        # Prefillé˜¶æ®µï¼šåˆå§‹åŒ–KV cache
        # ========================================
        outputs_theta = self.model_theta(
            input_ids=input_ids_theta,
            attention_mask=attention_mask_theta,
            use_cache=True
        )
        past_key_values_theta = outputs_theta.past_key_values

        if not self.same_model:
            outputs_t = self.model_t(
                input_ids=input_ids_t,
                attention_mask=attention_mask_t,
                use_cache=True
            )
            past_key_values_t = outputs_t.past_key_values
        else:
            past_key_values_t = None

        # ========================================
        # Decodeé˜¶æ®µï¼šSpeculative Decodingå¾ªç¯
        # ========================================
        total_generated = 0
        total_draft = 0
        total_accepted = 0

        while total_generated < max_new_tokens:
            # è®¡ç®—è¿™ä¸€è½®è¦ç”Ÿæˆå¤šå°‘ä¸ªdraft tokens
            remaining = max_new_tokens - total_generated
            k_this_round = min(k, remaining)

            # ========================================
            # Step 1: Drafté˜¶æ®µ - ç”¨Ï€_Î¸å¿«é€Ÿç”Ÿæˆkä¸ªå€™é€‰
            # ========================================
            draft_tokens, draft_probs, _ = self._draft_tokens_with_theta(
                input_ids=torch.cat([input_ids_theta] + all_generated_ids, dim=-1) if all_generated_ids else input_ids_theta,
                attention_mask=attention_mask_theta,
                past_key_values_theta=past_key_values_theta,
                k=k_this_round,
                temperature=temperature
            )

            total_draft += k_this_round

            # ========================================
            # Step 2: Verifyé˜¶æ®µ - æ‰¹é‡éªŒè¯å¹¶æ¥å—/æ‹’ç»
            # ========================================
            (
                accepted_tokens,
                alpha_values,
                diagnostics_list,
                past_key_values_theta,
                past_key_values_t,
                attention_mask_theta,
                attention_mask_t
            ) = self._verify_and_accept_batch(
                input_ids_theta=torch.cat([input_ids_theta] + all_generated_ids, dim=-1) if all_generated_ids else input_ids_theta,
                input_ids_t=torch.cat([input_ids_t] + all_generated_ids, dim=-1) if all_generated_ids else input_ids_t,
                draft_tokens=draft_tokens,
                draft_probs=draft_probs,
                attention_mask_theta=attention_mask_theta,
                attention_mask_t=attention_mask_t,
                past_key_values_theta=past_key_values_theta,
                past_key_values_t=past_key_values_t,
                temperature=temperature,
                use_different_prompts=use_different_prompts
            )

            num_accepted = len(accepted_tokens)
            total_accepted += num_accepted

            # ========================================
            # Step 3: ä¿å­˜ç»“æœ
            # ========================================
            for i in range(num_accepted):
                token = accepted_tokens[i]

                # æ£€æŸ¥EOS
                if eos_token_id is not None:
                    finished = finished | (token == eos_token_id)

                # å¯¹å·²å®Œæˆçš„æ ·æœ¬ä½¿ç”¨pad token
                if self.tokenizer_theta.pad_token_id is not None:
                    token = torch.where(finished,
                                       torch.tensor(self.tokenizer_theta.pad_token_id, device=token.device),
                                       token)

                all_generated_ids.append(token.unsqueeze(-1))

                if return_diagnostics:
                    all_alpha_values.append(alpha_values[i].cpu())
                    all_ess_ratios.append(diagnostics_list[i]["ess_ratio"].cpu())
                    all_diagnostics.append({k: v.cpu() for k, v in diagnostics_list[i].items()})

            total_generated += num_accepted

            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if stopping_criteria is not None:
                generated_so_far = torch.cat(all_generated_ids, dim=-1)
                current_input_ids = torch.cat([input_ids_theta, generated_so_far], dim=-1)
                if stopping_criteria(current_input_ids, None):
                    break

            if finished.all():
                break

        # ========================================
        # ç»„è£…ç»“æœ
        # ========================================
        generated_ids = torch.cat(all_generated_ids, dim=-1) if all_generated_ids else torch.empty((batch_size, 0), dtype=torch.long)

        if skip_decode:
            generated_texts = []
        else:
            generated_texts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        diagnostics = {}
        if return_diagnostics and all_diagnostics:
            alpha_values = torch.stack(all_alpha_values, dim=1)
            ess_ratios = torch.stack(all_ess_ratios, dim=1)

            for key in all_diagnostics[0].keys():
                values = torch.stack([d[key] for d in all_diagnostics], dim=1)
                diagnostics[key] = values

            # æ·»åŠ Speculative Decodingç»Ÿè®¡
            diagnostics['total_draft_tokens'] = total_draft
            diagnostics['total_accepted_tokens'] = total_accepted
            diagnostics['acceptance_rate'] = total_accepted / total_draft if total_draft > 0 else 0.0
            diagnostics['average_accepted_per_round'] = total_accepted / (total_generated // k + 1) if total_generated > 0 else 0.0
        else:
            alpha_values = torch.zeros((batch_size, 0))
            ess_ratios = torch.zeros((batch_size, 0))

        logits = None
        q_star_probs = None

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if return_diagnostics:
            accept_rate = diagnostics['acceptance_rate']
            print(f"\nğŸ“Š Speculative Decoding Stats:")
            print(f"   - Draft tokens: {total_draft}")
            print(f"   - Accepted tokens: {total_accepted}")
            print(f"   - Acceptance rate: {accept_rate:.2%}")
            print(f"   - Estimated speedup: {1 + (k-1) * accept_rate:.2f}x")

        return SamplingOutput(
            generated_texts=generated_texts,
            generated_ids=generated_ids,
            alpha_values=alpha_values,
            ess_ratios=ess_ratios,
            diagnostics=diagnostics,
            logits=logits,
            q_star_probs=q_star_probs
        )


# ============================================
# ä¾¿æ·å‡½æ•°
# ============================================

def create_optimal_sampling_model(
    model_theta: str,
    model_t: Optional[str] = None,
    alpha_method: str = "kl_symmetry",
    constraint_to_target: bool = True,
    target_top_k: int = 50,
    target_top_p: float = 1.0,
    force_target_for_special_tokens: bool = True,
    force_target_for_first_token: bool = True,
    **kwargs
) -> OptimalSamplingModel:
    """
    ä¾¿æ·çš„æ¨¡å‹åˆ›å»ºå‡½æ•°ï¼ˆä»…æ”¯æŒtransformers backendï¼‰

    Args:
        model_theta: Ï€_Î¸æ¨¡å‹è·¯å¾„ (Base model, å¦‚Llama-2-7b)
        model_t: Ï€_tæ¨¡å‹è·¯å¾„ï¼ˆTeacher/Instruct model, å¦‚Llama-2-7b-chatï¼‰
        alpha_method: Alphaè®¡ç®—æ–¹æ³•ï¼ˆÎ±è¡¨ç¤ºTeacheræƒé‡ï¼‰
        constraint_to_target: âœ¨ æ˜¯å¦é™åˆ¶åœ¨Ï€_tçš„supportä¸Šï¼ˆæ¨èTrueï¼‰
        target_top_k: âœ¨ Ï€_tçš„top-ké™åˆ¶
        target_top_p: âœ¨ Ï€_tçš„top-pé™åˆ¶
        force_target_for_special_tokens: âœ¨ å¯¹special tokensç›´æ¥ä½¿ç”¨Ï€_tï¼ˆæ¨èTrueï¼‰
        force_target_for_first_token: âœ¨ å¼ºåˆ¶ç¬¬ä¸€ä¸ªtokenä½¿ç”¨Ï€_tï¼ˆæ¨èTrueï¼‰
        **kwargs: ä¼ é€’ç»™transformersçš„é¢å¤–å‚æ•°ï¼ˆå¦‚load_in_4bit, attn_implementationç­‰ï¼‰

    Note:
        Î±çš„å«ä¹‰: Î±è¡¨ç¤º**Teacheræ¨¡å‹ (Ï€_t)** çš„æƒé‡
        - Î± = 0 â†’ å®Œå…¨ä½¿ç”¨Baseæ¨¡å‹
        - Î± = 1 â†’ å®Œå…¨ä½¿ç”¨Teacheræ¨¡å‹
        - Î± > 0.5 â†’ æ›´æ¥è¿‘Teacherï¼ˆé€šå¸¸æœŸæœ›ï¼‰
        - æ··åˆå…¬å¼: q*(x) = Ï€_Î¸(x)^(1-Î±) Ã— Ï€_t(x)^Î±

    Examples:
        >>> # åŸºç¡€ä½¿ç”¨ï¼ˆåŒä¸€ä¸ªæ¨¡å‹ï¼‰
        >>> model = create_optimal_sampling_model(
        ...     model_theta="meta-llama/Llama-2-7b-hf",
        ...     alpha_method="fixed",
        ...     fixed_alpha=0.5  # Î±=0.5è¡¨ç¤ºå‡åŒ€æ··åˆ
        ... )

        >>> # ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
        >>> model = create_optimal_sampling_model(
        ...     model_theta="meta-llama/Llama-2-7b-hf",        # Base
        ...     model_t="meta-llama/Llama-2-7b-chat-hf",       # Teacher/Instruct
        ...     alpha_method="kl_symmetry"
        ... )
        >>> # æœŸæœ›: Î± > 0.5ï¼ˆæ›´æ¥è¿‘é«˜è´¨é‡çš„Teacheræ¨¡å‹ï¼‰

        >>> # âœ¨ æ¨èï¼šä½¿ç”¨supportçº¦æŸï¼ˆæå‡æ•°å€¼ç¨³å®šæ€§ï¼‰
        >>> model = create_optimal_sampling_model(
        ...     model_theta="Qwen/Qwen3-8B-Base",
        ...     model_t="Qwen/Qwen3-8B",
        ...     alpha_method="kl_symmetry",
        ...     constraint_to_target=True,    # é™åˆ¶åœ¨Ï€_tçš„supportä¸Š
        ...     target_top_k=100,              # åªåœ¨Ï€_tçš„top-100 tokenä¸Šæ··åˆ
        ...     target_top_p=0.95              # æˆ–ä½¿ç”¨top-p
        ... )

        >>> # âœ¨âœ¨ æœ€ä½³å®è·µï¼šåŒæ—¶ä½¿ç”¨supportçº¦æŸå’Œspecial tokenå¤„ç†
        >>> model = create_optimal_sampling_model(
        ...     model_theta="Qwen/Qwen3-8B-Base",
        ...     model_t="Qwen/Qwen3-8B",
        ...     alpha_method="kl_symmetry",
        ...     constraint_to_target=True,              # Supportçº¦æŸ
        ...     target_top_k=100,
        ...     force_target_for_special_tokens=True,   # å¯¹EOSç­‰ç‰¹æ®Štokenä½¿ç”¨Ï€_t
        ...     force_target_for_first_token=True       # ç¬¬ä¸€ä¸ªtokenä½¿ç”¨Ï€_t
        ... )

        >>> # ğŸ”¥ å¤§æ¨¡å‹åŠ é€Ÿï¼šä½¿ç”¨quantization + Flash Attention 2
        >>> model = create_optimal_sampling_model(
        ...     model_theta="meta-llama/Llama-2-70b-hf",
        ...     model_t="meta-llama/Llama-2-70b-chat-hf",
        ...     alpha_method="kl_symmetry",
        ...     # Flash Attention 2 (2-4xåŠ é€Ÿ)
        ...     attn_implementation="flash_attention_2",
        ...     # INT4é‡åŒ– (75%æ˜¾å­˜å‡å°‘)
        ...     load_in_4bit=True,
        ...     bnb_4bit_compute_dtype=torch.bfloat16,
        ...     bnb_4bit_use_double_quant=True,
        ...     # å¤šGPU
        ...     device_map="auto",
        ...     max_memory={i: "38GB" for i in range(4)}
        ... )
    """
    return OptimalSamplingModel(
        model_theta_path=model_theta,
        model_t_path=model_t,
        alpha_method=alpha_method,
        constraint_to_target=constraint_to_target,
        target_top_k=target_top_k,
        target_top_p=target_top_p,
        force_target_for_special_tokens=force_target_for_special_tokens,
        force_target_for_first_token=force_target_for_first_token,
        **kwargs
    )


def _simple_template_fallback(messages: List[Dict[str, str]], add_generation_prompt: bool) -> str:
    """ç®€å•çš„æ¨¡æ¿fallbackï¼ˆå½“æ²¡æœ‰chat templateæˆ–jinja2ä¸å¯ç”¨æ—¶ï¼‰"""
    prompt = ""
    for msg in messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")
        if role == "user":
            prompt += f"Question: {content}\n\n"
        elif role == "assistant":
            prompt += f"Answer: {content}\n\n"
        elif role == "system":
            prompt += f"{content}\n\n"
    if add_generation_prompt:
        prompt += "Answer: "
    return prompt


# ============================================
# Chat Template å¸¸é‡
# ============================================

NATURAL_LANGUAGE_TEMPLATE = (
    "{% for message in messages %}"
    "{% if message['role'] == 'user' %}"
    "{{ 'Question: ' + message['content'] + '\\n\\n' }}"
    "{% elif message['role'] == 'assistant' %}"
    "{{ 'Answer: ' + message['content'] + '\\n\\n' }}"
    "{% elif message['role'] == 'system' %}"
    "{{ message['content'] + '\\n\\n' }}"
    "{% endif %}"
    "{% endfor %}"
    "{% if add_generation_prompt %}"
    "{{ 'Answer: ' }}"
    "{% endif %}"
)
"""è‡ªç„¶è¯­è¨€æ¨¡æ¿ï¼šä½¿ç”¨ç®€å•çš„ Question/Answer æ ¼å¼"""


def create_dual_prompts(
    messages_list: List[List[Dict[str, str]]],
    tokenizer_theta,
    tokenizer_t,
    force_nlt_in_theta: bool = True,
    base_template: Optional[str] = NATURAL_LANGUAGE_TEMPLATE,
    add_generation_prompt: bool = True,
    add_think_token: bool = False,
) -> tuple[List[str], List[str]]:
    """
    ä¸ºBaseå’ŒTeacheræ¨¡å‹åˆ›å»ºä¸åŒçš„prompts

    Args:
        messages_list: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        tokenizer_theta: Baseæ¨¡å‹çš„tokenizer
        tokenizer_t: Teacheræ¨¡å‹çš„tokenizer
        force_nlt_in_theta: æ˜¯å¦å¼ºåˆ¶Baseæ¨¡å‹ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿ï¼ˆé»˜è®¤Trueï¼‰
        base_template: Baseæ¨¡å‹ä½¿ç”¨çš„è‡ªå®šä¹‰æ¨¡æ¿ï¼ˆé»˜è®¤ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿ï¼‰
            ä»…åœ¨ force_nlt_in_theta=True æ—¶ç”Ÿæ•ˆ
        add_generation_prompt: æ˜¯å¦æ·»åŠ ç”Ÿæˆæç¤º

    Returns:
        (prompts_theta, prompts_t): ä¸¤ä¸ªæ¨¡å‹å„è‡ªçš„prompts

    Examples:
        >>> messages_list = [
        ...     [{"role": "user", "content": "What is machine learning?"}]
        ... ]
        >>> prompts_theta, prompts_t = create_dual_prompts(
        ...     messages_list,
        ...     model.tokenizer_theta,
        ...     model.tokenizer_t
        ... )
        >>> # prompts_theta: "Question: What is machine learning?\\n\\nAnswer: "
        >>> # prompts_t: "<|im_start|>user\\nWhat is machine learning?<|im_end|>\\n<|im_start|>assistant\\n"
    """
    prompts_theta = []
    prompts_t = []

    for messages in messages_list:
        # ========================================
        # å¤„ç† Base æ¨¡å‹çš„ prompt
        # ========================================
        if force_nlt_in_theta:
            # å¼ºåˆ¶ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿
            if base_template is not None:
                try:
                    from jinja2 import Template
                    template = Template(base_template)
                    prompt_theta = template.render(
                        messages=messages,
                        add_generation_prompt=add_generation_prompt
                    )
                except ImportError:
                    # Fallback: å¦‚æœæ²¡æœ‰ jinja2
                    prompt_theta = _simple_template_fallback(messages, add_generation_prompt)
            else:
                prompt_theta = _simple_template_fallback(messages, add_generation_prompt)
        else:
            # ä½¿ç”¨ tokenizer çš„ chat templateï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(tokenizer_theta, 'chat_template') and tokenizer_theta.chat_template is not None:
                prompt_theta = tokenizer_theta.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=add_generation_prompt
                )
            else:
                # Fallback: tokenizer æ²¡æœ‰ chat template
                prompt_theta = _simple_template_fallback(messages, add_generation_prompt)

        # ========================================
        # å¤„ç† Teacher æ¨¡å‹çš„ prompt
        # ========================================
        if hasattr(tokenizer_t, 'chat_template') and tokenizer_t.chat_template is not None:
            prompt_t = tokenizer_t.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt
            )
        else:
            # Fallback: tokenizer æ²¡æœ‰ chat template
            prompt_t = _simple_template_fallback(messages, add_generation_prompt)

        if add_think_token:
            prompt_theta += "<think>"
            prompt_t += "<think>"

        prompts_theta.append(prompt_theta)
        prompts_t.append(prompt_t)

    return prompts_theta, prompts_t


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹ï¼šå±•ç¤ºåŒ prompt åŠŸèƒ½ï¼ˆBase å’Œ Teacher ä½¿ç”¨ä¸åŒçš„ chat templateï¼‰
    print("Testing OptimalSamplingModel with Dual Prompts...")
    print("=" * 80)

    # æ³¨æ„: éœ€è¦æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹è·¯å¾„
    model = create_optimal_sampling_model(
        model_theta="Qwen/Qwen3-4B-Base",           # Base æ¨¡å‹
        model_t="Qwen/Qwen3-14B",      # Teacher/Instruct æ¨¡å‹
        alpha_method="kl_symmetry",
        constraint_to_target=True,
        target_top_k=32,
        force_target_for_first_token=True,
        force_target_for_special_tokens=True,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    messages_list = [
        [
            {"role": "user", "content": "Hello, how are you?"}
        ],
        [
            {"role": "user", "content": "What is 2+2? Explain your reasoning."}
        ],
        [
            {"role": "user",
             "content": "The operation $\\otimes$ is defined for all nonzero numbers by $a \\otimes b = \\frac{a^{2}}{b}$. Determine $[(1 \\otimes 2) \\otimes 3] - [1 \\otimes (2 \\otimes 3)]$."}
        ],
    ]

    print("\n" + "=" * 80)
    print("âœ¨ Using Dual Prompt Mode")
    print("=" * 80)
    print("- Base Model (Ï€_Î¸): Using Natural Language Template 'Question: ... Answer: ...'")
    print("- Teacher Model (Ï€_t): Using Std Chat Template (e.g., <|im_start|>...)")
    print()

    # âœ¨ åˆ›å»ºåŒ promptï¼ˆBase ç”¨è‡ªç„¶è¯­è¨€ï¼ŒTeacher ç”¨æ ‡å‡† templateï¼‰
    prompts_theta, prompts_t = create_dual_prompts(
        messages_list,
        model.tokenizer_theta,
        model.tokenizer_t,
        base_template=NATURAL_LANGUAGE_TEMPLATE  # è‡ªç„¶è¯­è¨€æ¨¡æ¿
    )

    # é€ä¸ªå¤„ç†æ¯ä¸ªé—®é¢˜
    for i in range(len(messages_list)):
        print(f"\n\n{'=' * 80}")
        print(f"Question: {i+1}/{len(messages_list)}")
        print("=" * 80)
        # print(f"é—®é¢˜: {messages_list[i][0]['content'][:80]}...")
        print("[PRMPT T] ", prompts_t[i])
        print("[PRMPT Î¸] ", prompts_theta[i])

        # âœ¨ ä½¿ç”¨åŒ prompt ç”Ÿæˆï¼ˆå¯é€‰ï¼šè¿”å› logitsï¼‰
        outputs = model.generate(
            prompts=[prompts_theta[i]],    # Base çœ‹è‡ªç„¶è¯­è¨€
            prompts_t=[prompts_t[i]],      # Teacher çœ‹ chat template
            max_new_tokens=4096,
            return_logits=False,            # å¯è®¾ä¸º True è¿”å› logits
            return_q_star_probs=False,      # å¯è®¾ä¸º True è¿”å› q*
            # temperature=0.7,
            # top_p=0.9,
        )

        print("\n" + "-" * 80)
        print("Response:")
        print("-" * 80)
        print(outputs.generated_texts[0])

        print("\n" + "-" * 80)
        print("Stats Info:")
        print("-" * 80)
        print(f"  Generated tokens: {outputs.generated_ids.shape[1]}")
        print(f"  Alpha mean: {outputs.alpha_values.mean():.3f}")
        print(f"  Alpha std: {outputs.alpha_values.std():.3f}")
        print(f"  Alpha min/max: {outputs.alpha_values.min():.3f} / {outputs.alpha_values.max():.3f}")
        print(f"  ESS ratio mean: {outputs.ess_ratios.mean():.3f}")
        print(f"  ESS ratio std: {outputs.ess_ratios.std():.3f}")

        # if outputs.diagnostics:
        #     kl_theta = outputs.diagnostics.get("kl_theta")
        #     kl_t = outputs.diagnostics.get("kl_t")
        #     if kl_theta is not None and kl_t is not None:
        #         print(f"  KL(q||Ï€_Î¸) mean: {kl_theta.mean():.3f}")
        #         print(f"  KL(q||Ï€_t) mean: {kl_t.mean():.3f}")

    print("\n\n" + "=" * 80)
    print("âœ… Test Success!")
    print("=" * 80)
    # print("\næç¤º:")
    # print("  - Alpha > 0.5 è¡¨ç¤ºæ›´æ¥è¿‘ Teacher æ¨¡å‹")
    # print("  - ESS ratio â‰ˆ 1.0 è¡¨ç¤ºä¸¤ä¸ªæ¨¡å‹çš„é‡‡æ ·æ•ˆç‡å¹³è¡¡")
    # print("  - å¯ä»¥è®¾ç½® return_logits=True æ¥è·å–æ¯æ­¥çš„ logits")
    # print("  - å¯ä»¥è®¾ç½® skip_decode=True åœ¨å¤–éƒ¨è¿›è¡Œ decode")
